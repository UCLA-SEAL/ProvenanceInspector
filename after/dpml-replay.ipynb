{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuWi1REPD_Ux"
   },
   "source": [
    "# DPML | Latency Replay\n",
    "\n",
    "In this notebook, we investigate the reproducibility of transformation sequences captured by `dpml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Zs0hYFFIYf"
   },
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8_koamxFHfR",
    "outputId": "e892403a-70b8-4728-ec18-c31ee7e81580"
   },
   "outputs": [],
   "source": [
    "from lineage import LeBatch\n",
    "from lineage.transformation import DPMLClassWrapper, DPMLCallableWrapper\n",
    "from lineage.utils import *\n",
    "\n",
    "from sibyl import *\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktCv--paFOqG"
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:50000]\")\n",
    "dataset = dataset.rename_column('sentence', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to be Tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scheduler.tran_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = SibylTransformScheduler(\"sentiment\", class_wrapper=DPMLClassWrapper)\n",
    "stochastic_list = [Concept2Sentence, ConceptMix, Emojify]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711d4a71a9c741e88e3a13b971544e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_trials = 3\n",
    "batch_size= 10\n",
    "\n",
    "text, label = dataset['text'], dataset['label'] \n",
    "new_text, new_label = [], []\n",
    "\n",
    "scheduler.num_INV = 1\n",
    "scheduler.num_SIB = 1\n",
    "\n",
    "transform_schedule = []\n",
    "for i in tqdm(range(0, len(label), batch_size)):\n",
    "    transforms = []\n",
    "    for transform in scheduler.sample():\n",
    "        if transform.wrapped_class in stochastic_list:\n",
    "            continue\n",
    "        transforms.append(transform)\n",
    "    transform_schedule.append(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating CSV Replay Time / Memory Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "class catchtime(object):\n",
    "    def __init__(self, name=\"Code Block\"):\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.t = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.t = time.perf_counter() - self.t\n",
    "        print('{0:6.3f}s : {1}'.format(self.t, self.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rng_state(fn, attr, state):\n",
    "    rng_state = preprocess_params(state)\n",
    "    random_generator = getattr(fn.func.__self__, attr)\n",
    "    random_generator.__setstate__(rng_state)\n",
    "    setattr(fn.func.__self__, attr, random_generator)\n",
    "    return fn\n",
    "\n",
    "def replay_all_from_csv():\n",
    "      \n",
    "    with catchtime(\"Load CSVTransformLogger\") as t:\n",
    "        from lineage.storage.csv.transform_logger import TransformLogger as CSVTransformLogger\n",
    "    \n",
    "    # fetch data\n",
    "    with catchtime(\"Load data\") as t:\n",
    "        logger = CSVTransformLogger()\n",
    "        df = pd.read_csv(logger.path, header=None, names=['batch_id', 'text', 'target', 'transform_prov'])\n",
    "        transform_df = pd.read_csv(logger.transform_path, header=None, index_col=0, names=['transform_id', 'transform'])\n",
    "    \n",
    "    with catchtime(\"Load batches + transform_set\") as t:\n",
    "        transform_idxs = set()\n",
    "        batches = {}\n",
    "        for idx, row in df.iterrows():\n",
    "            bid = row['batch_id']\n",
    "            if bid not in batches:\n",
    "                batches[bid] = {'text':[], 'target':[], 'transform': []}\n",
    "\n",
    "            batches[bid]['text'].append(row['text'])\n",
    "            batches[bid]['target'].append(row['target'])\n",
    "\n",
    "            if len(batches[bid]['transform']) == 0:\n",
    "                batches[bid]['transform'] = eval(row['transform_prov'])\n",
    "                transform_idxs = transform_idxs | set(batches[bid]['transform'])\n",
    "                    \n",
    "    with catchtime(\"Load transforms\") as t:\n",
    "        transforms = []\n",
    "        random_states = []\n",
    "        hashes = []\n",
    "        mapping = {}\n",
    "        for idx in transform_idxs:\n",
    "            t_prov = json.loads(transform_df.loc[idx]['transform'])\n",
    "            random_state_attr = t_prov.pop('class_rng')\n",
    "            random_state_info = t_prov.pop('callable_rng_state')\n",
    "            random_states.append((random_state_attr, random_state_info))\n",
    "\n",
    "            t_prov_hash = hash(repr(t_prov))\n",
    "            if t_prov_hash not in hashes:\n",
    "                transforms.append(load_transform_from_replay_provenance(t_prov))\n",
    "                hashes.append(t_prov_hash)\n",
    "                mapping[idx] = hashes.index(t_prov_hash)\n",
    "            else:\n",
    "                mapping[idx] = hashes.index(t_prov_hash)\n",
    "    load_time = t.t\n",
    "\n",
    "    # replay\n",
    "    with catchtime(\"Replay\") as t:\n",
    "        new_records = []\n",
    "        for batch_id in sorted(list(batches.keys())):\n",
    "            batch = (batches[batch_id]['text'], batches[batch_id]['target'])\n",
    "            for idx in batches[batch_id]['transform']:\n",
    "                rs_attr, rs_info = random_states[idx]\n",
    "                fn_id = mapping[idx]\n",
    "                t_fn = set_rng_state(transforms[fn_id], rs_attr, rs_info)\n",
    "                batch = t_fn(batch)\n",
    "            texts, labels = batch\n",
    "            new_records += [(x, y) for x,y in zip(texts, labels)]\n",
    "    replay_time = t.t\n",
    "            \n",
    "    return new_records, load_time, replay_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay with CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_pth = \"dpml/lineage/storage/dpml.csv\"\n",
    "if os.path.exists(csv_file_pth):\n",
    "    os.remove(csv_file_pth)\n",
    "if os.path.exists(\"dpml/lineage/storage/transform.csv\"):\n",
    "    os.remove(\"dpml/lineage/storage/transform.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is DA58-C5DE\n",
      "\n",
      " Directory of C:\\Users\\Fabrice\\Documents\\GitHub\\dpml\\after\\dpml\\lineage\\storage\n",
      "\n",
      "08/10/2022  12:23 PM    <DIR>          .\n",
      "08/09/2022  03:26 PM    <DIR>          ..\n",
      "07/27/2022  01:16 PM               312 __init__.py\n",
      "07/27/2022  01:16 PM    <DIR>          __pycache__\n",
      "08/09/2022  03:26 PM    <DIR>          csv\n",
      "08/09/2022  03:26 PM    <DIR>          sqlalchemy\n",
      "               1 File(s)            312 bytes\n",
      "               5 Dir(s)  346,171,846,656 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls \"dpml/lineage/storage/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1618331180b4b25b2da1d69fcfc5cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Trial 0: 167.340 seconds\n",
      "Elapsed logging time for Trial 0: 187.849 seconds\n",
      " 0.000s : Load CSVTransformLogger\n",
      " 0.129s : Load data\n",
      " 2.732s : Load batches + transform_set\n",
      " 5.627s : Load transforms\n",
      "155.464s : Replay\n",
      "Elapsed replay time for Trial 0: 163.978 seconds\n",
      "Replay mismatches for Trial 0: 0\n",
      "Elapsed time for Trial 1: 157.105 seconds\n",
      "Elapsed logging time for Trial 1: 188.200 seconds\n",
      " 0.000s : Load CSVTransformLogger\n",
      " 0.123s : Load data\n",
      " 2.106s : Load batches + transform_set\n",
      " 5.394s : Load transforms\n",
      "158.944s : Replay\n",
      "Elapsed replay time for Trial 1: 166.598 seconds\n",
      "Replay mismatches for Trial 1: 0\n",
      "Elapsed time for Trial 2: 161.151 seconds\n",
      "Elapsed logging time for Trial 2: 194.913 seconds\n",
      " 0.000s : Load CSVTransformLogger\n",
      " 0.129s : Load data\n",
      " 2.060s : Load batches + transform_set\n",
      " 4.955s : Load transforms\n",
      "158.782s : Replay\n",
      "Elapsed replay time for Trial 2: 165.958 seconds\n",
      "Replay mismatches for Trial 2: 0\n"
     ]
    }
   ],
   "source": [
    "no_lineage_times = []\n",
    "replay_logging_times, replay_fn_load_times, replay_generation_times, num_mismatches = [], [], [], []\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    no_lineage_text, no_lineage_targets = [], []\n",
    "    replay_log_text, replay_log_targets = [], []\n",
    "    \n",
    "    # no lineage ====================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(label), batch_size), transform_schedule):\n",
    "        \n",
    "        text_batch = text[i:i+batch_size]\n",
    "        label_batch = label[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        for transform in t_sched:\n",
    "            batch = transform.transform_batch(batch)\n",
    "            \n",
    "        no_lineage_text.extend(batch[0])\n",
    "        no_lineage_targets.extend(batch[1])\n",
    "        \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    no_lineage_times.append(run_time)\n",
    "    print('Elapsed time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay logging ================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(label), batch_size), transform_schedule):\n",
    "        text_batch = text[i:i+batch_size]\n",
    "        label_batch = label[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        \n",
    "        if len(t_sched) == 0:\n",
    "            continue\n",
    "            \n",
    "        with LeBatch(original_batch=batch) as le_batch:\n",
    "            init_rng_state = []\n",
    "            for transform in t_sched:\n",
    "                batch = le_batch.apply(batch, transform.transform_batch)\n",
    "            \n",
    "        replay_log_text.extend([x.text for x in batch])\n",
    "        replay_log_targets.extend([x.target for x in batch])\n",
    "            \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_logging_times.append(run_time)\n",
    "    print('Elapsed logging time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay generation ==============================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    new_records, load_time, replay_time = replay_all_from_csv()\n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_fn_load_times.append(load_time)\n",
    "    replay_generation_times.append(replay_time)\n",
    "    print('Elapsed replay time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    original_records = [(text, target) for text, target in zip(replay_log_text, replay_log_targets)]\n",
    "    num_mismatch = 0\n",
    "    counter = 0\n",
    "    for old_r, new_r in zip(original_records, new_records):\n",
    "        if old_r[0] != new_r[0] or np.any(old_r[1] != new_r[1]):\n",
    "            num_mismatch += 1  \n",
    "        counter += 1\n",
    "    num_mismatches.append(num_mismatch)\n",
    "    print('Replay mismatches for Trial {0}: {1}'.format(trial, num_mismatch))   \n",
    "    \n",
    "    # del original_records, new_records\n",
    "    \n",
    "    le_batch.transform_logger.clean_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_lineage_times: 161.86528426666675\n",
      "replay_logging_times: 190.3204662666667\n",
      "replay_fn_load_times: 5.325154599999905\n",
      "replay_generation_times: 157.73009239999988\n",
      "num_mismatches: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"no_lineage_times:\", np.mean(no_lineage_times))\n",
    "print(\"replay_logging_times:\", np.mean(replay_logging_times))\n",
    "print(\"replay_fn_load_times:\", np.mean(replay_fn_load_times))\n",
    "print(\"replay_generation_times:\", np.mean(replay_generation_times))\n",
    "print(\"num_mismatches:\", np.mean(num_mismatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new_records = replay_all_from_csv()\n",
    "# \n",
    "# original_records = [(text, target) for text, target in zip(replay_log_text, replay_log_targets)]\n",
    "# num_mismatch = 0\n",
    "# counter = 0\n",
    "# for old_r, new_r in zip(original_records, new_records):\n",
    "#     if old_r[0] != new_r[0] or np.any(old_r[1] != new_r[1]):\n",
    "#         num_mismatch += 1  \n",
    "#     counter += 1\n",
    "# num_mismatches.append(num_mismatch)\n",
    "# print('Replay mismatches for Trial {0}: {1}'.format(trial, num_mismatch))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(original_records, new_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t1, t2 in list(zip(transform_idx.values(), new_transform_idx.values())):\n",
    "#     print(t1.func.__self__.__class__.__name__)\n",
    "#     print(t2.func.__self__.__class__.__name__)\n",
    "#     print(t1.func.__self__.np_random.__getstate__())\n",
    "#     print(t2.func.__self__.np_random.__getstate__())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77742a28c7d64d2f9305215b36428eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Trial 0:  0.003 seconds\n",
      "Elapsed logging time for Trial 0:  0.169 seconds\n",
      "Elapsed replay time for Trial 0:  0.008 seconds\n",
      "Replay mismatches for Trial 0: 5\n"
     ]
    }
   ],
   "source": [
    "no_lineage_times = []\n",
    "replay_logging_times, replay_generation_times, num_mismatches = [], [], []\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    no_lineage_text, no_lineage_targets = [], []\n",
    "    replay_log_text, replay_log_targets = [], []\n",
    "    \n",
    "    # no lineage ====================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(label), batch_size), transform_schedule):\n",
    "        \n",
    "        text_batch = text[i:i+batch_size]\n",
    "        label_batch = label[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        for transform in t_sched:\n",
    "            batch = transform.transform_batch(batch)\n",
    "            \n",
    "        no_lineage_text.extend(batch[0])\n",
    "        no_lineage_targets.extend(batch[1])\n",
    "        \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    no_lineage_times.append(run_time)\n",
    "    print('Elapsed time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay logging ================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(label), batch_size), transform_schedule):\n",
    "        text_batch = text[i:i+batch_size]\n",
    "        label_batch = label[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        \n",
    "        if len(t_sched) == 0:\n",
    "            continue\n",
    "            \n",
    "        with LeBatch(original_batch=batch) as le_batch:\n",
    "            init_rng_state = []\n",
    "            for transform in t_sched:\n",
    "                batch = le_batch.apply(batch, transform.transform_batch)\n",
    "            \n",
    "        replay_log_text.extend([x.text for x in batch])\n",
    "        replay_log_targets.extend([x.target for x in batch])\n",
    "            \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_logging_times.append(run_time)\n",
    "    print('Elapsed logging time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay generation ==============================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    new_records = replay_all_from_db()\n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_generation_times.append(run_time)\n",
    "    print('Elapsed replay time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    original_records = [(text, target) for text, target in zip(replay_log_text, replay_log_targets)]\n",
    "    num_mismatch = 0\n",
    "    counter = 0\n",
    "    for old_r, new_r in zip(original_records, new_records):\n",
    "        if old_r[0] != new_r[0] or np.any(old_r[1] != new_r[1]):\n",
    "            num_mismatch += 1  \n",
    "        counter += 1\n",
    "    num_mismatches.append(num_mismatch)\n",
    "    print('Replay mismatches for Trial {0}: {1}'.format(trial, num_mismatch))    \n",
    "    \n",
    "    # truncate all table data\n",
    "    # le_batch.transform_logger.clean_db()\n",
    "        \n",
    "#     if os.path.exists(\"./dpml/lineage/storage/dpml.db\"):\n",
    "#         os.remove(\"./dpml/lineage/storage/dpml.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lineage.storage.sqlalchemy import *\n",
    "from sqlalchemy import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record\n",
      "{'id': 1, 'text': 'hide new secretions from the parental units ', 'target': '0', 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 2, 'text': 'contains no wit , only labored gags ', 'target': '0', 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 3, 'text': 'that loves its characters and communicates something rather beautiful about human nature ', 'target': '1', 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 4, 'text': 'remains utterly satisfied to remain the same throughout ', 'target': '0', 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 5, 'text': 'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up ', 'target': '0', 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "Transform\n",
      "{'id': 1, 'module_name': 'sibyl.transformations.text.insertion.sentiment_phrase', 'class_name': 'InsertPositivePhrase', 'class_args': 'null', 'class_kwargs': '{\"task_name\": \"sentiment\", \"return_metadata\": true}', 'class_rng': 'np_random', 'callable_name': 'transform_batch', 'callable_args': 'null', 'callable_kwargs': 'null', 'callable_rng_state': '{\"bit_generator\": \"PCG64\", \"state\": {\"state\": 98850545484944576192068515823091834897, \"inc\": 16450919397810582319219321886622321693}, \"has_uint32\": 1, \"uinteger\": 541040237}', 'callable_is_stochastic': True, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 2, 'module_name': 'sibyl.transformations.text.word_swap.word_deletion', 'class_name': 'WordDeletion', 'class_args': 'null', 'class_kwargs': '{\"task_name\": \"sentiment\", \"return_metadata\": true}', 'class_rng': 'np_random', 'callable_name': 'transform_batch', 'callable_args': 'null', 'callable_kwargs': 'null', 'callable_rng_state': '{\"bit_generator\": \"PCG64\", \"state\": {\"state\": 50726921910688525653174208925257482057, \"inc\": 16450919397810582319219321886622321693}, \"has_uint32\": 0, \"uinteger\": 0}', 'callable_is_stochastic': True, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "TransformApplied\n",
      "{'id': 1, 'input_record_id': 1, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 1, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 2, 'input_record_id': 1, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 2, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 3, 'input_record_id': 2, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 1, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 4, 'input_record_id': 2, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 2, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 5, 'input_record_id': 3, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 1, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 6, 'input_record_id': 3, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 2, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 7, 'input_record_id': 4, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 1, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 8, 'input_record_id': 4, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 2, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 9, 'input_record_id': 5, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 1, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n",
      "{'id': 10, 'input_record_id': 5, 'output_record_id': None, 'diff': None, 'diff_granularity': None, 'transformation_id': 2, 'created_at': datetime.datetime(2022, 8, 9, 22, 36, 38)}\n"
     ]
    }
   ],
   "source": [
    "logger = TransformLogger()\n",
    "    \n",
    "print('Record')\n",
    "stmt = select(Record)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)\n",
    "\n",
    "print('Transform')\n",
    "stmt = select(Transform)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)\n",
    "\n",
    "print('TransformApplied')\n",
    "stmt = select(TransformApplied)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.random._generator.Generator"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t_orig().np_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This is a test.\", \"This isn't a test!\"]\n",
    "target = [0, 1]\n",
    "batch = (text, target)\n",
    "\n",
    "t_orig = TRANSFORMATIONS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPMLClassWrapper\n",
      "DPMLClassWrapper | transform_batch\n",
      "(['This is a test.', 'This is not a test!'], [0, 1])\n",
      "_class_name: ExpandContractions\n",
      "_class_args: []\n",
      "_class_kwargs: {'task_name': 'sentiment', 'return_metadata': True}\n",
      "_class_rng: Generator(PCG64)\n",
      "_callable_name: transform_batch\n",
      "_callable_args: []\n",
      "_callable_kwargs: []\n",
      "_callable_rng_state: {'bit_generator': 'PCG64', 'state': {'state': 129413257090554225206130458028910539494, 'inc': 16450919397810582319219321886622321693}, 'has_uint32': 0, 'uinteger': 0}\n",
      "DPMLClassWrapper | transform_Xy\n",
      "This is not a test! 1\n",
      "_class_name: ExpandContractions\n",
      "_class_args: []\n",
      "_class_kwargs: {'task_name': 'sentiment', 'return_metadata': True}\n",
      "_class_rng: Generator(PCG64)\n",
      "_callable_name: transform_Xy\n",
      "_callable_args: []\n",
      "_callable_kwargs: []\n",
      "_callable_rng_state: {'bit_generator': 'PCG64', 'state': {'state': 129413257090554225206130458028910539494, 'inc': 16450919397810582319219321886622321693}, 'has_uint32': 0, 'uinteger': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"DPMLClassWrapper\")\n",
    "\n",
    "t_class_wrapped = DPMLClassWrapper(t_orig)\n",
    "t_class_wrapped = t_class_wrapped(task_name=\"sentiment\", return_metadata=True)\n",
    "\n",
    "batch = t_class_wrapped.transform_batch(batch)\n",
    "\n",
    "print(\"DPMLClassWrapper | transform_batch\")\n",
    "print(batch)\n",
    "print(\"_class_name:\", t_class_wrapped._class_name)\n",
    "print(\"_class_args:\", t_class_wrapped._class_args)\n",
    "print(\"_class_kwargs:\", t_class_wrapped._class_kwargs)\n",
    "print(\"_class_rng:\", t_class_wrapped._class_rng)\n",
    "print(\"_callable_name:\", t_class_wrapped._callable_name)\n",
    "print(\"_callable_args:\", t_class_wrapped._callable_args)\n",
    "print(\"_callable_kwargs:\", t_class_wrapped._callable_kwargs)\n",
    "print(\"_callable_rng_state:\", t_class_wrapped._callable_rng_state)\n",
    "\n",
    "X, y, meta = t_class_wrapped.transform_Xy(text[1], target[1])\n",
    "\n",
    "print(\"DPMLClassWrapper | transform_Xy\")\n",
    "print(X, y)\n",
    "print(\"_class_name:\", t_class_wrapped._class_name)\n",
    "print(\"_class_args:\", t_class_wrapped._class_args)\n",
    "print(\"_class_kwargs:\", t_class_wrapped._class_kwargs)\n",
    "print(\"_class_rng:\", t_class_wrapped._class_rng)\n",
    "print(\"_callable_name:\", t_class_wrapped._callable_name)\n",
    "print(\"_callable_args:\", t_class_wrapped._callable_args)\n",
    "print(\"_callable_kwargs:\", t_class_wrapped._callable_kwargs)\n",
    "print(\"_callable_rng_state:\", t_class_wrapped._callable_rng_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPMLCallableWrapper | transform_batch\n",
      "(['hide new secretions from the parental units ', 'contains no wit , only labored gags '], [0, 0])\n",
      "_callable_name ('transform_batch',)\n",
      "_callable_args []\n",
      "_callable_kwargs []\n",
      "DPMLCallableWrapper | transform_Xy\n",
      "contains no wit , only labored gags  1\n",
      "_callable_name ('transform_Xy',)\n",
      "_callable_args []\n",
      "_callable_kwargs []\n"
     ]
    }
   ],
   "source": [
    "t_init = t_orig(task_name=\"sentiment\", return_metadata=True)\n",
    "\n",
    "t_callable_wrapped = DPMLCallableWrapper(t_init.transform_batch)\n",
    "batch = t_callable_wrapped(batch)\n",
    "\n",
    "print(\"DPMLCallableWrapper | transform_batch\")\n",
    "print(batch)\n",
    "print(\"_callable_name\", t_callable_wrapped._callable_name)\n",
    "print(\"_callable_args\", t_callable_wrapped._callable_args)\n",
    "print(\"_callable_kwargs\", t_callable_wrapped._callable_kwargs)\n",
    "\n",
    "t_callable_wrapped = DPMLCallableWrapper(t_init.transform_Xy)\n",
    "X, y, meta = t_callable_wrapped(text[1], target[1])\n",
    "\n",
    "print(\"DPMLCallableWrapper | transform_Xy\")\n",
    "print(X, y)\n",
    "print(\"_callable_name\", t_callable_wrapped._callable_name)\n",
    "print(\"_callable_args\", t_callable_wrapped._callable_args)\n",
    "print(\"_callable_kwargs\", t_callable_wrapped._callable_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dpml",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
