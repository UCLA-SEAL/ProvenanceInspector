{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuWi1REPD_Ux"
   },
   "source": [
    "# DPML | Latency Replay\n",
    "\n",
    "In this notebook, we investigate the reproducibility of transformation sequences captured by `dpml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Zs0hYFFIYf"
   },
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8_koamxFHfR",
    "outputId": "e892403a-70b8-4728-ec18-c31ee7e81580"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lineage.logger.text_logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16044\\213270448.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlineage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeBatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msibyl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabrice\\documents\\github\\dpml\\after\\dpml\\lineage\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \"\"\"\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mle_record\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeRecord\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mle_text\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mle_target\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeTarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabrice\\documents\\github\\dpml\\after\\dpml\\lineage\\le_record.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlineage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_target\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeTarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlineage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_text\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\fabrice\\documents\\github\\dpml\\after\\dpml\\lineage\\le_target.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLeTarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n",
      "\u001b[1;32mc:\\users\\fabrice\\documents\\github\\dpml\\after\\dpml\\lineage\\logger\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtext_logger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtransformation_logger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransformationLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlabel_logger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lineage.logger.text_logger'"
     ]
    }
   ],
   "source": [
    "from lineage import LeBatch\n",
    "\n",
    "from sibyl import *\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktCv--paFOqG"
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:100]\")\n",
    "dataset = dataset.rename_column('sentence', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to be Tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = SibylTransformScheduler(\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37055c9011d7416e869b66136357b3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sibyl.transformations.text.mixture.text_mix.SentMix object at 0x0000020B23AE0370>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeSynonym object at 0x0000020B178EBAC0>\n",
      "<sibyl.transformations.text.insertion.sentiment_phrase.InsertPositivePhrase object at 0x0000020B0F7BA770>\n",
      "<sibyl.transformations.text.word_swap.word_deletion.WordDeletion object at 0x0000020B20BBF040>\n",
      "<sibyl.transformations.text.mixture.text_mix.SentMix object at 0x0000020B23AE0370>\n",
      "<sibyl.transformations.text.typos.char_substitute.RandomCharSubst object at 0x0000020B13F283D0>\n",
      "<sibyl.transformations.text.mixture.concept_mix.ConceptMix object at 0x0000020B23B05720>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeSynonym object at 0x0000020B178EBAC0>\n",
      "<sibyl.transformations.text.contraction.expand_contractions.ExpandContractions object at 0x0000020B0AD81D80>\n",
      "<sibyl.transformations.text.insertion.sentiment_phrase.InsertNegativePhrase object at 0x0000020B0F7BA9B0>\n",
      "<sibyl.transformations.text.links.add_sentiment_link.AddPositiveLink object at 0x0000020B0F7BBDF0>\n",
      "<sibyl.transformations.text.word_swap.random_swap.RandomSwap object at 0x0000020B20BF6050>\n",
      "<sibyl.transformations.text.mixture.text_mix.TextMix object at 0x0000020B20BF5780>\n",
      "<sibyl.transformations.text.emoji.demojify.Demojify object at 0x0000020B0AD832E0>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeAntonym object at 0x0000020B18CC3A30>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeHyponym object at 0x0000020B0DE3FE80>\n",
      "<sibyl.transformations.text.links.add_sentiment_link.AddNegativeLink object at 0x0000020B0F7BBFD0>\n",
      "<sibyl.transformations.text.insertion.insert_punctuation_marks.InsertPunctuationMarks object at 0x0000020B0F7BB850>\n",
      "<sibyl.transformations.text.mixture.text_mix.WordMix object at 0x0000020B23AE06D0>\n",
      "<sibyl.transformations.text.emoji.demojify.RemoveNeutralEmoji object at 0x0000020B0B7F8A30>\n",
      "Elapsed time: 23.076 seconds\n"
     ]
    }
   ],
   "source": [
    "text, label = dataset['text'], dataset['label'] \n",
    "new_text, new_label = [], []\n",
    "\n",
    "batch_size= 10\n",
    "\n",
    "scheduler.num_INV = 1\n",
    "scheduler.num_SIB = 1\n",
    "\n",
    "records = []\n",
    "startTime = time.perf_counter()\n",
    "for i in tqdm(range(0, len(label), batch_size)):\n",
    "    text_batch = text[i:i+batch_size]\n",
    "    label_batch = label[i:i+batch_size]\n",
    "    batch = (text_batch, label_batch)\n",
    "    for transform in scheduler.sample():\n",
    "        print(transform)\n",
    "        # batch = transform.transform_batch(batch)\n",
    "        batch = LeBatch(batch).apply(transform.transform_batch)\n",
    "    records.append(batch)\n",
    "print('Elapsed time: {:6.3f} seconds'.format(time.perf_counter() - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<LeRecord:\n",
       "  \t text=\"envelop new substance from the parental construct  ⚕️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hide new secretions from the parental units  ⚕️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hide new secretions from the parental units \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]'), (1, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"keep no someone , only undergo message  👨🏿‍🤝‍👨🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains no wit , only labored gags  👨🏿‍🤝‍👨🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains no wit , only labored gags \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]'), (1, (5, 7), 'replace: [5,7]-[5,7]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"that copulate its recommendation and intercommunicate something rather beautiful about human sort  🙎🏿‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that loves its characters and communicates something rather beautiful about human nature  🙎🏿‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that loves its characters and communicates something rather beautiful about human nature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'insert: [12,12]-[12,13]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'insert: [12,12]-[12,13]'), (1, (11, 12), 'replace: [11,12]-[11,12]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"be utterly satisfied to be the same throughout  🤞🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"remains utterly satisfied to remain the same throughout  🤞🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"remains utterly satisfied to remain the same throughout \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'insert: [8,8]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'insert: [8,8]-[8,9]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-the-nerds clichés the mortal could remove up  🧑🏾‍🤝‍🧑🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up  🧑🏾‍🤝‍🧑🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [10,10]-[10,11]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [10,10]-[10,11]'), (1, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"that 's far too tragic to be such superficial tending  🙌🏼\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's far too tragic to merit such superficial treatment  🙌🏼\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's far too tragic to merit such superficial treatment \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [10,10]-[10,11]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [10,10]-[10,11]'), (1, (9, 10), 'replace: [9,10]-[9,10]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"prove that the superior of such feel bomb as organism fauna can still palm out a small , personal product with an emotional move .  👩🏾‍🤝‍👨🏻\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .  👩🏾‍🤝‍👨🏻\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (25, 26), 'insert: [25,25]-[25,26]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (25, 26), 'insert: [25,25]-[25,26]'), (1, (23, 24), 'replace: [23,24]-[23,24]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"of saucy  👨‍🚒\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of saucy  👨‍🚒\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of saucy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a depressed fifteen-year-old 's suicidal communication  🚶🏿‍♂️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a depressed fifteen-year-old 's suicidal poetry  🚶🏿‍♂️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a depressed fifteen-year-old 's suicidal poetry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'insert: [6,6]-[6,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'insert: [6,6]-[6,7]'), (1, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"are more deeply focus through than in most ` right-thinking ' wrapper  🤷🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are more deeply thought through than in most ` right-thinking ' films  🤷🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are more deeply thought through than in most ` right-thinking ' films \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'insert: [12,12]-[12,13]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'insert: [12,12]-[12,13]'), (1, (11, 12), 'replace: [11,12]-[11,12]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"goes to absurd lengths \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"goes to absurd lengths \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"goes to absurd lengths \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do n't make movies like they used to anymore \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do n't make movies like they used to anymore \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do n't make movies like they used to anymore \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the part where nothing 's happening , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the part where nothing 's happening , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the part where nothing 's happening , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"saw how bad this movie was \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"saw how bad this movie was \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"saw how bad this movie was \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"lend some dignity to a dumb story \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"lend some dignity to a dumb story \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"lend some dignity to a dumb story \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the greatest musicians \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the greatest musicians \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the greatest musicians \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"cold movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cold movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cold movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"with his usual intelligence and subtlety \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"with his usual intelligence and subtlety \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"with his usual intelligence and subtlety \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"redundant concept \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"redundant concept \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"redundant concept \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"swimming is above all about a young woman 's face , and by casting an actress whose face projects that woman 's doubts and yearnings , it succeeds . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"swimming is above all about a young woman 's face , and by casting an actress whose face projects that woman 's doubts and yearnings , it succeeds . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"swimming is above all about a young woman 's face , and by casting an actress whose face projects that woman 's doubts and yearnings , it succeeds . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"the original and some ways even betters 🇮🇹\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the original and some ways even betters it\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"equals the original and in some ways even betters it \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"if , 👀 , who 🆙 storm as fringe named dirty dick .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"if , see , who up storm as fringe named dirty dick .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"if anything , see it for karen black , who camps up a storm as a fringe feminist conspiracy theorist named dirty dick . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🅰️ 😄 your 😮\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a smile your face\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a smile on your face \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"comes from the ,\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"comes from the ,\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"comes from the brave , uninhibited performances \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"unfunny and pitifully unromantic\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"unfunny and pitifully unromantic\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"excruciatingly unfunny and pitifully unromantic \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🇧🇾 an mixed cast of antic\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"by an mixed cast of antic\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"enriched by an imaginatively mixed cast of antic spirits \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"half of dragonfly 🇮🇸 worse the part 's happening , or the part where something happening\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"half of dragonfly is worse the part 's happening , or the part where something happening\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"which half of dragonfly is worse : the part where nothing 's happening , or the part where something 's happening \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🇮🇳 🎦\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"in cinema\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"in world cinema \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 2), 'replace: [0,2]-[0,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"very 🌟 alternative\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"very good alternative\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"very good viewing alternative \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the 🈚 boilerplate clichés 🇹🇴 finish ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the nothing boilerplate clichés to finish ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the plot is nothing but boilerplate clichés from start to finish , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"the is action xtilted\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the action is xtilted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the action is stilted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (3, 3), 'delete: [2,3]'), (0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (1, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"all on cylindegs\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on all cylindegs \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on all cylinders \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (2, 2), 'delete: [1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (0, 1), 'insert: [0,0]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"in find little of interest will this film , which is often oreachy and poorly acted\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"will find little of interest in this film , which is often oreachy and poorly acted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"will find little of interest in this film , which is often preachy and poorly acted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'replace: [12,13]-[12,13]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (5, 6), 'replace: [5,6]-[5,6]'), (0, (12, 13), 'replace: [12,13]-[12,13]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"by worst tbe far movie of the year\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"by far tbe worst movie of the year \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"by far the worst movie of the year \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (4, 4), 'delete: [2,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (1, 3), 'insert: [1,1]-[1,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"sit yhrough ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sit yhrough , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sit through , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"funny than anotuer `` best man '' clone by weaving a theme throughout this more film\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"more than anotuer `` best man '' clone by weaving a theme throughout this funny film \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"more than another `` best man '' clone by weaving a theme throughout this funny film \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (14, 15), 'replace: [14,15]-[14,15]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the issues real tucked between the silly and crude storypine\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the real issues tucked between the silly and crude storypine \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the real issues tucked between the silly and crude storyline \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (25, 25), 'delete: [24,25]'), (0, (31, 32), 'replace: [31,32]-[31,32]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (31, 32), 'replace: [31,32]-[31,32]'), (1, (23, 24), 'insert: [23,23]-[23,24]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"heroed\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"heroed \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"heroes \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"of to the existence oboivious this film\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"oboivious to the existence of this film \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"oblivious to the existence of this film \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"sharlly\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sharlly \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sharply \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"the entire point of a shaggy dog story , of course , is that it goes nowhere , and this is classic nowheresville in every sense ,  🧑‍🤝‍🧑\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the entire point of a shaggy dog story , of course , is that it goes nowhere , and this is classic nowheresville in every sense , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the entire point of a shaggy dog story , of course , is that it goes nowhere , and this is classic nowheresville in every sense . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (26, 27), 'replace: [26,27]-[26,27]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (27, 28), 'insert: [27,27]-[27,28]'), (0, (26, 27), 'replace: [26,27]-[26,27]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"some]imes dry  🌛\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"some]imes dry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sometimes dry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"as they come , already having beex recycled more times than i 'd care to count  🙇🏾‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as they come , already having beex recycled more times than i 'd care to count \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as they come , already having been recycled more times than i 'd care to count \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (16, 17), 'insert: [16,16]-[16,17]'), (0, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"covGrs this territory with wit and originality , suggesting that with his fourth feature  👧🏽\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"covGrs this territory with wit and originality , suggesting that with his fourth feature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"covers this territory with wit and originality , suggesting that with his fourth feature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (14, 15), 'insert: [14,14]-[14,15]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a $ 40 Xillion version of a game  🏋🏼‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a $ 40 Xillion version of a game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a $ 40 million version of a game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (8, 9), 'insert: [8,8]-[8,9]'), (0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"gorgeous and deceptively\n",
       "  minimalist  🦔\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"gorgeous and deceptively\n",
       "  minimalist \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"gorgeous and deceptively minimalist \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"cross swords with t}e best of them and  🔁\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cross swords with t}e best of them and \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cross swords with the best of them and \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (8, 9), 'insert: [8,8]-[8,9]'), (0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"as a fringe feminist conspiracy theori5t  👩🏽‍🦳\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a fringe feminist conspiracy theori5t \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a fringe feminist conspiracy theorist \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (6, 7), 'insert: [6,6]-[6,7]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"proves once again he has n't lost his touch , bringing off a superb performance in an a/mittedly middling film .  👨🏾‍⚖️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"proves once again he has n't lost his touch , bringing off a superb performance in an a/mittedly middling film . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"proves once again he has n't lost his touch , bringing off a superb performance in an admittedly middling film . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (17, 18), 'replace: [17,18]-[17,18]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (17, 18), 'replace: [17,18]-[17,18]'), (1, (21, 22), 'insert: [21,21]-[21,22]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"disappointmfnts  🍲\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"disappointmfnts \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"disappointments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"the rorrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the rorrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the horrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a muddle splashed with bloody beauty as vivid as any scorsese has ever giveW us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a muddle splashed with bloody beauty as vivid as any scorsese has ever giveW us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a muddle splashed with bloody beauty as vivid as any scorsese has ever given us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (13, 14), 'replace: [13,14]-[13,14]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"many point:ess \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"many point:ess \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"many pointless \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a beautifhlly \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a beautifhlly \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a beautifully \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"con}rived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"con}rived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contrived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a dya \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a dya \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a doa \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"poor ben bra@t could n't find stardom if mapquest emailed him point-to-point driving directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"poor ben bra@t could n't find stardom if mapquest emailed him point-to-point driving directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"poor ben bratt could n't find stardom if mapquest emailed him point-to-point driving directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"to be a` subtle and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to be a` subtle and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to be as subtle and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"starts with a legemd \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"starts with a legemd \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"starts with a legend \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"far less soghisticated and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'ContractContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"far less soghisticated and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"far less sophisticated and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"rich veins of funny stu^f in this movie \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich veins of funny stuff in this movie \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich veins of funny stuff in this movie \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"no apparent joy \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"no apparent joy \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"no apparent joy \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"shot on udly digi;al video \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shot on udly digital video \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shot on ugly digital video \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"... a sour little movie at its core ; an exploration of the emptiness that hnderlay the relentless gaiety of the 1920 's ... the film 's en\n",
       "  ing has a `` what was it all for ? '' \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"... a sour little movie at its core ; an exploration of the emptiness that hnderlay the relentless gaiety of the 1920 's ... the film 's ending has a `` what was it all for ? '' \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"... a sour little movie at its core ; an exploration of the emptiness that underlay the relentless gaiety of the 1920 's ... the film 's ending has a `` what was it all for ? '' \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"thojgh ford and neeson capably hold our interest , but its just not a thrilli8g movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"thojgh ford and neeson capably hold our interest , but its just not a thrilling movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"though ford and neeson capably hold our interest , but its just not a thrilling movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (14, 15), 'replace: [14,15]-[14,15]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"is prdtUy damned funny . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"is prdtty damned funny . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"is pretty damned funny . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"we nevez feel anything for thrse characters \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"we never feel anything for thrse characters \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"we never feel anything for these characters \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (1, 2), 'replace: [1,2]-[1,2]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"'s a lousy pne at t)at \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"'s a lousy pne at that \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"'s a lousy one at that \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (5, 6), 'replace: [5,6]-[5,6]'), (0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the corporate circus that is the recording industry in the current clim^te of mergers and downqizing \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the corporate circus that is the recording industry in the current climate of mergers and downqizing \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the corporate circus that is the recording industry in the current climate of mergers and downsizing \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]'), (1, (11, 12), 'replace: [11,12]-[11,12]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the storylines are woven together skilfully ,wthe magnificent swooping aerial shots are breathtaking , and the pverall experience is awesome . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the storylines are woven together skilfully , the magnificent swooping aerial shots are breathtaking , and the pverall experience is awesome . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the storylines are woven together skilfully , the magnificent swooping aerial shots are breathtaking , and the overall experience is awesome . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (17, 18), 'replace: [17,18]-[17,18]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"of the ost highly-praised diappointments i \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of the most highly-praised diappointments i \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of the most highly-praised disappointments i \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]'), (1, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"sounds like a cruel deception carried out by menof marginal intelligence , with reactionary ideas about women and a total lack of empthy . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sounds like a cruel deception carried out by men of marginal intelligence , with reactionary ideas about women and a total lack of empthy . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sounds like a cruel deception carried out by men of marginal intelligence , with reactionary ideas about women and a total lack of empathy . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (23, 24), 'replace: [23,24]-[23,24]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"seem fres\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"seem fres \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"seem fresh \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"to the dustbn of hstory \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to the dustbin of hstory \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to the dustbin of history \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]'), (1, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"as a director , eastwood is of hi game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a director , eastwood is of his game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a director , eastwood is off his game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (7, 8), 'replace: [7,8]-[7,8]'), (0, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"paysearnest homage toturntablists \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"paysearnest homage to turntablists \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"pays earnest homage to turntablists \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"ek and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"wek and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"weak and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (0, 1), 'replace: [0,1]-[0,1]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"skip his drec , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"skip this drec , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"skip this dreck , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"contains very fe laughs nd even less surprises \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains very fe laughs and even less surprises \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains very few laughs and even less surprises \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"film to affirm love 's power to help people endre almost unimaginble horror \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'RandomCharDel', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"film to affirm love 's power to help people endure almost unimaginble horror \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharDel', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"film to affirm love 's power to help people endure almost unimaginable horror \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (11, 12), 'replace: [11,12]-[11,12]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (9, 10), 'replace: [9,10]-[9,10]'), (0, (11, 12), 'replace: [11,12]-[11,12]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"are an absolute joyfulness \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are an absolute joy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are an absolute joy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"get \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"generates \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"generates \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", wish liveliness , is NT much merriment without the heights and first \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", like life , is n't much fun without the highs and lows \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", like life , is n't much fun without the highs and lows \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'replace: [12,13]-[12,13]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"free-base on a unfeigned and historically pregnant account \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"based on a true and historically significant story \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"based on a true and historically significant story \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 8), 'replace: [6,8]-[6,8]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"well-rounded testimonial \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"well-rounded tribute \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"well-rounded tribute \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", though many of the histrion fox off a sparkle or 2 when they firstly seem , they calcium NT get adequate rut in this insensate vacancy of a comedy to begin a reaction . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", though many of the actors throw off a spark or two when they first appear , they ca n't generate enough heat in this cold vacuum of a comedy to start a reaction . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", though many of the actors throw off a spark or two when they first appear , they ca n't generate enough heat in this cold vacuum of a comedy to start a reaction . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (31, 32), 'replace: [31,32]-[31,32]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"so lots corresponding a immature robert deniro \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"so much like a young robert deniro \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"so much like a young robert deniro \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"khouri do , with wonderful flair , to observe the extremum of crank farce and blood-curdling class intensity on peerless continuum . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"khouri manages , with terrific flair , to keep the extremes of screwball farce and blood-curdling family intensity on one continuum . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"khouri manages , with terrific flair , to keep the extremes of screwball farce and blood-curdling family intensity on one continuum . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (19, 20), 'replace: [19,20]-[19,20]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"forge an absorb entertainment out \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fashioning an engrossing entertainment out \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fashioning an engrossing entertainment out \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"spiffy invigorate lineament \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"spiffy animated feature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"spiffy animated feature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 3), 'replace: [1,3]-[1,3]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"that 's so sloppily not written and cast that you can not believe anyone more central to the creation of bugsy than the caterer\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the caterer \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the caterer \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... doesn't toss around some intriguing questions about the difference between human and Saint-Martinoise life\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... tosses around some intriguing questions about the difference between human and Saint-Martinoise life \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (24, 25), 'replace: [24,25]-[24,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"strung-together moments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"strung-together moments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"strung-together moments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", generous and subversive artworks \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", generous and subversive artworks \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", generous and subversive artworks \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"it does n't follow the stale , standard , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it does n't follow the stale , standard , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it does n't follow the stale , standard , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"funny yet \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"funny yet \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"funny yet \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"overbearing and over-the-top \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"overbearing and over-the-top \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"overbearing and over-the-top \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"it 's not robert duvall !\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's robert duvall ! \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's robert duvall ! \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"rich and sudden wisdom \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich and sudden wisdom \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich and sudden wisdom \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"acted and directed , it 's not clear that Murray most certainly has a new career ahead of him\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\"), (1, \"{'class': 'AddNegation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that Murray most certainly has a new career ahead of him \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeLocation', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that washington most certainly has a new career ahead of him \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (6, 7), 'insert: [6,6]-[6,7]'), (0, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LeRecord:\n",
       "\t text=\"envelop new substance from the parental construct  ⚕️\",\n",
       "\t target=\"0\",\n",
       "\t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "\t text=\"hide new secretions from the parental units  ⚕️\",\n",
       "\t target=\"0\",\n",
       "\t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "\t text=\"hide new secretions from the parental units \",\n",
       "\t target=\"0\",\n",
       "\t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]'), (1, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transform(transform_history):\n",
    "    transform_dict = ast.literal_eval(transform_history)\n",
    "    transform_name = transform_dict.pop(\"class\")\n",
    "    transform_fn = globals()[transform_name](**transform_dict)\n",
    "    return transform_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_record = records[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = text[0]\n",
    "target_text = target_record.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_history = target_record.le_attrs['transformation_provenance'].history\n",
    "tran_history = [x[1] for x in sorted(tran_history, key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\",\n",
       " \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string on line 1: <ast.Name object at 0x0000021E6EE92EC0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21672\\1093018184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtran_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtran_history\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21672\\1093018184.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtran_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtran_history\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21672\\4021414076.py\u001b[0m in \u001b[0;36minit_transform\u001b[1;34m(transform_history)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtransform_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtransform_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtransform_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransform_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtransform_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0m_raise_malformed_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             return dict(zip(map(_convert, node.keys),\n\u001b[0m\u001b[0;32m     98\u001b[0m                             map(_convert, node.values)))\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinOp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moperand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0m_raise_malformed_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_raise_malformed_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlno\u001b[0m \u001b[1;33m:=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lineno'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' on line {lno}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf': {node!r}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string on line 1: <ast.Name object at 0x0000021E6EE92EC0>"
     ]
    }
   ],
   "source": [
    "tran_fns = [init_transform(h) for h in tran_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string on line 1: <ast.Name object at 0x0000021E6EE911B0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21672\\731032356.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransform_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0m_raise_malformed_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             return dict(zip(map(_convert, node.keys),\n\u001b[0m\u001b[0;32m     98\u001b[0m                             map(_convert, node.values)))\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBinOp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mleft\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moperand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0m_raise_malformed_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_signed_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\ast.py\u001b[0m in \u001b[0;36m_raise_malformed_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlno\u001b[0m \u001b[1;33m:=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lineno'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' on line {lno}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf': {node!r}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string on line 1: <ast.Name object at 0x0000021E6EE911B0>"
     ]
    }
   ],
   "source": [
    "transform_dict = ast.literal_eval(transforms[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_name = transform_dict.pop(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_fn = globals()[tran_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sibyl.transformations.text.emoji.emojify.AddNeutralEmoji at 0x21e6ee90e50>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tran_fn(**transform_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dpml",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
