{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4bb3fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1476be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pth = './results_a2t_sst2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cdf0387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>result_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it 's a charming and often affecting journey .</td>\n",
       "      <td>it 's a charming and normally affecting journe...</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unflinchingly bleak and desperate</td>\n",
       "      <td>unflinchingly grim and desperate</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allows us to hope that nolan is poised to emba...</td>\n",
       "      <td>authorizes us to hopes that nolan is prepped t...</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the acting , costumes , music , cinematography...</td>\n",
       "      <td>the acting , costumes , music , cinematography...</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it 's slow -- very , very slow .</td>\n",
       "      <td>it 's slow -- very , very slower .</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>has all the depth of a wading pool .</td>\n",
       "      <td>has all the depths of a wading pool .</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>a movie with a real anarchic flair .</td>\n",
       "      <td>a films with a true anarchic flair .</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>a subject like this should inspire reaction in...</td>\n",
       "      <td>a subject like this should inspiring reply in ...</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>... is an arthritic attempt at directing by ca...</td>\n",
       "      <td>... is an arthritic endeavour at directing by ...</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>looking aristocratic , luminous yet careworn i...</td>\n",
       "      <td>looking aristocratic , luminous yet careworn i...</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         original_text  \\\n",
       "0      it 's a charming and often affecting journey .    \n",
       "1                   unflinchingly bleak and desperate    \n",
       "2    allows us to hope that nolan is poised to emba...   \n",
       "3    the acting , costumes , music , cinematography...   \n",
       "4                    it 's slow -- very , very slow .    \n",
       "..                                                 ...   \n",
       "867              has all the depth of a wading pool .    \n",
       "868              a movie with a real anarchic flair .    \n",
       "869  a subject like this should inspire reaction in...   \n",
       "870  ... is an arthritic attempt at directing by ca...   \n",
       "871  looking aristocratic , luminous yet careworn i...   \n",
       "\n",
       "                                        perturbed_text result_type  \n",
       "0    it 's a charming and normally affecting journe...      Failed  \n",
       "1                    unflinchingly grim and desperate       Failed  \n",
       "2    authorizes us to hopes that nolan is prepped t...      Failed  \n",
       "3    the acting , costumes , music , cinematography...  Successful  \n",
       "4                  it 's slow -- very , very slower .       Failed  \n",
       "..                                                 ...         ...  \n",
       "867             has all the depths of a wading pool .   Successful  \n",
       "868              a films with a true anarchic flair .       Failed  \n",
       "869  a subject like this should inspiring reply in ...  Successful  \n",
       "870  ... is an arthritic endeavour at directing by ...  Successful  \n",
       "871  looking aristocratic , luminous yet careworn i...  Successful  \n",
       "\n",
       "[872 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = pd.read_csv(osp.join(result_pth,'log.csv'))\n",
    "out_df = log_df[['original_text', 'perturbed_text', 'result_type']]\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c32b2af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>original_score</th>\n",
       "      <th>perturbed_score</th>\n",
       "      <th>original_output</th>\n",
       "      <th>perturbed_output</th>\n",
       "      <th>ground_truth_output</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>result_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it 's a charming and often affecting journey .</td>\n",
       "      <td>it 's a charming and normally affecting journe...</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unflinchingly bleak and desperate</td>\n",
       "      <td>unflinchingly grim and desperate</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.154515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allows us to hope that nolan is poised to emba...</td>\n",
       "      <td>authorizes us to hopes that nolan is prepped t...</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.283926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the acting , costumes , music , cinematography...</td>\n",
       "      <td>the acting , costumes , music , cinematography...</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.622039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it 's slow -- very , very slow .</td>\n",
       "      <td>it 's slow -- very , very slower .</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>has all the depth of a wading pool .</td>\n",
       "      <td>has all the depths of a wading pool .</td>\n",
       "      <td>0.185487</td>\n",
       "      <td>0.851692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>a movie with a real anarchic flair .</td>\n",
       "      <td>a films with a true anarchic flair .</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>a subject like this should inspire reaction in...</td>\n",
       "      <td>a subject like this should inspiring reply in ...</td>\n",
       "      <td>0.136275</td>\n",
       "      <td>0.518427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>... is an arthritic attempt at directing by ca...</td>\n",
       "      <td>... is an arthritic endeavour at directing by ...</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.734410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>looking aristocratic , luminous yet careworn i...</td>\n",
       "      <td>looking aristocratic , luminous yet careworn i...</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>0.820684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         original_text  \\\n",
       "0      it 's a charming and often affecting journey .    \n",
       "1                   unflinchingly bleak and desperate    \n",
       "2    allows us to hope that nolan is poised to emba...   \n",
       "3    the acting , costumes , music , cinematography...   \n",
       "4                    it 's slow -- very , very slow .    \n",
       "..                                                 ...   \n",
       "867              has all the depth of a wading pool .    \n",
       "868              a movie with a real anarchic flair .    \n",
       "869  a subject like this should inspire reaction in...   \n",
       "870  ... is an arthritic attempt at directing by ca...   \n",
       "871  looking aristocratic , luminous yet careworn i...   \n",
       "\n",
       "                                        perturbed_text  original_score  \\\n",
       "0    it 's a charming and normally affecting journe...        0.000234   \n",
       "1                    unflinchingly grim and desperate         0.014329   \n",
       "2    authorizes us to hopes that nolan is prepped t...        0.000426   \n",
       "3    the acting , costumes , music , cinematography...        0.003972   \n",
       "4                  it 's slow -- very , very slower .         0.002048   \n",
       "..                                                 ...             ...   \n",
       "867             has all the depths of a wading pool .         0.185487   \n",
       "868              a films with a true anarchic flair .         0.000322   \n",
       "869  a subject like this should inspiring reply in ...        0.136275   \n",
       "870  ... is an arthritic endeavour at directing by ...        0.003134   \n",
       "871  looking aristocratic , luminous yet careworn i...        0.013619   \n",
       "\n",
       "     perturbed_score  original_output  perturbed_output  ground_truth_output  \\\n",
       "0           0.000557              1.0               1.0                  1.0   \n",
       "1           0.154515              0.0               0.0                  0.0   \n",
       "2           0.283926              1.0               1.0                  1.0   \n",
       "3           0.622039              1.0               0.0                  1.0   \n",
       "4           0.002501              0.0               0.0                  0.0   \n",
       "..               ...              ...               ...                  ...   \n",
       "867         0.851692              0.0               1.0                  0.0   \n",
       "868         0.000818              1.0               1.0                  1.0   \n",
       "869         0.518427              0.0               1.0                  0.0   \n",
       "870         0.734410              0.0               1.0                  0.0   \n",
       "871         0.820684              1.0               0.0                  1.0   \n",
       "\n",
       "     num_queries result_type  \n",
       "0           14.0      Failed  \n",
       "1            5.0      Failed  \n",
       "2           32.0      Failed  \n",
       "3            8.0  Successful  \n",
       "4            4.0      Failed  \n",
       "..           ...         ...  \n",
       "867          2.0  Successful  \n",
       "868          5.0      Failed  \n",
       "869         13.0  Successful  \n",
       "870         11.0  Successful  \n",
       "871          9.0  Successful  \n",
       "\n",
       "[872 rows x 9 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7446d576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transformation</th>\n",
       "      <th>prev_text</th>\n",
       "      <th>after_text</th>\n",
       "      <th>prev_target</th>\n",
       "      <th>after_target</th>\n",
       "      <th>from_modified_indices</th>\n",
       "      <th>to_modified_indices</th>\n",
       "      <th>changes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transformation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>1982</td>\n",
       "      <td>2095</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>1982</td>\n",
       "      <td>2096</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>1982</td>\n",
       "      <td>2097</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>1982</td>\n",
       "      <td>2098</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>1982</td>\n",
       "      <td>2099</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2080 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      transformation  \\\n",
       "transformation_id                                                      \n",
       "0                  {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "1                  {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "2                  {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "3                  {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "4                  {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "...                                                              ...   \n",
       "2075               {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "2076               {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "2077               {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "2078               {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "2079               {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "\n",
       "                   prev_text  after_text  prev_target  after_target  \\\n",
       "transformation_id                                                     \n",
       "0                          0           1           -1            -1   \n",
       "1                          2           3           -1            -1   \n",
       "2                          2           4           -1            -1   \n",
       "3                          2           5           -1            -1   \n",
       "4                          2           6           -1            -1   \n",
       "...                      ...         ...          ...           ...   \n",
       "2075                    1982        2095           -1            -1   \n",
       "2076                    1982        2096           -1            -1   \n",
       "2077                    1982        2097           -1            -1   \n",
       "2078                    1982        2098           -1            -1   \n",
       "2079                    1982        2099           -1            -1   \n",
       "\n",
       "                  from_modified_indices to_modified_indices  \\\n",
       "transformation_id                                             \n",
       "0                                   {3}                 {3}   \n",
       "1                                   {3}                 {3}   \n",
       "2                                   {3}                 {3}   \n",
       "3                                   {3}                 {3}   \n",
       "4                                   {3}                 {3}   \n",
       "...                                 ...                 ...   \n",
       "2075                               {15}                {15}   \n",
       "2076                               {15}                {15}   \n",
       "2077                               {15}                {15}   \n",
       "2078                               {15}                {15}   \n",
       "2079                               {15}                {15}   \n",
       "\n",
       "                                        changes  \n",
       "transformation_id                                \n",
       "0                      ['replace: [3,4]-[3,4]']  \n",
       "1                      ['replace: [3,4]-[3,4]']  \n",
       "2                      ['replace: [3,4]-[3,4]']  \n",
       "3                      ['replace: [3,4]-[3,4]']  \n",
       "4                      ['replace: [3,4]-[3,4]']  \n",
       "...                                         ...  \n",
       "2075               ['replace: [15,16]-[15,16]']  \n",
       "2076               ['replace: [15,16]-[15,16]']  \n",
       "2077               ['replace: [15,16]-[15,16]']  \n",
       "2078               ['replace: [15,16]-[15,16]']  \n",
       "2079               ['replace: [15,16]-[15,16]']  \n",
       "\n",
       "[2080 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformation_log = pd.read_csv('../results/transformation.csv', index_col=0, names = [\"transformation_id\",\"transformation\",\n",
    "            \"prev_text\", \"after_text\", \"prev_target\", \"after_target\",\"from_modified_indices\", \"to_modified_indices\", \"changes\"])\n",
    "transformation_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b786302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>transformation</th>\n",
       "      <th>prev_target</th>\n",
       "      <th>after_target</th>\n",
       "      <th>from_modified_indices</th>\n",
       "      <th>to_modified_indices</th>\n",
       "      <th>changes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prev_text</th>\n",
       "      <th>after_text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>3</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{3}</td>\n",
       "      <td>{3}</td>\n",
       "      <td>['replace: [3,4]-[3,4]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1982</th>\n",
       "      <th>2095</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>{'class': 'WordSwapEmbedding', 'max_candidates...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>{15}</td>\n",
       "      <td>{15}</td>\n",
       "      <td>['replace: [15,16]-[15,16]']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2080 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         transformation  \\\n",
       "prev_text after_text                                                      \n",
       "0         1           {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "2         3           {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          4           {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          5           {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          6           {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "...                                                                 ...   \n",
       "1982      2095        {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          2096        {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          2097        {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          2098        {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "          2099        {'class': 'WordSwapEmbedding', 'max_candidates...   \n",
       "\n",
       "                      prev_target  after_target from_modified_indices  \\\n",
       "prev_text after_text                                                    \n",
       "0         1                    -1            -1                   {3}   \n",
       "2         3                    -1            -1                   {3}   \n",
       "          4                    -1            -1                   {3}   \n",
       "          5                    -1            -1                   {3}   \n",
       "          6                    -1            -1                   {3}   \n",
       "...                           ...           ...                   ...   \n",
       "1982      2095                 -1            -1                  {15}   \n",
       "          2096                 -1            -1                  {15}   \n",
       "          2097                 -1            -1                  {15}   \n",
       "          2098                 -1            -1                  {15}   \n",
       "          2099                 -1            -1                  {15}   \n",
       "\n",
       "                     to_modified_indices                       changes  \n",
       "prev_text after_text                                                    \n",
       "0         1                          {3}      ['replace: [3,4]-[3,4]']  \n",
       "2         3                          {3}      ['replace: [3,4]-[3,4]']  \n",
       "          4                          {3}      ['replace: [3,4]-[3,4]']  \n",
       "          5                          {3}      ['replace: [3,4]-[3,4]']  \n",
       "          6                          {3}      ['replace: [3,4]-[3,4]']  \n",
       "...                                  ...                           ...  \n",
       "1982      2095                      {15}  ['replace: [15,16]-[15,16]']  \n",
       "          2096                      {15}  ['replace: [15,16]-[15,16]']  \n",
       "          2097                      {15}  ['replace: [15,16]-[15,16]']  \n",
       "          2098                      {15}  ['replace: [15,16]-[15,16]']  \n",
       "          2099                      {15}  ['replace: [15,16]-[15,16]']  \n",
       "\n",
       "[2080 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_to_transformation = transformation_log.set_index(['prev_text', 'after_text'])\n",
    "edge_to_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f5bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = transformation_log[['prev_text','after_text']]\n",
    "forward_edges_df = edges_df.set_index('prev_text')\n",
    "backward_edges_df = edges_df.set_index('after_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ce4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_text = pd.read_csv('../results/text.csv', index_col=\"text_id\", names = [\"text_id\", \"text\"])\n",
    "text_to_id = pd.read_csv('../results/text.csv', index_col=\"text\", names = [\"text_id\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f88cf",
   "metadata": {},
   "source": [
    "## Transformation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2173446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 12:47:23.458798: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from lineage import InferQuery\n",
    "from collections import defaultdict\n",
    "from textattack.shared.utils import words_from_text\n",
    "queryAPI = InferQuery(dir_pth='./results_a2t_sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9524b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2504,\n",
       "  \"nicks , seemingly uncertain what 's going to make people laugh , runs the gamut from stale parody to raunchy sex gags to formula romantic comedy . \",\n",
       "  0.0),\n",
       " (\"{'class': 'WordSwapEmbedding', 'max_candidates': 20, 'embedding': WordEmbedding}\",\n",
       "  '{21}',\n",
       "  '{21}'),\n",
       " [('replace', (21, 22), (21, 22))],\n",
       " (2505,\n",
       "  \"nicks , seemingly uncertain what 's going to make people laugh , runs the gamut from stale parody to raunchy sex gags to formulas romantic comedy . \",\n",
       "  0.0),\n",
       " (\"{'class': 'WordSwapEmbedding', 'max_candidates': 20, 'embedding': WordEmbedding}\",\n",
       "  '{14}',\n",
       "  '{14}'),\n",
       " [('replace', (14, 15), (14, 15))],\n",
       " (2529,\n",
       "  \"nicks , seemingly uncertain what 's going to make people laugh , runs the gamut from archaic parody to raunchy sex gags to formulas romantic comedy . \",\n",
       "  0.0),\n",
       " (\"{'class': 'WordSwapEmbedding', 'max_candidates': 20, 'embedding': WordEmbedding}\",\n",
       "  '{12}',\n",
       "  '{12}'),\n",
       " [('replace', (12, 13), (12, 13))],\n",
       " (2545,\n",
       "  \"nicks , seemingly uncertain what 's going to make people laugh , runs the spectrum from archaic parody to raunchy sex gags to formulas romantic comedy . \",\n",
       "  1.0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_trace = queryAPI.get_trace_of_output_idx(25)\n",
    "api_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b12a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_traces = queryAPI.get_traces_of_all_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2854504b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, \"it 's a charming and often affecting journey . \", 1.0),\n",
       " (\"{'class': 'WordSwapEmbedding', 'max_candidates': 20, 'embedding': WordEmbedding}\",\n",
       "  '{5}',\n",
       "  '{5}'),\n",
       " [('replace', (5, 6), (5, 6))],\n",
       " (28, \"it 's a charming and normally affecting journey . \", 1.0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_traces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7ba2c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623c753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to stopwords...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon','stopwords')\n",
    "\n",
    "pos_list=set(opinion_lexicon.positive())\n",
    "neg_list=set(opinion_lexicon.negative())\n",
    "stop_words = set(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb43fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment_category(word):\n",
    "    if  word in stop_words:\n",
    "        return 'exclude'\n",
    "    elif word in pos_list:\n",
    "        return 'pos'\n",
    "    elif word in neg_list:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8c7a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edits = {}\n",
    "all_texts = {}\n",
    "for record in all_traces:\n",
    "    for i,step in enumerate(record):\n",
    "        if isinstance(step, list):\n",
    "            for edit in step:\n",
    "                op = edit[0]\n",
    "                from_span = edit[1]\n",
    "                to_span = edit[2]\n",
    "                from_text = tuple(words_from_text(record[i-2][1])[from_span[0]:from_span[1]])\n",
    "                to_text = tuple(words_from_text(record[i+1][1])[to_span[0]:to_span[1]])\n",
    "                \n",
    "                from_text_id = record[i-2][0]\n",
    "                to_text_id = record[i+1][0]\n",
    "                all_texts[record[i-2][0]] = record[i-2][1]\n",
    "                all_texts[record[i+1][0]] = record[i+1][1]\n",
    "                \n",
    "                from_label = record[i-2][2]\n",
    "                to_label = record[i+1][2]\n",
    "                \n",
    "                final_text_id = record[-1][0]\n",
    "                \n",
    "                # to category\n",
    "                from_cat = to_sentiment_category(from_text[0])\n",
    "                to_cat = to_sentiment_category(to_text[0])\n",
    "                \n",
    "                result_type = record[i+1][2]\n",
    "                if op not in all_edits:\n",
    "                    all_edits[op] = defaultdict(dict)\n",
    "                if to_cat not in all_edits[op][from_cat]:\n",
    "                    all_edits[op][from_cat][to_cat]={}\n",
    "                if (from_label, to_label) not in all_edits[op][from_cat][to_cat]:\n",
    "                    all_edits[op][from_cat][to_cat][(from_label, to_label)] = []\n",
    "                    \n",
    "                all_edits[op][from_cat][to_cat][(from_label, to_label)].append((from_text, to_text, from_text_id, to_text_id, final_text_id)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c59a3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_freqs = defaultdict(list)\n",
    "for op in all_edits.keys():\n",
    "    for from_cat in all_edits[op].keys():\n",
    "        for to_cat in all_edits[op][from_cat].keys():\n",
    "            for label_pair in all_edits[op][from_cat][to_cat].keys():\n",
    "                edit_freqs[label_pair].append((len(all_edits[op][from_cat][to_cat][label_pair]), op, from_cat, to_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2687f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 1.0)\n",
      "\t(659, 'replace', 'neutral', 'neutral')\n",
      "\t(134, 'replace', 'pos', 'neutral')\n",
      "\t(106, 'replace', 'pos', 'pos')\n",
      "\t(55, 'replace', 'neg', 'neg')\n",
      "\t(42, 'replace', 'neg', 'neutral')\n",
      "\t(30, 'replace', 'neutral', 'neg')\n",
      "\t(29, 'replace', 'neutral', 'pos')\n",
      "\t(18, 'replace', 'pos', 'neg')\n",
      "\t(7, 'replace', 'neutral', 'exclude')\n",
      "\t(2, 'replace', 'neg', 'pos')\n",
      "(1.0, 0.0)\n",
      "\t(52, 'replace', 'neutral', 'neutral')\n",
      "\t(16, 'replace', 'neg', 'neg')\n",
      "\t(10, 'replace', 'pos', 'pos')\n",
      "\t(8, 'replace', 'pos', 'neutral')\n",
      "\t(7, 'replace', 'neutral', 'neg')\n",
      "\t(3, 'replace', 'neg', 'neutral')\n",
      "\t(2, 'replace', 'neutral', 'exclude')\n",
      "\t(1, 'replace', 'pos', 'neg')\n",
      "(0.0, 0.0)\n",
      "\t(586, 'replace', 'neutral', 'neutral')\n",
      "\t(124, 'replace', 'neg', 'neg')\n",
      "\t(60, 'replace', 'neg', 'neutral')\n",
      "\t(50, 'replace', 'pos', 'pos')\n",
      "\t(36, 'replace', 'neutral', 'pos')\n",
      "\t(32, 'replace', 'pos', 'neutral')\n",
      "\t(18, 'replace', 'neutral', 'neg')\n",
      "\t(13, 'replace', 'neg', 'pos')\n",
      "\t(9, 'replace', 'neutral', 'exclude')\n",
      "\t(1, 'replace', 'pos', 'neg')\n",
      "(0.0, 1.0)\n",
      "\t(55, 'replace', 'neutral', 'neutral')\n",
      "\t(7, 'replace', 'neutral', 'pos')\n",
      "\t(7, 'replace', 'neg', 'neutral')\n",
      "\t(7, 'replace', 'neg', 'neg')\n",
      "\t(6, 'replace', 'pos', 'pos')\n",
      "\t(4, 'replace', 'pos', 'neutral')\n",
      "\t(2, 'replace', 'neutral', 'neg')\n",
      "\t(2, 'replace', 'neg', 'pos')\n",
      "\t(1, 'replace', 'pos', 'neg')\n",
      "\t(1, 'replace', 'neutral', 'exclude')\n"
     ]
    }
   ],
   "source": [
    "for label_pair in edit_freqs.keys():\n",
    "    print(label_pair)\n",
    "    edit_freqs[label_pair] = sorted(edit_freqs[label_pair], reverse=True)\n",
    "    for edit in edit_freqs[label_pair]:\n",
    "        print(f\"\\t{edit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0161b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 0.0)\n",
      "\t('replace', 'neg', 'neg'), 317.94%\n",
      "\t('replace', 'neg', 'neutral'), 78.07%\n",
      "\t('replace', 'neutral', 'exclude'), 312.27%\n",
      "\t('replace', 'neutral', 'neg'), 255.02%\n",
      "\t('replace', 'neutral', 'neutral'), 86.24%\n",
      "\t('replace', 'pos', 'neg'), 60.72%\n",
      "\t('replace', 'pos', 'neutral'), 65.25%\n",
      "\t('replace', 'pos', 'pos'), 103.11%\n",
      "(0.0, 1.0)\n",
      "\t('replace', 'neg', 'neg'), 57.00%\n",
      "\t('replace', 'neg', 'neutral'), 117.81%\n",
      "\t('replace', 'neg', 'pos'), 155.35%\n",
      "\t('replace', 'neutral', 'exclude'), 112.20%\n",
      "\t('replace', 'neutral', 'neg'), 112.20%\n",
      "\t('replace', 'neutral', 'neutral'), 94.77%\n",
      "\t('replace', 'neutral', 'pos'), 196.35%\n",
      "\t('replace', 'pos', 'neg'), 1009.78%\n",
      "\t('replace', 'pos', 'neutral'), 126.22%\n",
      "\t('replace', 'pos', 'pos'), 121.17%\n"
     ]
    }
   ],
   "source": [
    "label_percentage = {}\n",
    "for label_pair in edit_freqs.keys():\n",
    "    freq_sum = sum([tup[0] for tup in edit_freqs[label_pair]])\n",
    "    edits_dict = {}\n",
    "    for edit in edit_freqs[label_pair]:\n",
    "        edits_dict[edit[1:]] = edit[0]/freq_sum * 100\n",
    "    label_percentage[label_pair] = edits_dict\n",
    "        \n",
    "for label_pair in label_percentage.keys():\n",
    "    if label_pair[0] != label_pair[1]:\n",
    "        print(label_pair)\n",
    "        original_pair = (label_pair[0], label_pair[0])\n",
    "        for edit in sorted(label_percentage[label_pair].keys()):\n",
    "            cur_percent = label_percentage[label_pair][edit]\n",
    "            if edit in label_percentage[original_pair]:\n",
    "                original_percent = label_percentage[original_pair][edit]\n",
    "                print(f\"\\t{edit}, {cur_percent / original_percent * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9c933c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('replace', 'neutral', 'neutral'): 59.78260869565217,\n",
       " ('replace', 'neutral', 'pos'): 7.608695652173914,\n",
       " ('replace', 'neg', 'neutral'): 7.608695652173914,\n",
       " ('replace', 'neg', 'neg'): 7.608695652173914,\n",
       " ('replace', 'pos', 'pos'): 6.521739130434782,\n",
       " ('replace', 'pos', 'neutral'): 4.3478260869565215,\n",
       " ('replace', 'neutral', 'neg'): 2.1739130434782608,\n",
       " ('replace', 'neg', 'pos'): 2.1739130434782608,\n",
       " ('replace', 'pos', 'neg'): 1.0869565217391304,\n",
       " ('replace', 'neutral', 'exclude'): 1.0869565217391304}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_percentage[(0.0,1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "124af01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    PURPLE = '\\033[95m'\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fa7799a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i had to look away - this was \u001b[92mgod\u001b[0m horrendous . \n",
      "i had to look away - this was \u001b[91mgoodness\u001b[0m horrendous . \n",
      "\n",
      "the title not only depicts its \u001b[92mmain\u001b[0m characters , but the lazy people behind the camera as well . \n",
      "the title not only depicts its \u001b[91mleading\u001b[0m characters , but the lazy people behind the camera as well . \n",
      "\n",
      "for all its amazing craftsmanship , and despite an overbearing series of third-act crescendos , lily chou-chou never \u001b[92mreally\u001b[0m builds up a head of emotional vapour . \n",
      "for all its amazing craftsmanship , and despite an overbearing series of third-act crescendos , lily chou-chou never \u001b[91mtruthfully\u001b[0m builds up a head of emotional vapour . \n",
      "\n",
      "on the whole , the cinema lacked wit , \u001b[92mfeeling\u001b[0m and believability to compensate for its incessant coarseness and banality . \n",
      "on the whole , the cinema lacked wit , \u001b[91msensation\u001b[0m and believability to compensate for its incessant coarseness and banality . \n",
      "\n",
      "it all drags on so interminably it 's like watching a sorrowful relationship unfold in \u001b[92mreal\u001b[0m time . \n",
      "it all drags on so interminably it 's like watching a sorrowful relationship unfold in \u001b[91mgenuine\u001b[0m time . \n",
      "\n",
      "the finest that can be said about the work here of scottish director ritchie ... is that he \u001b[92mobviously\u001b[0m does n't have his heart in it . \n",
      "the finest that can be said about the work here of scottish director ritchie ... is that he \u001b[91massuredly\u001b[0m does n't have his heart in it . \n",
      "\n",
      "\u001b[92muncommonly\u001b[0m stylish but similarly silly ... the picture fails to begets much wait , nor does it ask searching enough questions to justify its pretensions . \n",
      "\u001b[91mtremendously\u001b[0m stylish but similarly silly ... the picture fails to begets much wait , nor does it ask searching enough questions to justify its pretensions . \n",
      "\n",
      "not \u001b[92mexactly\u001b[0m the bees knees \n",
      "not \u001b[91mprecisely\u001b[0m the bees knees \n",
      "\n",
      "may reawaken discussion of the kennedy assassination but this fictional film looks \u001b[92mmade\u001b[0m for cables rather than for the big screen . \n",
      "may reawaken discussion of the kennedy assassination but this fictional film looks \u001b[91maccomplished\u001b[0m for cables rather than for the big screen . \n",
      "\n",
      "does n't offer much besides glib soullessness , raunchy language and a series of ferocious set pieces ... that \u001b[92mraise\u001b[0m the solicitor on sleek screen violence . \n",
      "does n't offer much besides glib soullessness , raunchy language and a series of ferocious set pieces ... that \u001b[91melevate\u001b[0m the solicitor on sleek screen violence . \n",
      "\n",
      "it 's a wicked stuff when a movie has about as much substance as its end \u001b[92mcredits\u001b[0m blooper reel . \n",
      "it 's a wicked stuff when a movie has about as much substance as its end \u001b[91mcredence\u001b[0m blooper reel . \n",
      "\n",
      "a misogynistic slice of filth that endeavors to pass itself off as hip , young \u001b[92madult\u001b[0m entertainment . \n",
      "a misogynistic slice of filth that endeavors to pass itself off as hip , young \u001b[91mmature\u001b[0m entertainment . \n",
      "\n",
      "a misogynistic slice of filth that endeavors to pass itself off as hip , \u001b[92myoung\u001b[0m mature entertainment . \n",
      "a misogynistic slice of filth that endeavors to pass itself off as hip , \u001b[91myouthful\u001b[0m mature entertainment . \n",
      "\n",
      "\u001b[92mperceptive\u001b[0m in its vision of nascent industrialized world politics as a new art form , but far too clunky , didactic and saddled with scenes that seem simply an indisposed fit for this movie . \n",
      "\u001b[91minsightful\u001b[0m in its vision of nascent industrialized world politics as a new art form , but far too clunky , didactic and saddled with scenes that seem simply an indisposed fit for this movie . \n",
      "\n",
      "no one but a convict guilty of some \u001b[92mtruly\u001b[0m heinous crime should have to seated through the master of disguises . \n",
      "no one but a convict guilty of some \u001b[91mtruthfully\u001b[0m heinous crime should have to seated through the master of disguises . \n",
      "\n",
      "a silly movie with dumb characters doing dumb things and you have to be \u001b[92mreally\u001b[0m stupid not to see where this is going . \n",
      "a silly movie with dumb characters doing dumb things and you have to be \u001b[91mtruthfully\u001b[0m stupid not to see where this is going . \n",
      "\n",
      "it 's like watching a nightmare \u001b[92mmade\u001b[0m flesh . \n",
      "it 's like watching a nightmare \u001b[91maccomplished\u001b[0m flesh . \n",
      "\n",
      "serving sara does n't serve up a whole lot of \u001b[92mlaughs\u001b[0m . \n",
      "serving sara does n't serve up a whole lot of \u001b[91mgrin\u001b[0m . \n",
      "\n",
      "... a monotonous parades of talking heads and technical gibberish that will do little to \u001b[92madvance\u001b[0m the linux cause . \n",
      "... a monotonous parades of talking heads and technical gibberish that will do little to \u001b[91mheadway\u001b[0m the linux cause . \n",
      "\n",
      "his comedy premises are traditionally hackneyed or just plain crude , calculating to provoke amazed \u001b[92mlaughter\u001b[0m , without following up on a deeper level . \n",
      "his comedy premises are traditionally hackneyed or just plain crude , calculating to provoke amazed \u001b[91msmile\u001b[0m , without following up on a deeper level . \n",
      "\n",
      "not since freddy got fingered has a major \u001b[92mrelease\u001b[0m been so agonizing to sit through . \n",
      "not since freddy got fingered has a major \u001b[91mfreed\u001b[0m been so agonizing to sit through . \n",
      "\n",
      "trite , banal , cliched , principally \u001b[92minoffensive\u001b[0m . \n",
      "trite , banal , cliched , principally \u001b[91minnocuous\u001b[0m . \n",
      "\n",
      "the notion that bombing buildings is the funniest thing in the world goes entirely unexamined in this startlingly unfunny \u001b[92mcomedy\u001b[0m . \n",
      "the notion that bombing buildings is the funniest thing in the world goes entirely unexamined in this startlingly unfunny \u001b[91mhumour\u001b[0m . \n",
      "\n",
      "a sequel that 's much too \u001b[92mbig\u001b[0m for its britches . \n",
      "a sequel that 's much too \u001b[91mimmense\u001b[0m for its britches . \n",
      "\n",
      "care deftly captures the wonder and menace of growing up , but he never \u001b[92mreally\u001b[0m embraces the joy of fuhrman 's destructive escapism or the grace-in-rebellion found by his characters . \n",
      "care deftly captures the wonder and menace of growing up , but he never \u001b[91mtruthfully\u001b[0m embraces the joy of fuhrman 's destructive escapism or the grace-in-rebellion found by his characters . \n",
      "\n",
      "it has its times of swaggering camaraderie , but more traditionally just feels generic , derivative and \u001b[92mdone\u001b[0m to mortality . \n",
      "it has its times of swaggering camaraderie , but more traditionally just feels generic , derivative and \u001b[91maccomplished\u001b[0m to mortality . \n",
      "\n",
      "a disillusion for those who loved substituting versions of the bard , \u001b[92mparticularly\u001b[0m ones that involve deep fryers and hamburgers . \n",
      "a disillusion for those who loved substituting versions of the bard , \u001b[91mnotably\u001b[0m ones that involve deep fryers and hamburgers . \n",
      "\n",
      "it 's so mediocre , despite the dynamic duo on the marquee , that we just ca n't get no \u001b[92msatisfaction\u001b[0m . \n",
      "it 's so mediocre , despite the dynamic duo on the marquee , that we just ca n't get no \u001b[91mgratification\u001b[0m . \n",
      "\n",
      "from the opening scenes , it 's clear that all about the benjamins is a \u001b[92mtotally\u001b[0m formulaic movie . \n",
      "from the opening scenes , it 's clear that all about the benjamins is a \u001b[91mperfectly\u001b[0m formulaic movie . \n",
      "\n",
      "it can not be adored , even on the level that one enjoy a bad slasher flick , \u001b[92mprimarily\u001b[0m because it is drab . \n",
      "it can not be adored , even on the level that one enjoy a bad slasher flick , \u001b[91mnotably\u001b[0m because it is drab . \n",
      "\n",
      "a hamfisted romantic \u001b[92mcomedy\u001b[0m that makes our girl the hapless facilitator of an extended inexpensive shot across the mason-dixon line . \n",
      "a hamfisted romantic \u001b[91mhumor\u001b[0m that makes our girl the hapless facilitator of an extended inexpensive shot across the mason-dixon line . \n",
      "\n",
      "burns never \u001b[92mreally\u001b[0m harnessing to full effect the energetic cast . \n",
      "burns never \u001b[91mtruthfully\u001b[0m harnessing to full effect the energetic cast . \n",
      "\n",
      "in the end , the movie collapsed on its tenuous \u001b[92mfoundation\u001b[0m despite the best efforts of director joe carnahan . \n",
      "in the end , the movie collapsed on its tenuous \u001b[91mcornerstone\u001b[0m despite the best efforts of director joe carnahan . \n",
      "\n",
      "whaley 's determination to immerse you in \u001b[92msheer\u001b[0m , unrelenting wretchedness is exhausting . \n",
      "whaley 's determination to immerse you in \u001b[91mpure\u001b[0m , unrelenting wretchedness is exhausting . \n",
      "\n",
      "it 's not original , and , robbed of the element of surprise , it does n't have any huge \u001b[92mlaughs\u001b[0m in its story of irresponsible cops who love to play pranks . \n",
      "it 's not original , and , robbed of the element of surprise , it does n't have any huge \u001b[91mgrin\u001b[0m in its story of irresponsible cops who love to play pranks . \n",
      "\n",
      "given how heavy-handed and portent-heavy it is , this did be the hardest thing soderbergh has ever \u001b[92mdone\u001b[0m . \n",
      "given how heavy-handed and portent-heavy it is , this did be the hardest thing soderbergh has ever \u001b[91maccomplished\u001b[0m . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reversed_texts = []\n",
    "for edit in all_edits['replace']['neutral']['pos'][(0.0, 0.0)]:\n",
    "    before_text = all_texts[edit[2]]\n",
    "    left = before_text.find(edit[0][0])\n",
    "    right = left + len(edit[0][0])\n",
    "    \n",
    "    print(before_text[:left] + bcolors.GREEN + before_text[left:right] +bcolors.ENDC + before_text[right:])\n",
    "\n",
    "    after_text = all_texts[edit[3]]\n",
    "    left = after_text.find(edit[1][0])\n",
    "    right = left + len(edit[1][0])\n",
    "    \n",
    "    print(after_text[:left] + bcolors.RED + after_text[left:right] +bcolors.ENDC + after_text[right:])\n",
    "    print()\n",
    "\n",
    "    final_text = all_texts[edit[4]]\n",
    "    reverted_text = final_text.replace(edit[1][0], edit[0][0])\n",
    "    reversed_texts.append(reverted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fe5665a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "model.eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6c04bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef0b4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = []\n",
    "for text in reversed_texts:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "      text,\n",
    "      max_length=160,\n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',  # Return PyTorch tensors\n",
    "    )\n",
    "    output = model(\n",
    "      input_ids=encoding['input_ids'],\n",
    "      attention_mask=encoding['attention_mask']\n",
    "    )\n",
    "    new_results.append(torch.argmax(output.logits, dim=1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c4aef133",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = np.array(new_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a9de37c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(new_results > 0) / len(new_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ada933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpml",
   "language": "python",
   "name": "dpml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
