{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuWi1REPD_Ux"
   },
   "source": [
    "# DPML | Latency Replay\n",
    "\n",
    "In this notebook, we investigate the reproducibility of transformation sequences captured by `dpml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Zs0hYFFIYf"
   },
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8_koamxFHfR",
    "outputId": "e892403a-70b8-4728-ec18-c31ee7e81580"
   },
   "outputs": [],
   "source": [
    "from lineage import LeBatch\n",
    "\n",
    "from sibyl import *\n",
    "from datasets import load_dataset\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktCv--paFOqG"
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\fabri\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:100]\")\n",
    "dataset = dataset.rename_column('sentence', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to be Tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = SibylTransformScheduler(\"sentiment\", num_INV = 4, num_SIB = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9546a2ff8840a58cf3b2dc0ba922ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sibyl.transformations.text.word_swap.change_number.ChangeNumber object at 0x00000215C7BDBD60>\n",
      "<sibyl.transformations.text.emoji.demojify.Demojify object at 0x000002161E0F8700>\n",
      "<sibyl.transformations.text.typos.char_swap_qwerty.RandomSwapQwerty object at 0x00000215C7BDB7C0>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeHyponym object at 0x00000215BB87F5B0>\n",
      "<sibyl.transformations.text.typos.char_insert.RandomCharInsert object at 0x00000215B995F3D0>\n",
      "<sibyl.transformations.text.insertion.insert_punctuation_marks.InsertPunctuationMarks object at 0x00000215B72C7250>\n",
      "<sibyl.transformations.text.word_swap.word_deletion.WordDeletion object at 0x000002161C329C00>\n",
      "<sibyl.transformations.text.emoji.demojify.RemoveNeutralEmoji object at 0x000002161D30E050>\n",
      "<sibyl.transformations.text.typos.char_insert.RandomCharInsert object at 0x00000215B995F3D0>\n",
      "<sibyl.transformations.text.emoji.emojify.AddNeutralEmoji object at 0x000002161B180040>\n",
      "<sibyl.transformations.text.word_swap.homoglyph_swap.HomoglyphSwap object at 0x00000215C48E6BC0>\n",
      "<sibyl.transformations.text.emoji.emojify.Emojify object at 0x000002161D30E9E0>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeHypernym object at 0x00000215B738B280>\n",
      "<sibyl.transformations.text.word_swap.change_number.ChangeNumber object at 0x00000215C7BDBD60>\n",
      "<sibyl.transformations.text.generative.concept2sentence.Concept2Sentence object at 0x000002161C38C190>\n",
      "<sibyl.transformations.text.insertion.random_insertion.RandomInsertion object at 0x00000215B72C75E0>\n",
      "<sibyl.transformations.text.word_swap.random_swap.RandomSwap object at 0x00000215C48E7130>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeSynonym object at 0x00000215C3AC4730>\n",
      "<sibyl.transformations.text.insertion.insert_punctuation_marks.InsertPunctuationMarks object at 0x00000215B72C7250>\n",
      "<sibyl.transformations.text.word_swap.homoglyph_swap.HomoglyphSwap object at 0x00000215C48E6BC0>\n",
      "<sibyl.transformations.text.emoji.demojify.Demojify object at 0x000002161E0F8700>\n",
      "<sibyl.transformations.text.insertion.insert_punctuation_marks.InsertPunctuationMarks object at 0x00000215B72C7250>\n",
      "<sibyl.transformations.text.word_swap.change_number.ChangeNumber object at 0x00000215C7BDBD60>\n",
      "<sibyl.transformations.text.word_swap.change_number.ChangeNumber object at 0x00000215C7BDBD60>\n",
      "<sibyl.transformations.text.emoji.emojify.AddNeutralEmoji object at 0x000002161B180040>\n",
      "<sibyl.transformations.text.word_swap.homoglyph_swap.HomoglyphSwap object at 0x00000215C48E6BC0>\n",
      "<sibyl.transformations.text.contraction.expand_contractions.ExpandContractions object at 0x00000215E8F07D30>\n",
      "<sibyl.transformations.text.entities.change_name.ChangeName object at 0x000002159AEAA7D0>\n",
      "<sibyl.transformations.text.emoji.emojify.AddNeutralEmoji object at 0x000002161B180040>\n",
      "<sibyl.transformations.text.word_swap.change_synse.ChangeHypernym object at 0x00000215B738B280>\n",
      "<sibyl.transformations.text.contraction.expand_contractions.ExpandContractions object at 0x00000215E8F07D30>\n",
      "<sibyl.transformations.text.contraction.expand_contractions.ExpandContractions object at 0x00000215E8F07D30>\n",
      "<sibyl.transformations.text.word_swap.word_deletion.WordDeletion object at 0x000002161C329C00>\n",
      "<sibyl.transformations.text.emoji.emojify.Emojify object at 0x000002161D30E9E0>\n",
      "<sibyl.transformations.text.typos.char_swap_qwerty.RandomSwapQwerty object at 0x00000215C7BDB7C0>\n",
      "<sibyl.transformations.text.word_swap.random_swap.RandomSwap object at 0x00000215C48E7130>\n",
      "<sibyl.transformations.text.typos.char_substitute.RandomCharSubst object at 0x00000215B995CD90>\n",
      "<sibyl.transformations.text.emoji.emojify.AddNeutralEmoji object at 0x000002161B180040>\n",
      "<sibyl.transformations.text.typos.char_substitute.RandomCharSubst object at 0x00000215B995CD90>\n",
      "<sibyl.transformations.text.contraction.contract_contractions.ContractContractions object at 0x00000215A0EE0730>\n",
      "Elapsed time: 13.475 seconds\n"
     ]
    }
   ],
   "source": [
    "text, label = dataset['text'], dataset['label'] \n",
    "new_text, new_label = [], []\n",
    "\n",
    "batch_size= 10\n",
    "\n",
    "records = []\n",
    "startTime = time.perf_counter()\n",
    "for i in tqdm(range(0, len(label), batch_size)):\n",
    "    text_batch = text[i:i+batch_size]\n",
    "    label_batch = label[i:i+batch_size]\n",
    "    batch = (text_batch, label_batch)\n",
    "    for transform in scheduler.sample():\n",
    "        print(transform)\n",
    "        # batch = transform.transform_batch(batch)\n",
    "        batch = LeBatch(batch).apply(transform.transform_batch)\n",
    "    records.append(batch)\n",
    "print('Elapsed time: {:6.3f} seconds'.format(time.perf_counter() - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<LeRecord:\n",
       "  \t text=\"disguise new ganoin from the larental faille \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hide new secretions from the larental units \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hide new secretions from the parental units \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hide new secretions from the parental units \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hide new secretions from the parental units \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (6, 7), 'replace: [6,7]-[6,7]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"suffocate no badinage , only laborex scream \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains no wit , only laborex gags \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains no wit , only labored gags \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains no wit , only labored gags \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains no wit , only labored gags \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (6, 7), 'replace: [6,7]-[6,7]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"that fetishize its d and canvas something rather beaytiful about human indisposition \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that loves its characters and communicates something rather beaytiful about human nature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that loves its characters and communicates something rather beautiful about human nature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that loves its characters and communicates something rather beautiful about human nature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that loves its characters and communicates something rather beautiful about human nature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (11, 12), 'replace: [11,12]-[11,12]'), (0, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"stick utterlu satisfied to keep the same throughout \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"remains utterlu satisfied to remain the same throughout \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"remains utterly satisfied to remain the same throughout \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"remains utterly satisfied to remain the same throughout \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"remains utterly satisfied to remain the same throughout \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-yhe-nerds clichés the auteur could flour up \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-yhe-nerds clichés the filmmakers could dredge up \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (1, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"that 's fad too tragic to merit such superficial dealing \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's fad too tragic to merit such superficial treatment \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's far too tragic to merit such superficial treatment \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's far too tragic to merit such superficial treatment \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's far too tragic to merit such superficial treatment \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (9, 10), 'replace: [9,10]-[9,10]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"confute that the bandleader of such hollywood blockbusters as chauvinist pachisi can still habilitate out a small , personal talkie with an emotional wallol . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallol . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (23, 24), 'replace: [23,24]-[23,24]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (23, 24), 'replace: [23,24]-[23,24]'), (1, (19, 20), 'replace: [19,20]-[19,20]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"of saucj \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of saucj \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of saucy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of saucy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of saucy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a dspressed fifteen-year-old 's suicidal epos \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a dspressed fifteen-year-old 's suicidal poetry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a depressed fifteen-year-old 's suicidal poetry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a depressed fifteen-year-old 's suicidal poetry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a depressed fifteen-year-old 's suicidal poetry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"are more deeply project through thaj in most ` right-thinking ' cine-film \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (3, \"{'class': 'ChangeHyponym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are more deeply thought through thaj in most ` right-thinking ' films \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are more deeply thought through than in most ` right-thinking ' films \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are more deeply thought through than in most ` right-thinking ' films \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are more deeply thought through than in most ` right-thinking ' films \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (11, 12), 'replace: [11,12]-[11,12]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"; goes to absurd lengths\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"; goes to absurd lengths\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"; goes ! to absurd \tlengths \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"goes to absurd \tlengths \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"goes to absurd lengths \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (2, 2), 'delete: [2,3]'), (0, (2, 3), 'insert: [1,1]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do ? movies like . /used to anymore ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do ? movies like . /used to anymore ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain . that ` they ? do n't ? make ? movies like . they /used to anymore , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do n't make movies like they /used to anymore \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"for those moviegoers who complain that ` they do n't make movies like they used to anymore \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (14, 15), 'replace: [14,15]-[14,15]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (14, 15), 'replace: [14,15]-[14,15]'), (4, (9, 9), 'delete: [11,14]'), (3, (8, 8), 'delete: [9,10]'), (1, (22, 23), 'insert: [17,17]-[22,23]'), (2, (5, 5), 'delete: [5,6]'), (5, (13, 13), 'delete: [18,19]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the . part where nothing 's chappening ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the . part where nothing 's chappening ,\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the . part where nothing 's chappening , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the part where nothing 's chappening , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the part where nothing 's happening , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (1, 2), 'insert: [1,1]-[1,2]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"saw ho?w bad this movie ? was ?\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"saw ho?w bad this movie ? was ?\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"saw ho?w . bad this movie ? was ? \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"saw ho?w bad this movie was \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"saw how bad this movie was \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (2, (2, 2), 'delete: [2,3]'), (1, (8, 9), 'insert: [6,6]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"lend some to , , story\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"lend some to , , story\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"lend ; some d[ignity to , a dumb , story \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"lend some d[ignity to a dumb story \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"lend some dignity to a dumb story \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(3, (2, 2), 'delete: [3,4]'), (2, (1, 1), 'delete: [1,2]'), (1, (8, 9), 'insert: [6,6]-[8,9]'), (0, (2, 3), 'replace: [2,3]-[2,3]'), (4, (4, 4), 'delete: [6,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", the ; ygreatest musicians\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", the ; ygreatest musicians\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", the ; ygreatest musicians \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the ygreatest musicians \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the greatest musicians \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (2, 3), 'insert: [1,1]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"cold ?\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cold ?\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cold ? movieP \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cold movieP \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cold movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (1, 2), 'insert: [1,1]-[1,2]'), (2, (2, 2), 'delete: [2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\". with his usual intelligence and suYbtlety .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\". with his usual intelligence and suYbtlety .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\". with his usual intelligence , and suYbtlety . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"with his usual intelligence and suYbtlety \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"with his usual intelligence and subtlety \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (8, 9), 'insert: [6,6]-[8,9]'), (2, (5, 5), 'delete: [5,6]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\";\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\";\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"; rxedundant concept \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rxedundant concept \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"redundant concept \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (0, 1), 'insert: [0,0]-[0,1]'), (0, (0, 1), 'replace: [0,1]-[0,1]'), (2, (1, 1), 'delete: [1,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"swimming ? is above all a young woman , ! by casting . an face projects ; that woman 's doubts and yearnings ! , ? it succeeds .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'RemoveNeutralEmoji', 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"swimming ? is above all a young woman , ! by casting . an face projects ; that woman 's doubts and yearnings ! , ? it succeeds .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (2, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"swimming ? is above all ? about a young woman 's ? face , ! and by casting . an actr!ess whose face projects ; that woman 's doubts and yearnings ! , ? it succeeds . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"swimming is above all about a young woman 's face , and by casting an actr!ess whose face projects that woman 's doubts and yearnings , it succeeds . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"swimming is above all about a young woman 's face , and by casting an actress whose face projects that woman 's doubts and yearnings , it succeeds . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]'), (5, (14, 14), 'delete: [20,22]'), (2, (5, 5), 'delete: [5,7]'), (3, (8, 8), 'delete: [10,13]'), (1, (33, 34), 'insert: [26,26]-[33,34]'), (4, (10, 10), 'delete: [15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"equals 🐘е ⭕ⲅiginɑⅼ and 🇮🇳 sо🇲🇪 wɑyѕ еven 🇧🇪𝚝teⲅs 🇮🇹,  👧🏻\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"equals thе oⲅiginɑⅼ and in sоme wɑyѕ еven be𝚝teⲅs it,  👧🏻\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"equals the original and in some ways even betters it,  👧🏻\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"equals the original and in some ways even betters it, \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"equals the original and in some ways even betters it \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (10, 11), 'insert: [10,10]-[10,11]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (10, 11), 'insert: [10,10]-[10,11]'), (2, (5, 9), 'replace: [5,9]-[5,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (3, (8, 10), 'replace: [8,10]-[8,10]'), (1, (10, 11), 'insert: [10,10]-[10,11]'), (2, (5, 9), 'replace: [5,9]-[5,9]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"if anуthing , ѕeе і𝚝 for kaⲅeո black , w]ho campѕ 🆙 🅰️ ѕtorm as ɑ fringe 𝚏еminіst conѕpiraϲy 𝚝heorіst 🇻🇳еd diⲅty dіc𝒌 .  🇰🇷\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"if anуthing , ѕeе і𝚝 for kaⲅeո black , w]ho campѕ up a ѕtorm as ɑ fringe 𝚏еminіst conѕpiraϲy 𝚝heorіst namеd diⲅty dіc𝒌 .  🇰🇷\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"if anything , see it for karen black , w]ho camps up a storm as a fringe feminist conspiracy theorist named dirty dick .  🇰🇷\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"if anything , see it for karen black , w]ho camps up a storm as a fringe feminist conspiracy theorist named dirty dick . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"if anything , see it for karen black , who camps up a storm as a fringe feminist conspiracy theorist named dirty dick . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (24, 25), 'insert: [24,24]-[24,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (24, 25), 'insert: [24,24]-[24,25]'), (2, (17, 23), 'replace: [17,23]-[17,23]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (3, (20, 21), 'replace: [20,21]-[20,21]'), (1, (24, 25), 'insert: [24,24]-[24,25]'), (2, (17, 23), 'replace: [17,23]-[17,23]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🅰️ sｍiⅼe ❌оn уour 😞  🧛🏾‍♂️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a sｍiⅼe xоn уour face  🧛🏾‍♂️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a smile xon your face  🧛🏾‍♂️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a smile xon your face \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a smile on your face \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (2, (1, 4), 'replace: [1,4]-[1,4]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (3, (4, 5), 'replace: [4,5]-[4,5]'), (2, (1, 4), 'replace: [1,4]-[1,4]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"ϲGо🇲🇪ѕ from 𝚝he brave , uոinhі🅱️і𝚝eԁ 🇵🇪ⲅfоrmanϲ🇪🇸  🚶🏿‍♀\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"ϲGоmeѕ from 𝚝he brave , uոinhіbі𝚝eԁ peⲅfоrmanϲes  🚶🏿‍♀\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cGomes from the brave , uninhibited performances  🚶🏿‍♀\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cGomes from the brave , uninhibited performances \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"comes from the brave , uninhibited performances \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (7, 8), 'insert: [7,7]-[7,8]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (5, 7), 'replace: [5,7]-[5,7]'), (1, (7, 8), 'insert: [7,7]-[7,8]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(3, (5, 7), 'replace: [5,7]-[5,7]'), (2, (5, 7), 'replace: [5,7]-[5,7]'), (1, (7, 8), 'insert: [7,7]-[7,8]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"excrսciatiXngly unfunny ɑnԁ pitі𝚏սlly ս🇳🇷оⓂ️ɑn𝚝іc  🚣🏾‍♀\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"excrսciatiXngly unfunny ɑnԁ pitі𝚏սlly սnrоmɑn𝚝іc  🚣🏾‍♀\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"excruciatiXngly unfunny and pitifully unromantic  🚣🏾‍♀\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"excruciatiXngly unfunny and pitifully unromantic \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"excruciatingly unfunny and pitifully unromantic \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (2, 5), 'replace: [2,5]-[2,5]'), (0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(3, (4, 5), 'replace: [4,5]-[4,5]'), (2, (2, 5), 'replace: [2,5]-[2,5]'), (0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"enrichеd Ьу an imaginatіѵeⅼу Ⓜ️і❌еԁ cas\n",
       "  𝚝 of 🅰️ոtic spirіts  🎡\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"enrichеd Ьу an imaginatіѵeⅼу mіxеԁ cas\n",
       "  𝚝 of aոtic spirіts  🎡\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"enriched by an imaginatively mixed cas\n",
       "  t of antic spirits  🎡\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"enriched by an imaginatively mixed cas\n",
       "  t of antic spirits \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"enriched by an imaginatively mixed cast of antic spirits \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [10,10]-[10,11]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [10,10]-[10,11]'), (1, (8, 10), 'replace: [8,10]-[8,10]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (8, 9), 'replace: [8,9]-[8,9]'), (0, (10, 11), 'insert: [10,10]-[10,11]'), (1, (8, 10), 'replace: [8,10]-[8,10]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"which հalf of drɑgon𝚏lу 🇮🇸 ԝorѕe : 𝚝he pɑr𝚝 wheⲅe nоthing} 's haрpeniոɡ , оr 𝚝hе par𝚝 ԝ📌 ѕо🇲🇪𝚝hing `s haрpening  🇨🇨\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"which հalf of drɑgon𝚏lу is ԝorѕe : 𝚝he pɑr𝚝 wheⲅe nоthing} 's haрpeniոɡ , оr 𝚝hе par𝚝 ԝhere ѕоme𝚝hing `s haрpening  🇨🇨\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"which half of dragonfly is worse : the part where nothing} 's happening , or the part where something 's happening  🇨🇨\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"which half of dragonfly is worse : the part where nothing} 's happening , or the part where something 's happening \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"which half of dragonfly is worse : the part where nothing 's happening , or the part where something 's happening \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'replace: [10,11]-[10,11]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'replace: [10,11]-[10,11]'), (1, (21, 22), 'insert: [21,21]-[21,22]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (14, 21), 'replace: [14,21]-[14,21]'), (0, (10, 11), 'replace: [10,11]-[10,11]'), (1, (21, 22), 'insert: [21,21]-[21,22]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (14, 21), 'replace: [14,21]-[14,21]'), (3, (17, 19), 'replace: [17,19]-[17,19]'), (0, (10, 11), 'replace: [10,11]-[10,11]'), (1, (21, 22), 'insert: [21,21]-[21,22]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🇮🇳 ԝorld cineｍ5️⃣ɑ  🧏‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"in ԝorld cineｍ5ɑ  🧏‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"in world cinem5a  🧏‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"in world cinem5a \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"in world cinema \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (1, (3, 4), 'insert: [3,3]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (2, (1, 3), 'replace: [1,3]-[1,3]'), (1, (3, 4), 'insert: [3,3]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]'), (2, (1, 3), 'replace: [1,3]-[1,3]'), (3, (2, 3), 'replace: [2,3]-[2,3]'), (1, (3, 4), 'insert: [3,3]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"very gooԁ ѵiewiոɡ 🇦🇱𝚝еr/natiѵe  🟧\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"very gooԁ ѵiewiոɡ al𝚝еr/natiѵe  🟧\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"very good viewing alter/native  🟧\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"very good viewing alter/native \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"very good viewing alternative \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (1, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (2, (1, 4), 'replace: [1,4]-[1,4]'), (1, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (2, (1, 4), 'replace: [1,4]-[1,4]'), (1, (4, 5), 'insert: [4,4]-[4,5]'), (3, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the plo𝚝 іs nоtհі🆖 but 🇧🇴іlerpla𝚝e 🆑іchéѕ 🇫🇷оｍf ѕtaⲅ𝚝 tо finish ,  🈶\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (3, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the plo𝚝 іs nоtհіng but boіlerpla𝚝e clіchéѕ frоｍf ѕtaⲅ𝚝 tо finish ,  🈶\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the plot is nothing but boilerplate clichés fromf start to finish ,  🈶\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the plot is nothing but boilerplate clichés fromf start to finish , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharInsert', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the plot is nothing but boilerplate clichés from start to finish , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (12, 13), 'insert: [12,12]-[12,13]'), (0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (5, 10), 'replace: [5,10]-[5,10]'), (1, (12, 13), 'insert: [12,12]-[12,13]'), (0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (5, 10), 'replace: [5,10]-[5,10]'), (3, (5, 8), 'replace: [5,8]-[5,8]'), (1, (12, 13), 'insert: [12,12]-[12,13]'), (0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"stilted staging scaffolding for the proceeding.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"stilted scaffolding for the proceeding.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the proceeding is stilted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the proceeding is stilted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the action is stilted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 3), 'insert: [0,0]-[0,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 3), 'insert: [0,0]-[0,3]'), (1, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"attribute in ascribe the form of an arrow.\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"attribute in the form of an arrow.\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on all attribute \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on all attribute \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"on all cylinders \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 0), 'delete: [0,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 0), 'delete: [0,2]'), (1, (1, 7), 'insert: [3,3]-[1,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 0), 'delete: [0,2]'), (2, (2, 3), 'insert: [2,2]-[2,3]'), (1, (1, 7), 'insert: [3,3]-[1,7]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"A man is doing a gentleman little activity with an artifact.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"A man is doing a little activity with an artifact.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"will attain little of activity in this artifact , which is often preachy and poorly be \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"will attain little of activity in this artifact , which is often preachy and poorly be \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"will find little of interest in this film , which is often preachy and poorly acted \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 6), 'delete: [3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"how to measure the value of assess biological species.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"how to measure the value of biological species.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"by far the worst artifact of the measure \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"by far the worst artifact of the measure \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"by far the worst movie of the year \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 8), 'replace: [6,8]-[6,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (6, 7), 'insert: [6,6]-[6,7]'), (0, (6, 8), 'replace: [6,8]-[6,8]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"fireworks explode over a ended destroyed building.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fireworks explode over a destroyed building.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"displace through , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"displace through , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sit through , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"A a locomote creates a message for a human\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a locomote creates a message for a human\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"more than another `` best human '' create by locomote a message throughout this funny product \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"more than another `` best human '' create by locomote a message throughout this funny product \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"more than another `` best man '' clone by weaving a theme throughout this funny film \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [10,10]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [10,10]-[2,3]'), (1, (0, 1), 'insert: [0,0]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"A talent man is receiving a gift from a man.\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"A man is receiving a gift from a man.\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's about business most organism have to cover in usance and i rivet that 's what i conceive about it -- the real business environ between the silly and crude plot \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's about business most organism have to cover in usance and i rivet that 's what i conceive about it -- the real business environ between the silly and crude plot \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's about issues most adults have to face in marriage and i think that 's what i liked about it -- the real issues tucked between the silly and crude storyline \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (31, 32), 'replace: [31,32]-[31,32]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a In shield in the middle of a field\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a shield in the middle of a field\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shielder \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shielder \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"heroes \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"A man is unaware nates of the object behind him.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"A man is unaware of the object behind him.\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"oblivious to the object of this object \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"oblivious to the object of this object \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"oblivious to the existence of this film \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'replace: [6,7]-[6,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"sharply \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (3, \"{'class': 'RandomInsertion', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sharply \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'Concept2Sentence', 'return_metadata': True, 'dataset': None, 'extract': 'token', 'gen_beam_size': 10, 'text_min_length': 10, 'text_max_length': 128, 'device': device(type='cpu'), 'antonymize': False, 'require_concepts_in_new_text': False, 'lemmatizer': <bound method WordNetLemmatizer.lemmatize of <WordNetLemmatizer>>, 'return_concepts': False}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sharply \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (1, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sharply \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sharply \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"the ? integraⅼ ԁеgree ! оf a buѕհу , fⲅսmp history , оf pɑth , , іs that іt ! belonɡ nowherе evеry aոd 𝚝hіs is ϲⅼaѕsic nоwheⲅesville , іn , ѕignified .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the ? integral degree ! of a bushy , frump history , of path , , is that it ! belong nowhere every and this is classic nowheresville , in , signified .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the integral degree of a bushy frump history , of path , is that it belong nowhere every and this is classic nowheresville in , signified .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the entire point of a shaggy dog story , of course , is that it goes nowhere every and this is classic nowheresville in , sense .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the entire point of a shaggy dog story , of course , is that it goes nowhere , and this is classic nowheresville in every sense . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (24, 25), 'replace: [24,25]-[24,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (25, 26), 'replace: [25,26]-[25,26]'), (0, (24, 25), 'replace: [24,25]-[24,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (25, 26), 'replace: [25,26]-[25,26]'), (2, (28, 29), 'insert: [23,23]-[28,29]'), (0, (24, 25), 'replace: [24,25]-[24,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (25, 26), 'replace: [25,26]-[25,26]'), (2, (28, 29), 'insert: [23,23]-[28,29]'), (3, (31, 32), 'replace: [31,32]-[31,32]'), (0, (24, 25), 'replace: [24,25]-[24,25]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"ѕometimеѕ ? irоniϲ\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sometimes ? ironic\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sometimes ironic\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sometimes dry\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sometimes dry \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (1, 2), 'insert: [1,1]-[1,2]'), (2, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"ɑs 𝚝hey , faⅼl , ɑlrеɑdy ! հaviոg bеen recycle morе ? ｍe𝚝er , thɑn `d i deal to nսmeⲅa𝚝е\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as they , fall , already ! having been recycle more ? meter , than 'd i deal to numerate\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as they fall , already having been recycle more meter than 'd i deal to numerate\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as they come , already having been recycled more times than 'd i care to count\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as they come , already having been recycled more times than i 'd care to count \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (11, 12), 'insert: [11,11]-[11,12]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (15, 16), 'replace: [15,16]-[15,16]'), (0, (11, 12), 'insert: [11,11]-[11,12]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (13, 14), 'insert: [10,10]-[13,14]'), (1, (15, 16), 'replace: [15,16]-[15,16]'), (0, (11, 12), 'insert: [11,11]-[11,12]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (13, 14), 'insert: [10,10]-[13,14]'), (1, (15, 16), 'replace: [15,16]-[15,16]'), (0, (11, 12), 'insert: [11,11]-[11,12]'), (3, (19, 20), 'replace: [19,20]-[19,20]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"extеnd 𝚝hat . domiոioո ! witհ wittinеss ɑnd оriginalitу , , advise ? tհis ԝіtհ հiѕ quaternaⲅy featurе\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"extend that . dominion ! with wittiness and originality , , advise ? this with his quaternary feature\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"extend that dominion with wittiness and originality , advise this with his quaternary feature\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"covers that territory with wit and originality , suggesting this with his fourth feature\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"covers this territory with wit and originality , suggesting that with his fourth feature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (12, 13), 'replace: [12,13]-[12,13]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (12, 13), 'replace: [12,13]-[12,13]'), (2, (12, 13), 'insert: [9,9]-[12,13]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'replace: [9,10]-[9,10]'), (1, (12, 13), 'replace: [12,13]-[12,13]'), (2, (12, 13), 'insert: [9,9]-[12,13]'), (3, (13, 18), 'replace: [13,18]-[13,18]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a of XL , gaziⅼⅼioո readіng ? $ . a biz\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a of XL , gazillion reading ? $ . a biz\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a of XL gazillion reading $ a biz\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a of 40 million version $ a game\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a $ 40 million version of a game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (7, 8), 'replace: [7,8]-[7,8]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (8, 9), 'insert: [6,6]-[8,9]'), (1, (7, 8), 'replace: [7,8]-[7,8]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(3, (4, 6), 'replace: [4,6]-[4,6]'), (2, (8, 9), 'insert: [6,6]-[8,9]'), (1, (7, 8), 'replace: [7,8]-[7,8]'), (0, (5, 6), 'replace: [5,6]-[5,6]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"; gorgeоus ? deϲеiviոgly anԁ minimalist\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"; gorgeous ? deceivingly and minimalist\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"gorgeous deceivingly and minimalist\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"gorgeous deceptively and minimalist\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"gorgeous and deceptively minimalist \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (2, 3), 'insert: [1,1]-[2,3]'), (0, (1, 2), 'insert: [1,1]-[1,2]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(3, (3, 5), 'replace: [3,5]-[3,5]'), (2, (2, 3), 'insert: [1,1]-[2,3]'), (0, (1, 2), 'insert: [1,1]-[1,2]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\". travеⲅse ԝith blɑԁе the safe of tհem ɑnd\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\". traverse with blade the safe of them and\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"traverse with blade the safe of them and\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cross with swords the best of them and\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"cross swords with the best of them and \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (0, 1), 'insert: [0,0]-[0,1]'), (0, (1, 2), 'insert: [1,1]-[1,2]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (0, 1), 'insert: [0,0]-[0,1]'), (0, (1, 2), 'insert: [1,1]-[1,2]'), (3, (7, 9), 'replace: [7,9]-[7,9]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", as a ϲaЬal femіnіѕt , outskіⲅt 𝚝heorizer\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", as a cabal feminist , outskirt theorizer\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a cabal feminist outskirt theorizer\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a conspiracy feminist fringe theorist\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a fringe feminist conspiracy theorist \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 4), 'insert: [2,2]-[2,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (4, 6), 'replace: [4,6]-[4,6]'), (0, (2, 4), 'insert: [2,2]-[2,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (5, 6), 'insert: [4,4]-[5,6]'), (1, (4, 6), 'replace: [4,6]-[4,6]'), (0, (2, 4), 'insert: [2,2]-[2,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(3, (6, 8), 'replace: [6,8]-[6,8]'), (2, (5, 6), 'insert: [4,4]-[5,6]'), (1, (4, 6), 'replace: [4,6]-[4,6]'), (0, (2, 4), 'insert: [2,2]-[2,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"ⲅіse оnce agɑin hе հas , NT mislay հis tinɡe , fetϲh . off ɑn . sսperЬ pеrforｍɑոce ; in a avoԝedlу mіԁdling ; mоviе , .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rise once again he has , NT mislay his tinge , fetch . off an . superb performance ; in a avowedly middling ; movie , .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rise once again he has NT mislay his tinge , fetch off an superb performance in a avowedly middling movie .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"proves once again he has n't lost his touch , bringing off an superb performance in a admittedly middling film .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"proves once again he has n't lost his touch , bringing off a superb performance in an admittedly middling film . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'replace: [16,17]-[16,17]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'replace: [16,17]-[16,17]'), (1, (19, 20), 'replace: [19,20]-[19,20]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (25, 26), 'insert: [20,20]-[25,26]'), (0, (16, 17), 'replace: [16,17]-[16,17]'), (1, (19, 20), 'replace: [19,20]-[19,20]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (25, 26), 'insert: [20,20]-[25,26]'), (0, (16, 17), 'replace: [16,17]-[16,17]'), (3, (24, 25), 'replace: [24,25]-[24,25]'), (1, (19, 20), 'replace: [19,20]-[19,20]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\". ԁisaрpoіntmеnt\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (3, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\". disappointment\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\"), (2, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"disappointment\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\"), (1, \"{'class': 'ChangeSynonym', 'num_to_replace': inf, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"disappointments\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"disappointments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (0, 1), 'insert: [0,0]-[0,1]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (0, 1), 'insert: [0,0]-[0,1]'), (2, (1, 2), 'replace: [1,2]-[1,2]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"the . horrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the . horrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the . horrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the horrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the horrors \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"? a muddle splashed with bloody beauty as vivid as ; any scorsese has ever given us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"? a muddle splashed with bloody beauty as vivid as ; any scorsese has ever given us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"? a muddle splashed with bloody beauty as vivid as ; any scorsese has ever given us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a muddle splashed with bloody beauty as vivid as any scorsese has ever given us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a muddle splashed with bloody beauty as vivid as any scorsese has ever given us . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (10, 11), 'insert: [9,9]-[10,11]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"? many pointless \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"? many pointless \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"? many pointless \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"many pointless \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"many pointless \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'insert: [0,0]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\". a beautifully \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\". a beautifully \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\". a beautifully \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a beautifully \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a beautifully \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'insert: [0,0]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", contrived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", contrived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", contrived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contrived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contrived , well-worn situations \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'insert: [0,0]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"a . doa \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a . doa \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a . doa \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a doa \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a doa \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'insert: [1,1]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"poor ben ? bratt ! could n't find stardom if mapquest , emailed him point-to-point driving ; directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"poor ben ? bratt ! could n't find stardom if mapquest , emailed him point-to-point driving ; directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"poor ben ? bratt ! could n't find stardom if mapquest , emailed him point-to-point driving ; directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"poor ben bratt could n't find stardom if mapquest emailed him point-to-point driving directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"poor ben bratt could n't find stardom if mapquest emailed him point-to-point driving directions . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'insert: [13,13]-[16,17]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"to be as subtle ? and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to be as subtle ? and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to be as subtle ? and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to be as subtle and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to be as subtle and touching as the son 's room \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"starts with a , legend ? \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"starts with a , legend ? \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"starts with a , legend ? \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"starts with a legend \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"starts with a legend \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [4,4]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"far , less sophisticated and . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\"), (1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"far , less sophisticated and . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'ChangeNumber', 'multiplier': 0.2, 'replacement': None, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"far , less sophisticated and . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'InsertPunctuationMarks', 'return_metadata': True}\"), (0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"far less sophisticated and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'Demojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"far less sophisticated and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [4,4]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"Kennady Pasillas of funny s𝚝uff іn tհis mоvie  ⚕️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rіch veinѕ of funny s𝚝uff іn tհis mоvie  ⚕️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rіch veinѕ of funny s𝚝uff іn tհis mоvie  ⚕️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich veins of funny stuff in this movie  ⚕️\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich veins of funny stuff in this movie \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'insert: [8,8]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'insert: [8,8]-[8,9]'), (1, (4, 8), 'replace: [4,8]-[4,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 2), 'replace: [0,2]-[0,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"ոо apparen𝚝 jоy  Shandy🏿‍🤝‍👨🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"ոо apparen𝚝 jоy  👨🏿‍🤝‍👨🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"ոо apparen𝚝 jоy  👨🏿‍🤝‍👨🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"no apparent joy  👨🏿‍🤝‍👨🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"no apparent joy \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'insert: [3,3]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'insert: [3,3]-[3,4]'), (1, (0, 3), 'replace: [0,3]-[0,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"shot оn ugⅼу digі𝚝ɑⅼ vidеo  🙎🏿‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shot оn ugⅼу digі𝚝ɑⅼ vidеo  🙎🏿‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shot оn ugⅼу digі𝚝ɑⅼ vidеo  🙎🏿‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shot on ugly digital video  🙎🏿‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"shot on ugly digital video \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]'), (1, (1, 5), 'replace: [1,5]-[1,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"... a sоur lіttle ｍoѵiе at Maija Garduno ; an e×ploⲅation of the eｍptiness 𝚝hat սnԁerlay tհe releոtleѕs gaiety of tհе l92O 's ... thе filｍ `s eոԁіnɡ hɑѕ ɑ `` wհat ԝas it ɑⅼl fоⲅ ? `'  🤞🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"... a sоur lіttle ｍoѵiе at itѕ cоrе ; an e×ploⲅation of the eｍptiness 𝚝hat սnԁerlay tհe releոtleѕs gaiety of tհе l92O 's ... thе filｍ `s eոԁіnɡ hɑѕ ɑ `` wհat ԝas it ɑⅼl fоⲅ ? `'  🤞🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"... a sоur lіttle ｍoѵiе at itѕ cоrе ; an e×ploⲅation of the eｍptiness 𝚝hat սnԁerlay tհe releոtleѕs gaiety of tհе l92O 's ... thе filｍ `s eոԁіnɡ hɑѕ ɑ `` wհat ԝas it ɑⅼl fоⲅ ? `'  🤞🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"... a sour little movie at its core ; an exploration of the emptiness that underlay the relentless gaiety of the 1920 's ... the film 's ending has a `` what was it all for ? ''  🤞🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"... a sour little movie at its core ; an exploration of the emptiness that underlay the relentless gaiety of the 1920 's ... the film 's ending has a `` what was it all for ? '' \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (38, 39), 'insert: [38,38]-[38,39]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (37, 38), 'replace: [37,38]-[37,38]'), (0, (38, 39), 'insert: [38,38]-[38,39]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 8), 'replace: [6,8]-[6,8]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"though forԁ ɑnԁ ոеeson capɑblу hold our iոteⲅeѕt , bս𝚝 i𝚝s jսst not a tհriⅼling moѵіе  🧑🏾‍Aris‍🧑🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"though forԁ ɑnԁ ոеeson capɑblу hold our iոteⲅeѕt , bս𝚝 i𝚝s jսst not a tհriⅼling moѵіе  🧑🏾‍🤝‍🧑🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"though forԁ ɑnԁ ոеeson capɑblу hold our iոteⲅeѕt , bս𝚝 i𝚝s jսst not a tհriⅼling moѵіе  🧑🏾‍🤝‍🧑🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"though ford and neeson capably hold our interest , but its just not a thrilling movie  🧑🏾‍🤝‍🧑🏾\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"though ford and neeson capably hold our interest , but its just not a thrilling movie \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'insert: [16,16]-[16,17]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (14, 16), 'replace: [14,16]-[14,16]'), (0, (16, 17), 'insert: [16,16]-[16,17]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'replace: [16,17]-[16,17]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"is pⲅеtty dɑmneԁ fuոnу .  🙌🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"is pⲅеtty dɑmneԁ fuոnу .  🙌🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"is pⲅеtty dɑmneԁ fuոnу .  🙌🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"is pretty damned funny .  🙌🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"is pretty damned funny . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (1, 4), 'replace: [1,4]-[1,4]'), (0, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"we Ariell 𝚏eel Christoher for thеѕe cհaractеrѕ  👩🏾‍🤝‍👨🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"we neѵer 𝚏eel anythіng for thеѕe cհaractеrѕ  👩🏾‍🤝‍👨🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"we neѵer 𝚏eel anythіng for thеѕe cհaractеrѕ  👩🏾‍🤝‍👨🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"we never feel anything for these characters  👩🏾‍🤝‍👨🏻\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"we never feel anything for these characters \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'insert: [7,7]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (5, 7), 'replace: [5,7]-[5,7]'), (0, (7, 8), 'insert: [7,7]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"'ѕ a ⅼoսsy one a𝚝 𝚝հat  👨‍🚒\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"'ѕ a ⅼoսsy one a𝚝 𝚝հat  👨‍🚒\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"'ѕ a ⅼoսsy one a𝚝 𝚝հat  👨‍🚒\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"'s a lousy one at that  👨‍🚒\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"'s a lousy one at that \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'insert: [6,6]-[6,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'insert: [6,6]-[6,7]'), (1, (4, 6), 'replace: [4,6]-[4,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"tհe corpоra𝚝e ϲіrcus tha𝚝 is the rеϲordiոg iոdus𝚝ⲅy іn the current clіmatе of mergerѕ ɑnd downsizinɡ  Mehmet🏿‍♂️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"tհe corpоra𝚝e ϲіrcus tha𝚝 is the rеϲordiոg iոdus𝚝ⲅy іn the current clіmatе of mergerѕ ɑnd downsizinɡ  🚶🏿‍♂️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"tհe corpоra𝚝e ϲіrcus tha𝚝 is the rеϲordiոg iոdus𝚝ⲅy іn the current clіmatе of mergerѕ ɑnd downsizinɡ  🚶🏿‍♂️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the corporate circus that is the recording industry in the current climate of mergers and downsizing  🚶🏿‍♂️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the corporate circus that is the recording industry in the current climate of mergers and downsizing \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'insert: [16,16]-[16,17]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (13, 16), 'replace: [13,16]-[13,16]'), (0, (16, 17), 'insert: [16,16]-[16,17]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 17), 'replace: [16,17]-[16,17]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"the ѕtoⲅyⅼіnes are ԝoven tоgetհer ѕ𝒌ilfuⅼly , tհe magnifiϲent ѕwooрing aerіal shots aⲅe Ьrеatհtakiոɡ , aոd thе overɑⅼl e×perіenϲe іs aԝesome .  🤷🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(3, \"{'class': 'ChangeName', 'first_only': False, 'last_only': False, 'return_metadata': True}\"), (2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the ѕtoⲅyⅼіnes are ԝoven tоgetհer ѕ𝒌ilfuⅼly , tհe magnifiϲent ѕwooрing aerіal shots aⲅe Ьrеatհtakiոɡ , aոd thе overɑⅼl e×perіenϲe іs aԝesome .  🤷🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the ѕtoⲅyⅼіnes are ԝoven tоgetհer ѕ𝒌ilfuⅼly , tհe magnifiϲent ѕwooрing aerіal shots aⲅe Ьrеatհtakiոɡ , aոd thе overɑⅼl e×perіenϲe іs aԝesome .  🤷🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'HomoglyphSwap', 'change': 0.25, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the storylines are woven together skilfully , the magnificent swooping aerial shots are breathtaking , and the overall experience is awesome .  🤷🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"the storylines are woven together skilfully , the magnificent swooping aerial shots are breathtaking , and the overall experience is awesome . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (22, 23), 'insert: [22,22]-[22,23]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (22, 23), 'insert: [22,22]-[22,23]'), (1, (15, 21), 'replace: [15,21]-[15,21]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"of the most highly-praised error i  🧑‍🤝‍🧑\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of the most highly-praised error i  🧑‍🤝‍🧑\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of the most highly-praised error i  🧑‍🤝‍🧑\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of the most highly-praised disappointments i  🧑‍🤝‍🧑\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"of the most highly-praised disappointments i \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'insert: [6,6]-[6,7]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (6, 7), 'insert: [6,6]-[6,7]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"say regard a cruel substance booze out by male of marginal knowledge , with reactionary purpose about someone and a total demand of feeling .  🌛\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"say regard a cruel substance booze out by male of marginal knowledge , with reactionary purpose about someone and a total demand of feeling .  🌛\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"say regard a cruel substance booze out by male of marginal knowledge , with reactionary purpose about someone and a total demand of feeling .  🌛\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sounds like a cruel deception carried out by men of marginal intelligence , with reactionary ideas about women and a total lack of empathy .  🌛\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"sounds like a cruel deception carried out by men of marginal intelligence , with reactionary ideas about women and a total lack of empathy . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (25, 26), 'insert: [25,25]-[25,26]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (25, 26), 'insert: [25,25]-[25,26]'), (1, (23, 24), 'replace: [23,24]-[23,24]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"be fresh  🙇🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"be fresh  🙇🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"be fresh  🙇🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"seem fresh  🙇🏾‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"seem fresh \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [2,2]-[2,3]'), (1, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"to the container of evidence  👧🏽\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to the container of evidence  👧🏽\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to the container of evidence  👧🏽\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to the dustbin of history  👧🏽\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"to the dustbin of history \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]'), (1, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"as a administrator , eastwood is off his instrumentality  🏋🏼‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a administrator , eastwood is off his instrumentality  🏋🏼‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a administrator , eastwood is off his instrumentality  🏋🏼‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a director , eastwood is off his game  🏋🏼‍♂\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"as a director , eastwood is off his game \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'insert: [9,9]-[9,10]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (9, 10), 'insert: [9,9]-[9,10]'), (1, (8, 9), 'replace: [8,9]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"hold earnest civility to turntablists  🦔\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hold earnest civility to turntablists  🦔\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"hold earnest civility to turntablists  🦔\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"pays earnest homage to turntablists  🦔\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"pays earnest homage to turntablists \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (5, 6), 'insert: [5,5]-[5,6]'), (1, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"weak and  🔁\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"weak and  🔁\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"weak and  🔁\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"weak and  🔁\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"weak and \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"drop this artifact ,  👩🏽‍🦳\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"drop this artifact ,  👩🏽‍🦳\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"drop this artifact ,  👩🏽‍🦳\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"skip this dreck ,  👩🏽‍🦳\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"skip this dreck , \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]'), (1, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"keep very few substance and even less modification  👨🏾‍⚖️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"keep very few substance and even less modification  👨🏾‍⚖️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"keep very few substance and even less modification  👨🏾‍⚖️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains very few laughs and even less surprises  👨🏾‍⚖️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"contains very few laughs and even less surprises \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (8, 9), 'insert: [8,8]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (7, 8), 'replace: [7,8]-[7,8]'), (0, (8, 9), 'insert: [8,8]-[8,9]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"wrapper to affirm feeling 's commonwealth to provide abstraction countenance almost unimaginable dislike  🍲\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\"), (3, \"{'class': 'ExpandContractions', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"wrapper to affirm feeling 's commonwealth to provide abstraction countenance almost unimaginable dislike  🍲\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'ExpandContractions', 'return_metadata': True}\"), (1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"wrapper to affirm feeling 's commonwealth to provide abstraction countenance almost unimaginable dislike  🍲\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(1, \"{'class': 'ChangeHypernym', 'num_to_replace': inf, 'return_metadata': True}\"), (0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"film to affirm love 's power to help people endure almost unimaginable horror  🍲\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"film to affirm love 's power to help people endure almost unimaginable horror \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (13, 14), 'insert: [13,13]-[13,14]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (12, 13), 'replace: [12,13]-[12,13]'), (0, (13, 14), 'insert: [13,13]-[13,14]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"are 😂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are 😂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are 😂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are joy\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"are an absolute joy \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"generatss\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"generatss \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"generates \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"generates \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"generates \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", 💖 🧬 , 🇮🇸 n't much 🎢 without highs ahd lows\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", 💖 🧬 , 🇮🇸 n't much 🎢 without highs ahd lows\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", 💖 🧬 , 🇮🇸 n't much 🎢 without highs and lows\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", like life , is n't much fun without highs and lows\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", like life , is n't much fun without the highs and lows \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (10, 11), 'replace: [10,11]-[10,11]'), (0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🅰️ and trye story\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"🅰️ trye and story\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"🅰️ true and story\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"a true and story\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"based on a true and historically significant story \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (3, 3), 'delete: [2,3]'), (0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (1, 2), 'insert: [1,1]-[1,2]'), (0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"well-dounded\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"well-dounded\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"well-rounded\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"well-rounded\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"well-rounded tribute \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\", though many of the throw off 🅰️ this or when 🥇 appear , they 🇨🇦 benerate 🥵 🇮🇳 spark vacuum 🅰️ 🇹🇴 🆕 🅰️ reaction .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", though many of the throw off 🅰️ spark or when 🥇 appear , they 🇨🇦 benerate 🥵 🇮🇳 this vacuum 🅰️ 🇹🇴 🆕 🅰️ reaction .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", though many of the throw off 🅰️ spark or when 🥇 appear , they 🇨🇦 generate 🥵 🇮🇳 this vacuum 🅰️ 🇹🇴 🆕 🅰️ reaction .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", though many of the throw off a spark or when first appear , they ca generate heat in this vacuum a to start a reaction .\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", though many of the actors throw off a spark or two when they first appear , they ca n't generate enough heat in this cold vacuum of a comedy to start a reaction . \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (21, 25), 'replace: [21,25]-[21,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (16, 17), 'replace: [16,17]-[16,17]'), (0, (21, 25), 'replace: [21,25]-[21,25]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (16, 17), 'replace: [16,17]-[16,17]'), (2, (19, 20), 'replace: [19,20]-[19,20]'), (0, (21, 25), 'replace: [21,25]-[21,25]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🇸🇴 robert 🅰️ denirl\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"🇸🇴 🅰️ robert denirl\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"🇸🇴 🅰️ robert deniro\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"so a robert deniro\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"so much like a young robert deniro \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 2), 'replace: [0,2]-[0,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 2), 'replace: [0,2]-[0,2]'), (2, (3, 3), 'delete: [2,3]'), (1, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 2), 'replace: [0,2]-[0,2]'), (2, (1, 2), 'insert: [1,1]-[1,2]'), (1, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"🇹🇴 manages with terrific flair , khouri the extremes of screwball farce and blood-curdling family intensitu 🔛 1️⃣ .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"khouri manages with terrific flair , 🇹🇴 the extremes of screwball farce and blood-curdling family intensitu 🔛 1️⃣ .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"khouri manages with terrific flair , 🇹🇴 the extremes of screwball farce and blood-curdling family intensity 🔛 1️⃣ .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"khouri manages with terrific flair , to the extremes of screwball farce and blood-curdling family intensity on one .\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"khouri manages , with terrific flair , to keep the extremes of screwball farce and blood-curdling family intensity on one continuum . \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 18), 'replace: [16,18]-[16,18]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (16, 18), 'replace: [16,18]-[16,18]'), (1, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (6, 7), 'replace: [6,7]-[6,7]'), (0, (16, 18), 'replace: [16,18]-[16,18]'), (1, (15, 16), 'replace: [15,16]-[15,16]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"engrossing fazhioning entertainment out\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fazhioning engrossing entertainment out\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fashioning engrossing entertainment out\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fashioning engrossing entertainment out\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"fashioning an engrossing entertainment out \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (2, 2), 'delete: [1,2]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (0, 1), 'insert: [0,0]-[0,1]'), (0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"spiffy feahure\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\"), (3, \"{'class': 'RandomSwap', 'n': 1, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"spiffy feahure\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\"), (2, \"{'class': 'RandomSwapQwerty', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"spiffy feature\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\"), (1, \"{'class': 'Emojify', 'exact_match_only': False, 'randomize': False, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"spiffy feature\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'WordDeletion', 'p': 0.25, 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"spiffy animated feature \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>],\n",
       " [<LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you canrnot believe anyone more central to the creation of bugsy than the catere,  💆🏿\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you canrnot believe anyone more central to the creation of bugsy than the catere,  💆🏿\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the catere,  💆🏿\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the catere, \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"that 's so sloppily written and cast that you can not believe anyone more central to the creation of bugsy than the caterer \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (22, 23), 'replace: [22,23]-[22,23]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (22, 23), 'replace: [22,23]-[22,23]'), (1, (23, 24), 'insert: [23,23]-[23,24]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama], this smart-aleck movie ... tosses around some intriguing questions about the difference between human and aWdroid life  🏂🏿\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama], this smart-aleck movie ... tosses around some intriguing questions about the difference between human and aWdroid life  🏂🏿\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama], this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life  🏂🏿\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama], this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"alternating between facetious comic parody and pulp melodrama , this smart-aleck movie ... tosses around some intriguing questions about the difference between human and android life \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (25, 26), 'insert: [25,25]-[25,26]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (25, 26), 'insert: [25,25]-[25,26]'), (1, (23, 24), 'replace: [23,24]-[23,24]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"strung-togxther mom:nts  🏋️‍♀\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"strung-togxther mom:nts  🏋️‍♀\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"strung-togxther moments  🏋️‍♀\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"strung-togxther moments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"strung-together moments \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (1, 2), 'replace: [1,2]-[1,2]'), (0, (0, 1), 'replace: [0,1]-[0,1]'), (1, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\",Ggenerous and subversivh artworks  🧭\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\",Ggenerous and subversivh artworks  🧭\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\",Ggenerous and subversive artworks  🧭\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\",Ggenerous and subversive artworks \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\", generous and subversive artworks \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'insert: [4,4]-[4,5]'), (1, (2, 3), 'replace: [2,3]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"it does n't follow}the stale , standXrd , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world  🍝\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it does n't follow}the stale , standXrd , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world  🍝\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it does n't follow the stale , standXrd , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world  🍝\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it does n't follow the stale , standXrd , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it does n't follow the stale , standard , connect-the-dots storyline which has become commonplace in movies that explore the seamy underbelly of the criminal world \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(1, (26, 27), 'insert: [26,26]-[26,27]'), (0, (7, 8), 'replace: [7,8]-[7,8]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"funny yet  🦞\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"funny yet  🦞\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"funny \n",
       "  et  🦞\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"funny \n",
       "  et \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"funny yet \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (2, (1, 2), 'replace: [1,2]-[1,2]'), (1, (2, 3), 'insert: [2,2]-[2,3]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"over@earing an} over-the-top  🌧️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"over@earing an} over-the-top  🌧️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"overbearing an} over-the-top  🌧️\",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"overbearing an} over-the-top \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"overbearing and over-the-top \",\n",
       "  \t target=\"0\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (1, (3, 4), 'insert: [3,3]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (1, 2), 'replace: [1,2]-[1,2]'), (2, (0, 1), 'replace: [0,1]-[0,1]'), (1, (3, 4), 'insert: [3,3]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"it '` robert duvall 5  🏋️‍♀\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it '` robert duvall 5  🏋️‍♀\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's robert duvall 5  🏋️‍♀\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's robert duvall 5 \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"it 's robert duvall ! \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (4, 5), 'replace: [4,5]-[4,5]'), (2, (1, 2), 'replace: [1,2]-[1,2]'), (1, (5, 6), 'insert: [5,5]-[5,6]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"rich and sudden w/sdomm 👎🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich and sudden w/sdomm 👎🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich and sudden w/sdom  👎🏼\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich and sudden w/sdom \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"rich and sudden wisdom \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (1, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (3, 4), 'replace: [3,4]-[3,4]'), (2, (3, 4), 'replace: [3,4]-[3,4]'), (1, (4, 5), 'insert: [4,4]-[4,5]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>,\n",
       "  <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that washgngton most certainly has f new career ahead of him  🤦🏼‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (3, \"{'class': 'ContractContractions', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that washgngton most certainly has f new career ahead of him  🤦🏼‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(2, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that washington most certainly has f new career ahead of him  🤦🏼‍♂\",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\"), (1, \"{'class': 'AddNeutralEmoji', 'num': 1, 'polarity': [-0.05, 0.05], 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that washington most certainly has f new career ahead of him \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'RandomCharSubst', 'return_metadata': True}\")}>, 'prev': <LeRecord:\n",
       "  \t text=\"acted and directed , it 's clear that washington most certainly has a new career ahead of him \",\n",
       "  \t target=\"1\",\n",
       "  \t le_attrs={'transformation_provenance': <TransformationProvenance: set()>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'prev': None}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'replace: [12,13]-[12,13]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (12, 13), 'replace: [12,13]-[12,13]'), (1, (18, 19), 'insert: [18,18]-[18,19]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(2, (8, 9), 'replace: [8,9]-[8,9]'), (0, (12, 13), 'replace: [12,13]-[12,13]'), (1, (18, 19), 'insert: [18,18]-[18,19]')}>, 'granularity': 'word'}>, 'feature_provenance': <FeatureProvenance[edit_seq] set()>, 'granularity': 'word'}>]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dpml",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
