{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuWi1REPD_Ux"
   },
   "source": [
    "# DPML | Data Provenance for Machine Learning\n",
    "\n",
    "In this notebook, we investigate the provenance capabilities of TextAttack so that we can potentially modify it for our purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Zs0hYFFIYf"
   },
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpcEJzHdfCx2",
    "outputId": "f1a66fcc-871e-4dc0-93e4-7243d852bb2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/coraline/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8_koamxFHfR",
    "outputId": "e892403a-70b8-4728-ec18-c31ee7e81580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coraline/anaconda3/envs/dpml/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-28 18:31:42.452460: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import textattack\n",
    "from sibyl import ChangeSynonym, ExpandContractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktCv--paFOqG"
   },
   "source": [
    "## Create Datasets\n",
    "\n",
    "Examples of 3 primary kinds of datasets in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BgF7JI9_EbHt"
   },
   "outputs": [],
   "source": [
    "# Classification | 1 Text Input | 1 Int Output | Sentiment Analysis\n",
    "single_text_data = [(\"I enjoyed the movie a lot!\", 1), (\"Absolutely horrible film.\", 0), (\"Our family had a fun time!\", 1)]\n",
    "single_text_dataset = textattack.datasets.Dataset(single_text_data)\n",
    "\n",
    "# Classification | 2 Text Inputs | 1 Int Output | Natural Language Inference\n",
    "multi_text_data = [((\"A man inspects the uniform of a figure in some East Asian country.\", \"The man is sleeping\"), 1)]\n",
    "multi_text_dataset = textattack.datasets.Dataset(multi_text_data, input_columns=(\"premise\", \"hypothesis\"))\n",
    "\n",
    "# Seq2Seq | 1 Text Input | 1 Text Output\n",
    "seq2seq_data = [(\"J'aime le film.\", \"I love the movie.\")]\n",
    "seq2seq_dataset = textattack.datasets.Dataset(seq2seq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAWZzLBTPbFJ"
   },
   "source": [
    "## `AttackedText` Class\n",
    "\n",
    "This class tracks changes made to python strings and links back to previous version of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LSNm9R5xOHz0"
   },
   "outputs": [],
   "source": [
    "from textattack.shared import AttackedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OTzPdBkRnCG"
   },
   "source": [
    "### Single-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hInt1PEWPwg5"
   },
   "outputs": [],
   "source": [
    "single_text = single_text_dataset[0][0]['text']\n",
    "attacked_single_text = AttackedText(single_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aZ4PvA7WQgGo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I enjoyed the movie a lot!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacked_single_text.printable_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I05uYlN1Ro1_"
   },
   "source": [
    "### Multi-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QXSkDc5QQcDh"
   },
   "outputs": [],
   "source": [
    "multi_text = multi_text_dataset[0][0]\n",
    "attacked_multi_text = AttackedText(multi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_usl2WJnPy3l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Premise: A man inspects the uniform of a figure in some East Asian country.\\nHypothesis: The man is sleeping'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacked_multi_text.printable_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os00h06bRsqd"
   },
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xbLOJ4B3QRgD"
   },
   "outputs": [],
   "source": [
    "seq2seq_text = seq2seq_dataset[0][0]\n",
    "attacked_seq2seq_text = AttackedText(seq2seq_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TdV4cOdIR0Lz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime le film.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacked_seq2seq_text.printable_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FptulT_03eIN"
   },
   "source": [
    "## `LeText` | Lineage-enabled Text\n",
    "\n",
    "A class that represents a string that can be transformed (or attacked), tracking the changes made to text and label components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMqB0o0ndbt7"
   },
   "source": [
    "#### Demo `diff_text` functionality for different granularities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cX7WGOCQ9zny"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import difflib\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1rUx9OeVOn8l"
   },
   "outputs": [],
   "source": [
    "def diff_text(a, b, granularity=\"word\"): \n",
    "    \"\"\"\n",
    "    Intakes two text documents and optionally parses then for a desired\n",
    "    granularity in ['paragraph', 'sentence', 'word', 'character'].\n",
    "\n",
    "    Returns a the optionally parsed documents as well as a list of \n",
    "    difflib.SequenceMatcher.opcodes where tags reflect the type of \n",
    "    opertion and the indices reflect the desired granularities.\n",
    "\n",
    "    opcode tags\n",
    "      - 'replace' | a[i1:i2] should be replaced by b[j1:j2].\n",
    "      - 'delete'  | a[i1:i2] should be deleted. \n",
    "      - 'insert'  | b[j1:j2] should be inserted at a[i1:i1]. \n",
    "      - 'equal'   | a[i1:i2] == b[j1:j2] (the sub-sequences are equal).\n",
    "    \"\"\"\n",
    "\n",
    "    # parse texts to desired granularity\n",
    "    if granularity == \"paragraph\":\n",
    "        parsed_a = a.split('\\n')\n",
    "        parsed_b = b.split('\\n')\n",
    "    elif granularity == \"sentence\":\n",
    "        sent_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "        parsed_a = sent_tokenizer.tokenize(a)\n",
    "        parsed_b = sent_tokenizer.tokenize(b)\n",
    "    elif granularity == \"word\":\n",
    "        parsed_a = a.split()\n",
    "        parsed_b = b.split()\n",
    "    elif granularity == \"character\":\n",
    "        # no change necessary, difflib is character-level by default\n",
    "        parsed_a = a\n",
    "        parsed_b = b\n",
    "\n",
    "    seq = difflib.SequenceMatcher(None, parsed_a, parsed_b)\n",
    "\n",
    "    return parsed_a, parsed_b, seq.get_opcodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "akZx2ZF5QG2n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "granularity: paragraph\n",
      "replace parsed_a[0:1] (['This is sentence one in paragraph one. This is sentence two in paragraph one.']) parsed_b[0:1] (['This be sentence one in paragraph one. This iz sentence two in paragraph one.'])\n",
      "  equal parsed_a[1:2] (['This is sentence one in paragraph two. This is sentence two in paragraph two.']) parsed_b[1:2] (['This is sentence one in paragraph two. This is sentence two in paragraph two.'])\n",
      "\n",
      "granularity: sentence\n",
      "replace parsed_a[0:2] (['This is sentence one in paragraph one.', 'This is sentence two in paragraph one.']) parsed_b[0:2] (['This be sentence one in paragraph one.', 'This iz sentence two in paragraph one.'])\n",
      "  equal parsed_a[2:4] (['This is sentence one in paragraph two.', 'This is sentence two in paragraph two.']) parsed_b[2:4] (['This is sentence one in paragraph two.', 'This is sentence two in paragraph two.'])\n",
      "\n",
      "granularity: word\n",
      "  equal parsed_a[0:1] (['This']) parsed_b[0:1] (['This'])\n",
      "replace parsed_a[1:2] (['is']) parsed_b[1:2] (['be'])\n",
      "  equal parsed_a[2:8] (['sentence', 'one', 'in', 'paragraph', 'one.', 'This']) parsed_b[2:8] (['sentence', 'one', 'in', 'paragraph', 'one.', 'This'])\n",
      "replace parsed_a[8:9] (['is']) parsed_b[8:9] (['iz'])\n",
      "  equal parsed_a[9:28] (['sentence', 'two', 'in', 'paragraph', 'one.', 'This', 'is', 'sentence', 'one', 'in', 'paragraph', 'two.', 'This', 'is', 'sentence', 'two', 'in', 'paragraph', 'two.']) parsed_b[9:28] (['sentence', 'two', 'in', 'paragraph', 'one.', 'This', 'is', 'sentence', 'one', 'in', 'paragraph', 'two.', 'This', 'is', 'sentence', 'two', 'in', 'paragraph', 'two.'])\n",
      "\n",
      "granularity: character\n",
      "  equal parsed_a[0:5] (This ) parsed_b[0:5] (This )\n",
      "replace parsed_a[5:7] (is) parsed_b[5:7] (be)\n",
      "  equal parsed_a[7:45] ( sentence one in paragraph one. This i) parsed_b[7:45] ( sentence one in paragraph one. This i)\n",
      "replace parsed_a[45:46] (s) parsed_b[45:46] (z)\n",
      "  equal parsed_a[46:155] ( sentence two in paragraph one.\n",
      "This is sentence one in paragraph two. This is sentence two in paragraph two.) parsed_b[46:155] ( sentence two in paragraph one.\n",
      "This is sentence one in paragraph two. This is sentence two in paragraph two.)\n"
     ]
    }
   ],
   "source": [
    "a = \"\"\"This is sentence one in paragraph one. This is sentence two in paragraph one.\n",
    "This is sentence one in paragraph two. This is sentence two in paragraph two.\"\"\"\n",
    "b = \"\"\"This be sentence one in paragraph one. This iz sentence two in paragraph one.\n",
    "This is sentence one in paragraph two. This is sentence two in paragraph two.\"\"\"\n",
    "\n",
    "for granularity in ['paragraph', 'sentence', 'word', 'character']:\n",
    "    print('\\ngranularity:', granularity)\n",
    "    parsed_a, parsed_b, changes = diff_text(a, b, granularity)\n",
    "    for (tag, i1, i2, j1, j2) in changes:\n",
    "        print (\"%7s parsed_a[%d:%d] (%s) parsed_b[%d:%d] (%s)\" %\n",
    "              (tag, i1, i2, parsed_a[i1:i2], j1, j2, parsed_b[j1:j2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKyEYzYvdjwZ"
   },
   "source": [
    "#### `LeText` Class for **L**ineage-**e**nabled **Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NB87-cEB8TUD"
   },
   "outputs": [],
   "source": [
    "class LeText:\n",
    "    \"\"\"\n",
    "    A helper class that represents a string that can be transformed, \n",
    "    tracking the transformations made to it.\n",
    "\n",
    "    Modifying ``LeText`` instances results in the generation of new ``LeText`` \n",
    "    instances with a reference pointer (``le_attrs[\"previous\"]``), so that \n",
    "    the full chain of transforms might be reconstructed by using this key to \n",
    "    form a linked list.\n",
    "\n",
    "    Args:\n",
    "       text (str or dict): The string or dict that this ``LeText`` represents\n",
    "       granularity (str): Specifies the default level at which \n",
    "            lineage should be tracked. Value must be in:\n",
    "                ['paragraph', 'sentence', 'word', 'character']\n",
    "       le_attrs (dict): Dictionary of various attributes stored while \n",
    "            transforming the underlying text. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_input, granularity=\"word\", le_attrs=None):\n",
    "        # Read in ``text_input`` as a string .\n",
    "        if isinstance(text_input, str):\n",
    "            self._text_input = {\"text\": text_input}\n",
    "        elif isinstance(text_input, dict):\n",
    "            self._text_input = text_input\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Invalid text_input type {type(text_input)} (requires str or dict)\"\n",
    "            )\n",
    "\n",
    "        if granularity in ['paragraph', 'sentence', 'word', 'character']:\n",
    "          \n",
    "            self.granularity = granularity\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Invalid granularity {granularity} (must be one of the \\\n",
    "                following: ['paragraph', 'sentence', 'word', 'character'])\"\n",
    "            )\n",
    "        \n",
    "        # Process input lazily.\n",
    "        self._chars = None\n",
    "        self._words = None\n",
    "        self._sents = None\n",
    "        self._paras = None\n",
    "\n",
    "        # create le_attrs if none exists\n",
    "        if le_attrs is None:\n",
    "            self.le_attrs = dict()\n",
    "        elif isinstance(le_attrs, dict):\n",
    "            self.le_attrs = le_attrs\n",
    "        else:\n",
    "            raise TypeError(f\"Non-dict provided for le_attrs: {type(le_attrs)}.\")\n",
    "\n",
    "        # parsers\n",
    "        self.sent_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "\n",
    "        # Lineage Attributes\n",
    "        self.le_attrs.setdefault(\"granularity\", self.granularity)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Compares two LeText instances, making sure that they also share\n",
    "        the same lineage attributes.\n",
    "\n",
    "        Since some elements stored in ``self.le_attrs`` may be numpy\n",
    "        arrays, we have to take special care when comparing them.\n",
    "        \"\"\"\n",
    "        if not (self.text == other.text):\n",
    "            return False\n",
    "        if len(self.le_attrs) != len(other.le_attrs):\n",
    "            return False\n",
    "        for key in self.le_attrs:\n",
    "            if key not in other.le_attrs:\n",
    "                return False\n",
    "            elif isinstance(self.le_attrs[key], np.ndarray):\n",
    "                if not (self.le_attrs[key].shape == other.le_attrs[key].shape):\n",
    "                    return False\n",
    "                elif not (self.le_attrs[key] == other.le_attrs[key]).all():\n",
    "                    return False\n",
    "            else:\n",
    "                if not self.le_attrs[key] == other.le_attrs[key]:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.text)\n",
    "\n",
    "    def apply(self, fn, granularity=\"word\"):\n",
    "        \"\"\"\n",
    "        Applies fn(self.text), tracking the transformation info and output as\n",
    "        a new LeText instance with a reference back to the source LeText.\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the provided function to the text stored in LeText\n",
    "        output_text = fn(self.text)\n",
    "\n",
    "        # find changes between self.text and output_text\n",
    "        parsed_a, parsed_b, changes = diff_text(self.text, output_text, granularity)\n",
    "\n",
    "        new_le_attrs = {\n",
    "            \"granularity\": self.granularity,\n",
    "            \"changes\": changes,\n",
    "            \"transformation\": fn,\n",
    "            \"previous\": self # current LeText\n",
    "        }\n",
    "        \n",
    "        output_LeText = LeText(output_text, \n",
    "                               granularity=granularity,\n",
    "                               le_attrs=new_le_attrs)\n",
    "        \n",
    "        return output_LeText\n",
    "        \n",
    "    def parse_text(self):\n",
    "        if self.granularity == \"paragraph\":\n",
    "            parsed_text = self.text.split('\\n')\n",
    "        elif self.granularity == \"sentence\":\n",
    "            parsed_text = self.sent_tokenizer.tokenize(self.text)\n",
    "        elif self.granularity == \"word\":\n",
    "            parsed_text = self.text.split()\n",
    "        elif self.granularity == \"character\":\n",
    "            parsed_text = list(self.text)\n",
    "        return parsed_text\n",
    "\n",
    "    @property\n",
    "    def column_labels(self):\n",
    "        \"\"\"Returns the labels for this text's columns.\n",
    "\n",
    "        For single-sequence inputs, this simply returns ['text'].\n",
    "        \"\"\"\n",
    "        return list(self._text_input.keys())\n",
    "\n",
    "    @property\n",
    "    def chars(self):\n",
    "        if not self._chars:\n",
    "            self._chars = list(self.text)\n",
    "        return self._chars\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if not self._words:\n",
    "            self._words = self.text.split()\n",
    "        return self._words\n",
    "\n",
    "    @property\n",
    "    def sents(self):\n",
    "        if not self._sents:\n",
    "            self._sents = self.sent_tokenizer(self.text)\n",
    "        return self._sents\n",
    "\n",
    "    @property\n",
    "    def paras(self):\n",
    "        if not self._paras:\n",
    "            self._paras = self.text.split(\"\\n\")\n",
    "        return self._paras\n",
    "\n",
    "    @property\n",
    "    def num_chars(self):\n",
    "        \"\"\"Returns the number of characters in the text.\"\"\"\n",
    "        return len(self.chars)\n",
    "\n",
    "    @property\n",
    "    def num_words(self):\n",
    "        \"\"\"Returns the number of words in the text.\"\"\"\n",
    "        return len(self.words)\n",
    "\n",
    "    @property\n",
    "    def num_sents(self):\n",
    "        \"\"\"Returns the number of sentences in the text.\"\"\"\n",
    "        return len(self.sents)\n",
    "\n",
    "    @property\n",
    "    def num_paras(self):\n",
    "        \"\"\"Returns the number of paragraphs in the text.\"\"\"\n",
    "        return len(self.paras)\n",
    "\n",
    "    @property\n",
    "    def num_units(self):\n",
    "        \"\"\"Returns the number of \"units\" of text given the default granularity.\"\"\"\n",
    "        if self.granularity == \"paragraph\":\n",
    "            return self.num_paras\n",
    "        elif self.granularity == \"sentence\":\n",
    "            return self.num_sents\n",
    "        elif self.granularity == \"word\":\n",
    "            return self.num_words\n",
    "        elif self.granularity == \"character\":\n",
    "            return self.num_chars\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"Represents full text input.\n",
    "\n",
    "        Multiple inputs are joined with a line break.\n",
    "        \"\"\"\n",
    "        return \"\\n\".join(self._text_input.values())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<LeText \"{self.text}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UXQu8fRJ_AC0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeText \"Don't you just love pizza?\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'granularity': 'word'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = LeText(\"Don't you just love pizza?\")\n",
    "print(text1)\n",
    "text1.le_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BlTgAExrApZo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeText \"Don't you just bed pizza?\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'granularity': 'word',\n",
       " 'changes': [('equal', 0, 3, 0, 3),\n",
       "  ('replace', 3, 4, 3, 4),\n",
       "  ('equal', 4, 5, 4, 5)],\n",
       " 'transformation': <sibyl.transformations.text.word_swap.change_synse.ChangeSynonym at 0x7f95020b28d0>,\n",
       " 'previous': <LeText \"Don't you just love pizza?\">}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = text1.apply(ChangeSynonym())\n",
    "print(text2)\n",
    "text2.le_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eO2AN62sbSRq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeText \"Do not you just bed pizza?\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'granularity': 'word',\n",
       " 'changes': [('replace', 0, 1, 0, 2), ('equal', 1, 5, 2, 6)],\n",
       " 'transformation': <sibyl.transformations.text.contraction.expand_contractions.ExpandContractions at 0x7f95020b2c50>,\n",
       " 'previous': <LeText \"Don't you just bed pizza?\">}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = text2.apply(ExpandContractions())\n",
    "print(text3)\n",
    "text3.le_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "n-IpI9DCcE21"
   },
   "outputs": [],
   "source": [
    "class LeTarget:\n",
    "    \"\"\"\n",
    "    A helper class that represents target data (e.g. most likely a label in the \n",
    "    case of classification, but could also be text or some other arbitary\n",
    "    data structure for other tasks). This target can be transformed, as with \n",
    "    sibylvariant augmentations, and this class tracks changes to the target.\n",
    "\n",
    "    Modifying ``LeTarget`` instances results in the generation of new ``LeTarget`` \n",
    "    instances with a reference pointer (``le_attrs[\"previous\"]``), so that \n",
    "    the full chain of transforms might be reconstructed by using this key to \n",
    "    form a linked list.\n",
    "\n",
    "    Args:\n",
    "       target (any): The data which may be transformed\n",
    "       le_attrs (dict): Dictionary of various attributes stored while \n",
    "            transforming the underlying data. \n",
    "\n",
    "    TODO:\n",
    "        - type check target:\n",
    "            - if isinstance(target, int):   e.g. classification\n",
    "            - if isinstance(target, float): e.g. regression\n",
    "            - if ifinstance(target, str):   e.g. seq2seq (save as LeText?)\n",
    "            - if ifinstance(target, list):  e.g. multi-label classification, soft-label\n",
    "            - if ifinstance(target, dict):  e.g. pose detection\n",
    "        - determine how to do diff-ing for other data types\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target, le_attrs=None):\n",
    "\n",
    "        self.target = target\n",
    "        self.target_type = type(target)\n",
    "        \n",
    "        # create le_attrs if none exists\n",
    "        if le_attrs is None:\n",
    "            self.le_attrs = dict()\n",
    "        elif isinstance(le_attrs, dict):\n",
    "            self.le_attrs = le_attrs\n",
    "        else:\n",
    "            raise TypeError(f\"Non-dict provided for le_attrs: {type(le_attrs)}.\")\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Compares two LeTarget instances, making sure that they also share\n",
    "        the same lineage attributes.\n",
    "\n",
    "        Since some elements stored in ``self.le_attrs`` may be numpy\n",
    "        arrays, we have to take special care when comparing them.\n",
    "        \"\"\"\n",
    "        if not (self.target == other.target):\n",
    "            return False\n",
    "        if len(self.le_attrs) != len(other.le_attrs):\n",
    "            return False\n",
    "        for key in self.le_attrs:\n",
    "            if key not in other.le_attrs:\n",
    "                return False\n",
    "            elif isinstance(self.le_attrs[key], np.ndarray):\n",
    "                if not (self.le_attrs[key].shape == other.le_attrs[key].shape):\n",
    "                    return False\n",
    "                elif not (self.le_attrs[key] == other.le_attrs[key]).all():\n",
    "                    return False\n",
    "            else:\n",
    "                if not self.le_attrs[key] == other.le_attrs[key]:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.target)\n",
    "\n",
    "    def apply(self, fn):\n",
    "        \"\"\"\n",
    "        Applies fn(self.target), tracking the transformation info and output as\n",
    "        a new LeTarget instance with a reference back to the source LeTarget.\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the provided function to the target stored in LeTarget\n",
    "        output_target = fn(self.target)\n",
    "\n",
    "        # diff \n",
    "        # changes = ???\n",
    "\n",
    "        new_le_attrs = {\n",
    "            \"transformation\": fn,\n",
    "            # \"changes\": changes, \n",
    "            \"previous\": self # current LeTarget\n",
    "        }\n",
    "        \n",
    "        output_LeTarget = LeTarget(output_target, \n",
    "                               le_attrs=new_le_attrs)\n",
    "        \n",
    "        return output_LeTarget\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<LeTarget \"{self.target}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OlEGAd6fu7MI"
   },
   "outputs": [],
   "source": [
    "from sibyl import invert_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "xFsFD70Eu3YR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeTarget \"0\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1 = LeTarget(0)\n",
    "print(target1)\n",
    "target1.le_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sldDuMK4vOV9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeTarget \"1\">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transformation': <function sibyl.transformations.utils.invert_label(y, soften=False, num_classes=None)>,\n",
       " 'previous': <LeTarget \"0\">}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2 = target1.apply(invert_label)\n",
    "print(target2)\n",
    "target2.le_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DwD58zmtR9HV"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "class LeRecord:\n",
    "    \"\"\"\n",
    "    A class that represents a data record that can be transformed (or attacked), \n",
    "    tracking the changes made to input (text) and target components. \n",
    "\n",
    "    Models that take multiple documents as input separate them via \n",
    "    ``SPLIT_TOKEN`` by default. \n",
    "    Transformations may be be applied in one of the following ways:\n",
    "      - The entire input, joined into one string, without ``SPLIT_TOKEN``.\n",
    "      - Specific documents associated with the data record without affecting\n",
    "        other documents. \n",
    "\n",
    "    ``LeRecord`` instances that were transformed from other ``LeRecord``\n",
    "    objects contain a pointer to the previous instance via an attributes\n",
    "    dictionary (i.e. ``le_attrs[\"previous_LeRecord\"]``), so that the full chain of\n",
    "    transformations might be reconstructed by using this key to form a linked\n",
    "    list.\n",
    "\n",
    "    Args:\n",
    "       record (dict): Dictionary of inputs, labels, and potentially other data\n",
    "           associated with dataset record. \n",
    "       granularity (str): Specifies the default level at which \n",
    "            lineage should be tracked. Value must be in:\n",
    "                ['paragraph', 'sentence', 'word', 'character']\n",
    "       text_key (str or List[str]): key value(s) for the text input(s)\n",
    "       target_key (str or List[str]): key value(s) for the target(s)\n",
    "       le_attrs (dict): Dictionary of various attributes stored\n",
    "           while transforming the underlying data record.\n",
    "    \"\"\"\n",
    "\n",
    "    SPLIT_TOKEN = \"<SPLIT>\"\n",
    "\n",
    "    def __init__(self, record, \n",
    "                 granularity=\"word\", \n",
    "                 text_keys=None,\n",
    "                 target_keys=None,\n",
    "                 le_attrs=None):\n",
    "        if not isinstance(record, dict):\n",
    "            raise TypeError(\n",
    "                f\"Invalid text_input type {type(record)} (required dict)\"\n",
    "            )\n",
    "        \n",
    "        self.record = record\n",
    "        self.granularity = granularity\n",
    "        self.text_keys = text_keys\n",
    "        self.target_keys = target_keys\n",
    "\n",
    "        # if no text keys are given, guess 'em\n",
    "        if not self.text_keys:\n",
    "            self.text_keys = self.infer_text_keys()\n",
    "\n",
    "        # if no target keys are given, guess 'em\n",
    "        if not self.target_keys:\n",
    "            self.target_keys = self.infer_target_keys()\n",
    "\n",
    "        # parse record into input and target components\n",
    "        self.extract_texts_and_targets()\n",
    "\n",
    "        self.add_lineage()\n",
    "\n",
    "    def infer_text_keys(self):\n",
    "        \"\"\"\n",
    "        Traverses self.record and guesses which keys contain the text inputs\n",
    "        \"\"\"\n",
    "        text_keys = []\n",
    "        for k, v in self.record.items():\n",
    "            if isinstance(v, LeText) or k not in [\"label\", \"target\"]:\n",
    "                text_keys.append(k)\n",
    "        return text_keys\n",
    "\n",
    "    def infer_target_keys(self):\n",
    "        \"\"\"\n",
    "        Traverses self.record and guesses which keys contain the target\n",
    "        \"\"\"\n",
    "        target_keys = []\n",
    "        for k, v in self.record.items():\n",
    "            if isinstance(v, LeTarget) or k in [\"label\", \"target\"]:\n",
    "                target_keys.append(k)\n",
    "        return target_keys\n",
    "\n",
    "    def extract_texts_and_targets(self):\n",
    "        self.texts, self.targets = [], []\n",
    "        for key in self.text_keys:\n",
    "            text = self.record[key]\n",
    "            if isinstance(text, LeText):\n",
    "                text = text.text\n",
    "            self.texts.append(text)\n",
    "        for key in self.target_keys:\n",
    "            target = self.record[key]\n",
    "            if isinstance(target, LeTarget):\n",
    "                target = target.target\n",
    "            self.targets.append(target)\n",
    "            \n",
    "    def backgenerate_LeText(self, old_text, new_text, fn, granularity=None):\n",
    "        if not granularity:\n",
    "            granularity = self.granularity\n",
    "        \n",
    "        if not isinstance(old_text, LeText):\n",
    "            old_text = LeText(old_text)\n",
    "            \n",
    "        parsed_a, parsed_b, changes = diff_text(old_text.text, \n",
    "                                                new_text, \n",
    "                                                granularity)\n",
    "        le_attrs = {\n",
    "            \"granularity\": granularity,\n",
    "            \"changes\": changes,\n",
    "            \"transformation\": fn,\n",
    "            \"previous\": old_text\n",
    "        }\n",
    "        new_LeText = LeText(new_text, le_attrs=le_attrs)\n",
    "        return new_LeText\n",
    "\n",
    "    def backgenerate_LeTarget(self, old_target, new_target, fn):     \n",
    "        if not isinstance(old_target, LeTarget):\n",
    "            old_target = LeTarget(old_target)\n",
    "\n",
    "        le_attrs = {\n",
    "            # \"changes\": changes,\n",
    "            \"transformation\": fn,\n",
    "            \"previous\": old_target\n",
    "        }\n",
    "        new_LeTarget = LeTarget(new_target, le_attrs=le_attrs)\n",
    "        return new_LeTarget\n",
    "\n",
    "    def add_lineage(self):\n",
    "        for k in self.text_keys:\n",
    "            v = self.record[k]\n",
    "            if not isinstance(v, LeText):\n",
    "               self.record[k] = LeText(v)\n",
    "        for k in self.target_keys:\n",
    "            v = self.record[k]\n",
    "            if not isinstance(v, LeTarget):\n",
    "               self.record[k] = LeTarget(v)\n",
    "        \n",
    "    def remove_lineage(self):\n",
    "        for k, v in record.items():\n",
    "            if isinstance(v, LeText):\n",
    "                v = v.text\n",
    "            if isinstance(v, LeTarget):\n",
    "                v = v.target\n",
    "            self.record[k] = v\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.record)\n",
    "\n",
    "    def apply(self, fn, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Applies fn(self.record), tracking the transformation info \n",
    "        and outputs as a new LeRecord instance with a reference back to the \n",
    "        source LeRecord. Text and target indices are transformed separately. \n",
    "        Args:\n",
    "          fn (function): function which intakes a dict and returns a dict\n",
    "        \"\"\"\n",
    "\n",
    "        # apply transform to self.record\n",
    "        self.remove_lineage()\n",
    "        output_record = fn(self.record, *args, **kwargs)\n",
    "\n",
    "        output_record = LeRecord(output_record)\n",
    "        new_texts = output_record.texts\n",
    "        new_targets = output_record.targets\n",
    "\n",
    "        # recreate record from text components\n",
    "        _new_texts = {}\n",
    "        for key, old_text, new_text in zip(self.text_keys, self.texts, new_texts):\n",
    "            _new_texts[key] = self.backgenerate_LeText(old_text, new_text, fn)\n",
    "\n",
    "        # recreate record from target components\n",
    "        _new_targets = {}\n",
    "        for key, old_target, new_target in zip(self.target_keys, self.targets, new_targets):\n",
    "            _new_targets[key] = self.backgenerate_LeTarget(old_target, new_target, fn)\n",
    "\n",
    "        # join text and target components together\n",
    "        # output_record = _new_texts | _new_targets # only python 3.9+\n",
    "        output_record = {**_new_texts, **_new_targets}\n",
    "\n",
    "        # diff \n",
    "        # changes = ???\n",
    "        \n",
    "        new_le_attrs = {\n",
    "            \"transformation\": fn,\n",
    "            # \"changes\": changes, \n",
    "            \"previous\": self # current LeRecord\n",
    "        }\n",
    "        \n",
    "        output_LeRecord = LeRecord(output_record, \n",
    "                                   le_attrs=new_le_attrs)\n",
    "        \n",
    "        return output_LeRecord\n",
    "\n",
    "    def apply_to_components(self, fn, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Applies fn(self.record), tracking the transformation info \n",
    "        and outputs as a new LeRecord instance with a reference back to the \n",
    "        source LeRecord. Text and target indices are transformed separately. \n",
    "        Args:\n",
    "          fn (function): transformation function which intakes text and target \n",
    "                         separately and returns them in a similar way\n",
    "        \"\"\"\n",
    "\n",
    "        # apply transform to self.texts, self.targets\n",
    "        new_texts, new_targets = fn(self.texts, self.targets, *args, **kwargs)\n",
    "\n",
    "        # recreate record from text components\n",
    "        if not isinstance(new_texts, list):\n",
    "            new_texts = [new_texts]\n",
    "\n",
    "        _new_texts = {}\n",
    "        for key, old_text, new_text in zip(self.text_keys, self.texts, new_texts):\n",
    "            _new_texts[key] = self.backgenerate_LeText(old_text, new_text, fn)\n",
    "\n",
    "        # recreate record from target components\n",
    "        if not isinstance(new_targets, Iterable):\n",
    "            new_targets = [new_targets]\n",
    "\n",
    "        _new_targets = {}\n",
    "        for key, old_target, new_target in zip(self.target_keys, self.targets, new_targets):\n",
    "            _new_targets[key] = self.backgenerate_LeTarget(old_target, new_target, fn)\n",
    "\n",
    "        # join text and target components together\n",
    "        # output_record = _new_texts | _new_targets # only python 3.9+\n",
    "        output_record = {**_new_texts, **_new_targets}\n",
    "\n",
    "        # diff \n",
    "        # changes = ???\n",
    "\n",
    "        new_le_attrs = {\n",
    "            \"transformation\": fn,\n",
    "            # \"changes\": changes, \n",
    "            \"previous\": self # current LeRecord\n",
    "        }\n",
    "        \n",
    "        output_LeRecord = LeRecord(output_record, \n",
    "                                   le_attrs=new_le_attrs)\n",
    "        \n",
    "        return output_LeRecord\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<LeRecord \"{self.record}\">'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IbhP31GWw_35"
   },
   "outputs": [],
   "source": [
    "record1 = {\n",
    "    \"text\": \"This is a single input record.\",\n",
    "    \"label\": 0\n",
    "}\n",
    "\n",
    "record2 = {\n",
    "    \"text1\": \"This is a double input record - 1.\",\n",
    "    \"text2\": \"This is a double input record - 2.\",\n",
    "    \"label\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JRSnFgCL1UN"
   },
   "source": [
    "#### `LeRecord` | Toy Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0BUodSqJL9bA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 text + 1 target record + apply\n",
      "\n",
      "<LeRecord \"{'text': <LeText \"This is a single input record.\">, 'label': <LeTarget \"0\">}\">\n",
      "<LeRecord \"{'text': <LeText \"This is a single input record. This is a test.\">, 'label': <LeTarget \"1\">}\">\n",
      "<LeRecord \"{'text': <LeText \"This is a single input record. This is a test.\">, 'label': <LeTarget \"1\">}\">\n",
      "\n",
      "2 text + 1 target record + apply\n",
      "\n",
      "<LeRecord \"{'text1': <LeText \"This is a double input record - 1.\">, 'text2': <LeText \"This is a double input record - 2.\">, 'label': <LeTarget \"0\">}\">\n",
      "<LeRecord \"{'text1': <LeText \"This is a double input record - 1. Yo.\">, 'text2': <LeText \"This is a double input record - 2. Whoa!!!\">, 'label': <LeTarget \"1\">}\">\n",
      "<LeRecord \"{'text1': <LeText \"This is a double input record - 1. Yo.\">, 'text2': <LeText \"This is a double input record - 2. Whoa!!!\">, 'label': <LeTarget \"1\">}\">\n",
      "\n",
      "1 text + 1 target record + apply_to_components\n",
      "\n",
      "<LeRecord \"{'text': <LeText \"This is a single input record.\">, 'label': <LeTarget \"0\">}\">\n",
      "<LeRecord \"{'text': <LeText \"This is a single input record. This is a test.\">, 'label': <LeTarget \"1\">}\">\n",
      "<LeRecord \"{'text': <LeText \"This is a single input record. This is a test. This is a test.\">, 'label': <LeTarget \"0\">}\">\n",
      "\n",
      "2 text + 1 target record + apply_to_components\n",
      "\n",
      "<LeRecord \"{'text1': <LeText \"This is a double input record - 1.\">, 'text2': <LeText \"This is a double input record - 2.\">, 'label': <LeTarget \"0\">}\">\n",
      "<LeRecord \"{'text1': <LeText \"This is a double input record - 1. Yo.\">, 'text2': <LeText \"This is a double input record - 2.Whoa!!!\">, 'label': <LeTarget \"1\">}\">\n",
      "<LeRecord \"{'text1': <LeText \"This is a double input record - 1. Yo. Yo.\">, 'text2': <LeText \"This is a double input record - 2.Whoa!!!Whoa!!!\">, 'label': <LeTarget \"0\">}\">\n"
     ]
    }
   ],
   "source": [
    "# 1 text + 1 target record + apply\n",
    "print(\"1 text + 1 target record + apply\\n\")\n",
    "\n",
    "def record_transformer(record):\n",
    "    text = record['text']\n",
    "    label = record['label']\n",
    "\n",
    "    new_text = text + \" This is a test.\"\n",
    "    new_label = invert_label(label)\n",
    "    return {\"text\": new_text, \"label\": new_label}\n",
    "\n",
    "record = record1\n",
    "\n",
    "le_record1 = LeRecord(record)\n",
    "print(le_record1)\n",
    "# print(le_record1.texts, le_record1.targets)\n",
    "\n",
    "le_record2 = le_record1.apply(record_transformer)\n",
    "print(le_record2)\n",
    "# print(le_record2.texts, le_record2.targets)\n",
    "\n",
    "le_record3 = le_record2.apply(record_transformer)\n",
    "print(le_record3)\n",
    "# print(le_record3.texts, le_record3.targets)\n",
    "\n",
    "# 2 text + 1 target record + apply\n",
    "print(\"\\n2 text + 1 target record + apply\\n\")\n",
    "\n",
    "def record_transformer2(record):\n",
    "    text1 = record['text1']\n",
    "    text2 = record['text2']\n",
    "    label = record['label']\n",
    "\n",
    "    new_text1 = text1 + \" Yo.\"\n",
    "    new_text2 = text2 + \" Whoa!!!\"\n",
    "    new_label = invert_label(label)\n",
    "    return {\"text1\": new_text1, \"text2\": new_text2, \"label\": new_label}\n",
    "\n",
    "record = record2\n",
    "\n",
    "le_record1 = LeRecord(record)\n",
    "print(le_record1)\n",
    "# print(le_record1.texts, le_record1.targets)\n",
    "\n",
    "le_record2 = le_record1.apply(record_transformer2)\n",
    "print(le_record2)\n",
    "# print(le_record2.texts, le_record2.targets)\n",
    "\n",
    "le_record3 = le_record2.apply(record_transformer2)\n",
    "print(le_record3)\n",
    "# print(le_record3.texts, le_record3.targets)\n",
    "\n",
    "# 1 text + 1 target record + apply_to_components\n",
    "print(\"\\n1 text + 1 target record + apply_to_components\\n\")\n",
    "\n",
    "def record_components_transformer(text, label):\n",
    "    new_text = text[0] + \" This is a test.\"\n",
    "    new_label = invert_label(label[0])\n",
    "    return new_text, new_label\n",
    "\n",
    "record = record1\n",
    "\n",
    "le_record1 = LeRecord(record)\n",
    "print(le_record1)\n",
    "# print(le_record1.texts, le_record1.targets)\n",
    "\n",
    "le_record2 = le_record1.apply_to_components(record_components_transformer)\n",
    "print(le_record2)\n",
    "# print(le_record2.texts, le_record2.targets)\n",
    "\n",
    "le_record3 = le_record2.apply_to_components(record_components_transformer)\n",
    "print(le_record3)\n",
    "# print(le_record3.texts, le_record3.targets)\n",
    "\n",
    "# 2 text + 1 target record + apply_to_components\n",
    "print(\"\\n2 text + 1 target record + apply_to_components\\n\")\n",
    "\n",
    "def record_components_transformer2(texts, label):\n",
    "    new_text = [texts[0] + \" Yo.\", texts[1] + \"Whoa!!!\"]\n",
    "    new_label = invert_label(label[0])\n",
    "    return new_text, new_label\n",
    "\n",
    "record = record2\n",
    "\n",
    "le_record1 = LeRecord(record)\n",
    "print(le_record1)\n",
    "# print(le_record1.texts, le_record1.targets)\n",
    "\n",
    "le_record2 = le_record1.apply_to_components(record_components_transformer2)\n",
    "print(le_record2)\n",
    "# print(le_record2.texts, le_record2.targets)\n",
    "\n",
    "le_record3 = le_record2.apply_to_components(record_components_transformer2)\n",
    "print(le_record3)\n",
    "# print(le_record3.texts, le_record3.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA5Ejv92QTIN"
   },
   "source": [
    "#### `LeRecord` | Sibyl Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gjkya0e-QVB1"
   },
   "outputs": [],
   "source": [
    "from sibyl import ChangeAntonym, InsertNegativePhrase, TextMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "O_m_cwRFQWzb"
   },
   "outputs": [],
   "source": [
    "task_config = {\n",
    "    \"input_idx\": [1],\n",
    "    \"task_name\": \"sentiment\",\n",
    "    \"tran_type\": \"SIB\",\n",
    "    \"label_type\": \"hard\"\n",
    "}\n",
    "\n",
    "def trans_fn(X, y, task_config):\n",
    "    if isinstance(y, list):\n",
    "        y = y[0]\n",
    "    new_X, new_y = ChangeAntonym().transform_Xy(X, y, task_config)\n",
    "    return new_X, new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Qr-lTQ2m35U"
   },
   "source": [
    "##### `ChangeAntonym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "StIt0W1ZUbT8"
   },
   "outputs": [],
   "source": [
    "record1 = {\n",
    "    \"text\": \"This is a nasty input record.\",\n",
    "    \"label\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "csOIC7cwXLPO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeText \"This is a double input record - 1. Yo. Yo.\">\n",
      "<LeText \"This is a double input record - 1. Yo.\">\n"
     ]
    }
   ],
   "source": [
    "print(le_record3.record['text1'])\n",
    "print(le_record3.record['text1'].le_attrs['previous'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "w2rm83rHPMkW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LeRecord \"{'text': <LeText \"This is a nasty input record.\">, 'label': <LeTarget \"0\">}\">\n",
      "<LeRecord \"{'text': <LeText \"This is a nice input record.\">, 'label': <LeTarget \"1\">}\">\n",
      "<LeRecord \"{'text': <LeText \"This is a awful input record.\">, 'label': <LeTarget \"0\">}\">\n"
     ]
    }
   ],
   "source": [
    "record = record1\n",
    "\n",
    "le_record1 = LeRecord(record)\n",
    "print(le_record1)\n",
    "# print(le_record1.texts, le_record1.targets)\n",
    "\n",
    "le_record2 = le_record1.apply_to_components(trans_fn, task_config=task_config)\n",
    "print(le_record2)\n",
    "# print(le_record2.texts, le_record2.targets)\n",
    "\n",
    "le_record3 = le_record2.apply_to_components(trans_fn, task_config=task_config)\n",
    "print(le_record3)\n",
    "# print(le_record3.texts, le_record3.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "i-CP8ZtRnI43"
   },
   "outputs": [],
   "source": [
    "record1 = {\n",
    "    \"text\": \"This is a nasty input record.\",\n",
    "    \"label\": 0\n",
    "}\n",
    "\n",
    "record2 = {\n",
    "    \"text\": \"This is a beautiful input record.\",\n",
    "    \"label\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg45oG3bnGtD"
   },
   "source": [
    "##### `TextMix`\n",
    "\n",
    "When you think about how the data is actually processed, it's almost always done separately (e.g. we have a `inputs` variable and a `labels` variable. Work is done essentially by column rather than record and therefore we should be tracking lineage on by text and target rather than record. LeRecord is likely to be an unnecessary abstraction or maybe just a convenience class that helps quickly enable lineage on a data row. \n",
    "\n",
    "Let's experiment with changing tracking the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "r4samVjRTxZx"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fWGyYlN_TxW1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/coraline/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "DwDnLwsXTxME"
   },
   "outputs": [],
   "source": [
    "def enable_dataset_lineage(batch):\n",
    "    \"\"\"\n",
    "    To be used with datasets.Dataset.map()\n",
    "    Example:\n",
    "    `\n",
    "    updated_dataset = dataset.map(enable_dataset_lineage, \n",
    "                                  batched=True, \n",
    "                                  batch_size=batch_size)\n",
    "    `\n",
    "    NOTE: This doesn't work because LeText and LeTarget\n",
    "          are not permissible Arrow data types.\n",
    "    \"\"\"\n",
    "    data = [LeText(t) for t in batch['sentence']]\n",
    "    target = [LeTarget(t) for t in batch['label']]\n",
    "    return {'sentence': data, 'label': batch['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "X_-cXNLYTxRW"
   },
   "outputs": [],
   "source": [
    "def enable_dataset_lineage(dataset, text_keys=None, target_keys=None):\n",
    "\n",
    "    # guess keys if none provided\n",
    "    if not text_keys or not target_keys:\n",
    "        dataset_features = dataset.features\n",
    "        if not text_keys:\n",
    "            text_keys = []\n",
    "            for k in dataset_features.keys():\n",
    "                if (k in [\"text\", \"sentence\"] \n",
    "                    or dataset_features[k].dtype == \"string\"):\n",
    "                    text_keys.append(k)\n",
    "        if not target_keys:\n",
    "            target_keys = []\n",
    "            for k in dataset_features.keys():\n",
    "                if k in [\"label\", \"target\"]:\n",
    "                    target_keys.append(k)\n",
    "\n",
    "    # enable text lineage\n",
    "    if len(text_keys) == 1:\n",
    "        le_texts = [LeText(t) for t in dataset[text_keys[0]]]\n",
    "    else:\n",
    "        le_texts = []\n",
    "        for k in text_keys:\n",
    "            le_texts.append([LeText(t) for t in dataset[k]])\n",
    "\n",
    "    # enable target lineage\n",
    "    if len(target_keys) == 1:\n",
    "        le_targets = [LeTarget(t) for t in dataset[target_keys[0]]]\n",
    "    else:\n",
    "        le_targets = []\n",
    "        for k in target_keys:\n",
    "            le_targets.append([LeTarget(t) for t in dataset[k]])\n",
    "\n",
    "    return le_texts, le_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "eNSwMUuaTxG9"
   },
   "outputs": [],
   "source": [
    "texts, labels = enable_dataset_lineage(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "x2gm9SdwTw_S"
   },
   "outputs": [],
   "source": [
    "# # create a batch to process\n",
    "# batch_size = 12\n",
    "# batch = (texts[:batch_size], labels[:batch_size])\n",
    "\n",
    "# # define transform function\n",
    "# trans_fn = TextMix()\n",
    "\n",
    "# # apply transform\n",
    "# trans_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "1OBPUaj5da7Y"
   },
   "outputs": [],
   "source": [
    "# class Provenance\n",
    "# class FeatureProvenance(Provenance)\n",
    "# class TransformationProvenance(Provenance)\n",
    "\n",
    "# class LeText:\n",
    "#    self.prov = prov \n",
    "\n",
    "# look at https://github.com/maligulzar/OptDebug and replicate basic structure into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "jNkT6m6NTn_e"
   },
   "outputs": [],
   "source": [
    "class LeContext:\n",
    "\n",
    "    def __init__(self, original_batch):\n",
    "        self.original_batch = original_batch\n",
    "        orig_texts, orig_targets = self.original_batch\n",
    "\n",
    "        # extract text and targets for transformation\n",
    "        self.texts = [t.text if isinstance(t, LeText) else t for t in orig_texts]\n",
    "        self.targets = [t.target if isinstance(t, LeTarget) else t for t in orig_targets]\n",
    "\n",
    "        # enable lineage if not already\n",
    "        self.le_texts = [LeText(t) if not isinstance(t, LeText) else t for t in orig_texts]\n",
    "        self.le_targets = [LeTarget(t) if not isinstance(t, LeTarget) else t for t in orig_targets]\n",
    "\n",
    "        # variables to extract from LeContext after processing\n",
    "        self.linked_texts = None\n",
    "        self.linked_targets = None\n",
    "    \n",
    "    def apply(self, fn, *args, **kwargs):\n",
    "        self.fn = fn\n",
    "        new_texts, new_targets = fn((self.texts, self.targets), *args, **kwargs)\n",
    "        self.new_texts = new_texts\n",
    "        self.new_targets = new_targets\n",
    "\n",
    "        linked_texts, linked_targets = [], []\n",
    "        for text1, text2, target1, target2 in zip(self.le_texts,\n",
    "                                                  self.new_texts,\n",
    "                                                  self.le_targets,\n",
    "                                                  self.new_targets):\n",
    "          \n",
    "            # track changes to texts\n",
    "            parsed_a, parsed_b, changes = diff_text(text1.text, \n",
    "                                                    text2, \n",
    "                                                    granularity=text1.granularity)\n",
    "            \n",
    "            text_attrs = {\n",
    "                \"granularity\": text1.granularity,\n",
    "                \"changes\": changes,\n",
    "                \"transformation\": self.fn,\n",
    "                \"previous\": text1\n",
    "            }\n",
    "            text2 = LeText(text2, text1.granularity, text_attrs)\n",
    "            linked_texts.append(text2)\n",
    "\n",
    "            # track changes to targets\n",
    "            target_attrs = {\n",
    "                \"transformation\": self.fn,\n",
    "                \"previous\": target1\n",
    "            }\n",
    "            target2 = LeTarget(target2, target_attrs)\n",
    "            linked_targets.append(target2)\n",
    "        \n",
    "        self.linked_texts = linked_texts\n",
    "        self.linked_targets = linked_targets\n",
    "        return self.linked_texts, self.linked_targets\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        if isinstance(exc_value, Exception):\n",
    "            print(f\"An exception occurred in your with block: {exc_type}\")\n",
    "            print(f\"Exception message: {exc_value}\")\n",
    "            print(f\"Traceback info: {exc_tb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test new LeContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coraline/anaconda3/envs/dpml/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-28 22:03:40.374269: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from lineage import LeContext\n",
    "from sibyl import ChangeAntonym, InsertNegativePhrase, TextMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5o19TakXhtOD"
   },
   "outputs": [],
   "source": [
    "transform = TextMix()\n",
    "\n",
    "in_text = [\n",
    "    \"The characters are unlikeable and the script is awful. It's a waste of the talents of Deneuve and Auteuil.\", \n",
    "    \"Unwatchable. You can't even make it past the first three minutes. And this is coming from a huge Adam Sandler fan!!1\",\n",
    "    \"An unfunny, unworthy picture which is an undeserving end to Peter Sellers' career. It is a pity this movie was ever made.\",\n",
    "    \"I think it's one of the greatest movies which are ever made, and I've seen many... The book is better, but it's still a very good movie!\",\n",
    "    \"The only thing serious about this movie is the humor. Well worth the rental price. I'll bet you watch it twice. It's obvious that Sutherland enjoyed his role.\",\n",
    "    \"Touching; Well directed autobiography of a talented young director/producer. A love story with Rabin's assassination in the background. Worth seeing\"\n",
    "]\n",
    "\n",
    "in_target = [0, 0, 0, 1, 1, 1] # (imdb dataset 0=negative, 1=positive)\n",
    "\n",
    "batch = (in_text, in_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with LeContext(batch) as le:\n",
    "    new_records = le.apply(transform, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch = new_records\n",
    "\n",
    "with LeContext(new_batch) as le:\n",
    "    new_records2 = le.apply(transform, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gQ_6LWFPkOJI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"Unwatchable. You can\\'t even make it past the first three minutes. And this is coming from a huge Adam Sandler fan!!1 Unwatchable. You can\\'t even make it past the first three minutes. And this is coming from a huge Adam Sandler fan!!1\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_records[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y9o3XQOhoilo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LeRecord:\n",
       "\t text=\"b'b\"Unwatchable. You can\\'t even make it past the first three minutes. And this is coming from a huge Adam Sandler fan!!1 Unwatchable. You can\\'t even make it past the first three minutes. And this is coming from a huge Adam Sandler fan!!1\" b\"An unfunny, unworthy picture which is an undeserving end to Peter Sellers\\' career. It is a pity this movie was ever made. The only thing serious about this movie is the humor. Well worth the rental price. I\\'ll bet you watch it twice. It\\'s obvious that Sutherland enjoyed his role.\"'\",\n",
       "\t target=\"[0.8884123728427289, 0.11158762715727105]\",\n",
       "\t le_attrs={'transformation_provenance': <TransformationProvenance: {(0, \"{'class': 'TextMix', 'return_metadata': False}\"), (1, \"{'class': 'TextMix', 'return_metadata': False}\")}>, 'feature_provenance': <FeatureProvenance[edit_seq] {(0, (41, 42), 'replace: [20,21]-[41,42]'), (1, (42, 92), 'insert: [42,42]-[42,92]')}>, 'granularity': 'word'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_records2[1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dpml",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
