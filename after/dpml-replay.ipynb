{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuWi1REPD_Ux"
   },
   "source": [
    "# DPML | Latency Replay\n",
    "\n",
    "In this notebook, we investigate the reproducibility of transformation sequences captured by `dpml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9Zs0hYFFIYf"
   },
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t8_koamxFHfR",
    "outputId": "e892403a-70b8-4728-ec18-c31ee7e81580"
   },
   "outputs": [],
   "source": [
    "from lineage import LeBatch\n",
    "from lineage.transformation import DPMLClassWrapper, DPMLCallableWrapper\n",
    "from lineage.utils import *\n",
    "\n",
    "from sibyl import *\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktCv--paFOqG"
   },
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\Fabrice\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:50000]\")\n",
    "dataset = dataset.rename_column('sentence', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routine to be Tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = SibylTransformScheduler(\"sentiment\", class_wrapper=DPMLClassWrapper)\n",
    "stochastic_list = [Concept2Sentence, ConceptMix, Emojify]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486cf1008daa425e820ba48628d847f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_trials = 3\n",
    "batch_size= 10\n",
    "\n",
    "texts, labels = dataset['text'], dataset['label'] \n",
    "new_texts, new_labels = [], []\n",
    "\n",
    "scheduler.num_INV = 1\n",
    "scheduler.num_SIB = 1\n",
    "\n",
    "transform_schedule = []\n",
    "for i in tqdm(range(0, len(labels), batch_size)):\n",
    "    transforms = []\n",
    "    for transform in scheduler.sample():\n",
    "        if transform.wrapped_class in stochastic_list:\n",
    "            continue\n",
    "        transforms.append(transform)\n",
    "    transform_schedule.append(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating CSV Replay Time / Memory Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "class catchtime(object):\n",
    "    def __init__(self, name=\"Code Block\"):\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.t = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.t = time.perf_counter() - self.t\n",
    "        print('{0:6.3f}s : {1}'.format(self.t, self.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rng_state(fn, attr, state):\n",
    "    rng_state = preprocess_params(state)\n",
    "    random_generator = getattr(fn.func.__self__, attr)\n",
    "    random_generator.__setstate__(rng_state)\n",
    "    setattr(fn.func.__self__, attr, random_generator)\n",
    "    return fn\n",
    "\n",
    "def print_query(session, q_string, args):\n",
    "    rows = session.execute(q_string, args)\n",
    "    for row in rows:\n",
    "        print(row._mapping)\n",
    "        \n",
    "def collect_from_query(session, q_string, args):\n",
    "    rows = session.execute(q_string, args)\n",
    "    return [row._mapping for row in rows]\n",
    "\n",
    "def replay_all_from_csv():\n",
    "      \n",
    "    with catchtime(\"Load CSVTransformLogger\") as t:\n",
    "        from lineage.storage.csv.transform_logger import TransformLogger as CSVTransformLogger\n",
    "    \n",
    "    # fetch data\n",
    "    with catchtime(\"Load data\") as t:\n",
    "        logger = CSVTransformLogger()\n",
    "        df = pd.read_csv(logger.path, header=None, names=['batch_id', 'text', 'target', 'transform_prov'])\n",
    "        transform_df = pd.read_csv(logger.transform_path, header=None, index_col=0, names=['transform_id', 'transform'])\n",
    "    \n",
    "    with catchtime(\"Load batches + transform_set\") as t:\n",
    "        transform_idxs = set()\n",
    "        batches = {}\n",
    "        for idx, row in df.iterrows():\n",
    "            bid = row['batch_id']\n",
    "            if bid not in batches:\n",
    "                batches[bid] = {'text':[], 'target':[], 'transform': []}\n",
    "\n",
    "            batches[bid]['text'].append(row['text'])\n",
    "            batches[bid]['target'].append(row['target'])\n",
    "\n",
    "            if len(batches[bid]['transform']) == 0:\n",
    "                batches[bid]['transform'] = eval(row['transform_prov'])\n",
    "                transform_idxs = transform_idxs | set(batches[bid]['transform'])\n",
    "                    \n",
    "    with catchtime(\"Load transforms\") as t:\n",
    "        transforms = []\n",
    "        random_states = []\n",
    "        hashes = []\n",
    "        mapping = {}\n",
    "        for idx in transform_idxs:\n",
    "            t_prov = json.loads(transform_df.loc[idx]['transform'])\n",
    "            random_state_attr = t_prov.pop('class_rng')\n",
    "            random_state_info = t_prov.pop('callable_rng_state')\n",
    "            random_states.append((random_state_attr, random_state_info))\n",
    "\n",
    "            t_prov_hash = hash(repr(t_prov))\n",
    "            if t_prov_hash not in hashes:\n",
    "                transforms.append(load_transform_from_replay_provenance(t_prov))\n",
    "                hashes.append(t_prov_hash)\n",
    "                mapping[idx] = hashes.index(t_prov_hash)\n",
    "            else:\n",
    "                mapping[idx] = hashes.index(t_prov_hash)\n",
    "    load_time = t.t\n",
    "\n",
    "    # replay\n",
    "    with catchtime(\"Replay\") as t:\n",
    "        new_records = []\n",
    "        for batch_id in sorted(list(batches.keys())):\n",
    "            batch = (batches[batch_id]['text'], batches[batch_id]['target'])\n",
    "            for idx in batches[batch_id]['transform']:\n",
    "                rs_attr, rs_info = random_states[idx]\n",
    "                fn_id = mapping[idx]\n",
    "                t_fn = set_rng_state(transforms[fn_id], rs_attr, rs_info)\n",
    "                batch = t_fn(batch)\n",
    "            texts, labels = batch\n",
    "            new_records += [(x, y) for x,y in zip(texts, labels)]\n",
    "    replay_time = t.t\n",
    "            \n",
    "    return new_records, load_time, replay_time\n",
    "\n",
    "def replay_all_from_db(run_id=None):\n",
    "    \n",
    "    with catchtime(\"Load SQLTransformLogger\") as t:\n",
    "        from lineage.storage.sqlalchemy.transform_logger import TransformLogger as SQLTransformLogger\n",
    "        logger = SQLTransformLogger()\n",
    "    \n",
    "    # fetch data\n",
    "    with Session(logger.engine) as session:\n",
    "        if not run_id:\n",
    "            run_id = session.query(Run).order_by(Run.id.desc()).first().id\n",
    "        \n",
    "        \n",
    "        with catchtime(\"Load data\") as t:\n",
    "            ta_stmt = text(\n",
    "                \"\"\"\n",
    "                SELECT DISTINCT \n",
    "                       ta.batch_id, \n",
    "                       ta.transform_id, \n",
    "                       ta.transform_state\n",
    "                FROM TransformApplied ta \n",
    "                WHERE ta.run_id == :run_id\n",
    "                \"\"\"\n",
    "            )\n",
    "            ta_rows = collect_from_query(session, ta_stmt, {'run_id': run_id})\n",
    "\n",
    "            t_stmt = text(\n",
    "                \"\"\"\n",
    "                SELECT DISTINCT t.*\n",
    "                FROM Transform t\n",
    "                INNER JOIN TransformApplied ta ON t.id = ta.transform_id\n",
    "                WHERE ta.run_id == :run_id\n",
    "                \"\"\"\n",
    "            )\n",
    "            t_rows = collect_from_query(session, t_stmt, {'run_id': run_id})\n",
    "\n",
    "            r_stmt = text(\n",
    "                \"\"\"\n",
    "                SELECT r.id, r.text, r.target, ta.batch_id\n",
    "                FROM Record r\n",
    "                INNER JOIN (\n",
    "                    SELECT DISTINCT ta.input_record_id, ta.batch_id\n",
    "                    FROM TransformApplied ta \n",
    "                    WHERE ta.run_id == :run_id  \n",
    "                ) ta ON ta.input_record_id = r.id\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "            r_rows = collect_from_query(session, r_stmt, {'run_id': run_id})\n",
    "\n",
    "    with catchtime(\"Load batches + transform_set\") as t:\n",
    "        batches = {}\n",
    "        for row in r_rows:\n",
    "            bid = row['batch_id']\n",
    "            if bid not in batches:\n",
    "                batches[bid] = {'text':[], 'target':[]}\n",
    "\n",
    "            batches[bid]['text'].append(row['text'])\n",
    "            batches[bid]['target'].append(row['target'])\n",
    "\n",
    "        transforms = {}\n",
    "        for row in t_rows:\n",
    "            transforms[row['id']] = (load_transform_from_replay_provenance(row), row['class_rng'])\n",
    "    load_time = t.t\n",
    "\n",
    "    # replay\n",
    "    with catchtime(\"Replay\") as t:\n",
    "        new_records = []    \n",
    "        for batch_id, batch in batches.items():\n",
    "            batch = (batch['text'], [eval(t) for t in batch['target']])\n",
    "            tas = [row for row in ta_rows if row['batch_id'] == batch_id]    \n",
    "            for row in tas:\n",
    "                t_fn, rs_attr = transforms[row['transform_id']]\n",
    "                t_fn = set_rng_state(t_fn, rs_attr, row['transform_state'])\n",
    "                batch = t_fn(batch)\n",
    "            texts, labels = batch\n",
    "            new_records += [(x, y) for x,y in zip(texts, labels)]\n",
    "    replay_time = t.t\n",
    "        \n",
    "    return new_records, load_time, replay_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "\n",
    "from lineage.storage.sqlalchemy.transform_logger import TransformLogger as SQLTransformLogger\n",
    "from lineage.storage.sqlalchemy.models import *\n",
    "from sqlalchemy.sql import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file_pth = \"dpml/lineage/storage/dpml.db\"\n",
    "if os.path.exists(db_file_pth):\n",
    "    os.remove(db_file_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_batch.transform_logger.clean_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4753a631d14e7db51be292379f3a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Trial 0: 135.674 seconds\n",
      "Elapsed logging time for Trial 0: 173.182 seconds\n",
      " 0.409s : Load SQLTransformLogger\n",
      " 0.444s : Load data\n",
      " 4.022s : Load batches + transform_set\n",
      "139.458s : Replay\n",
      "Elapsed replay time for Trial 0: 144.368 seconds\n",
      "Replay mismatches for Trial 0: 0\n",
      "Elapsed time for Trial 1: 133.144 seconds\n",
      "Elapsed logging time for Trial 1: 171.747 seconds\n",
      " 0.408s : Load SQLTransformLogger\n",
      " 1.143s : Load data\n",
      " 3.903s : Load batches + transform_set\n",
      "138.900s : Replay\n",
      "Elapsed replay time for Trial 1: 144.399 seconds\n",
      "Replay mismatches for Trial 1: 0\n",
      "Elapsed time for Trial 2: 135.252 seconds\n",
      "Elapsed logging time for Trial 2: 173.665 seconds\n",
      " 0.410s : Load SQLTransformLogger\n",
      " 0.412s : Load data\n",
      " 3.932s : Load batches + transform_set\n",
      "150.177s : Replay\n",
      "Elapsed replay time for Trial 2: 154.977 seconds\n",
      "Replay mismatches for Trial 2: 0\n"
     ]
    }
   ],
   "source": [
    "no_lineage_times = []\n",
    "replay_logging_times, replay_fn_load_times, replay_generation_times, num_mismatches = [], [], [], []\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    no_lineage_text, no_lineage_targets = [], []\n",
    "    replay_log_text, replay_log_targets = [], []\n",
    "    \n",
    "    # no lineage ====================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(labels), batch_size), transform_schedule):\n",
    "        \n",
    "        text_batch = texts[i:i+batch_size]\n",
    "        label_batch = labels[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        for transform in t_sched:\n",
    "            batch = transform.transform_batch(batch)\n",
    "            \n",
    "        no_lineage_text.extend(batch[0])\n",
    "        no_lineage_targets.extend(batch[1])\n",
    "        \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    no_lineage_times.append(run_time)\n",
    "    print('Elapsed time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay logging ================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(labels), batch_size), transform_schedule):\n",
    "        text_batch = texts[i:i+batch_size]\n",
    "        label_batch = labels[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        \n",
    "        if len(t_sched) == 0:\n",
    "            continue\n",
    "            \n",
    "        with LeBatch(original_batch=batch) as le_batch:\n",
    "            for transform in t_sched:\n",
    "                batch = le_batch.apply(batch, transform.transform_batch)\n",
    "            \n",
    "        replay_log_text.extend([x.text for x in batch])\n",
    "        replay_log_targets.extend([x.target for x in batch])\n",
    "        \n",
    "    le_batch.transform_logger.flush(force=True)        \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_logging_times.append(run_time)\n",
    "    print('Elapsed logging time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay generation ==============================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    new_records, load_time, replay_time = replay_all_from_db()\n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_fn_load_times.append(load_time)\n",
    "    replay_generation_times.append(replay_time)\n",
    "    print('Elapsed replay time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    original_records = [(text, target) for text, target in zip(replay_log_text, replay_log_targets)]\n",
    "    num_mismatch = 0\n",
    "    counter = 0\n",
    "    for old_r, new_r in zip(original_records, new_records):\n",
    "        if old_r[0] != new_r[0] or np.any(old_r[1] != new_r[1]):\n",
    "            num_mismatch += 1  \n",
    "        counter += 1\n",
    "    num_mismatches.append(num_mismatch)\n",
    "    print('Replay mismatches for Trial {0}: {1}'.format(trial, num_mismatch))   \n",
    "    \n",
    "    # del original_records, new_records\n",
    "    \n",
    "    le_batch.transform_logger.clean_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_lineage_times: 134.6898679666659\n",
      "replay_logging_times: 172.86470113333357\n",
      "replay_fn_load_times: 3.952111300000373\n",
      "replay_generation_times: 142.84499380000003\n",
      "num_mismatches: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"no_lineage_times:\", np.mean(no_lineage_times))\n",
    "print(\"replay_logging_times:\", np.mean(replay_logging_times))\n",
    "print(\"replay_fn_load_times:\", np.mean(replay_fn_load_times))\n",
    "print(\"replay_generation_times:\", np.mean(replay_generation_times))\n",
    "print(\"num_mismatches:\", np.mean(num_mismatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay with CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_pth = \"dpml/lineage/storage/dpml.csv\"\n",
    "if os.path.exists(csv_file_pth):\n",
    "    os.remove(csv_file_pth)\n",
    "if os.path.exists(\"dpml/lineage/storage/transform.csv\"):\n",
    "    os.remove(\"dpml/lineage/storage/transform.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is DA58-C5DE\n",
      "\n",
      " Directory of C:\\Users\\Fabrice\\Documents\\GitHub\\dpml\\after\\dpml\\lineage\\storage\n",
      "\n",
      "08/16/2022  02:17 PM    <DIR>          .\n",
      "08/16/2022  12:07 PM    <DIR>          ..\n",
      "07/27/2022  01:16 PM               312 __init__.py\n",
      "07/27/2022  01:16 PM    <DIR>          __pycache__\n",
      "08/09/2022  03:26 PM    <DIR>          csv\n",
      "08/16/2022  02:17 PM            40,960 dpml.db\n",
      "08/09/2022  03:26 PM    <DIR>          sqlalchemy\n",
      "               2 File(s)         41,272 bytes\n",
      "               5 Dir(s)  337,850,900,480 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls \"dpml/lineage/storage/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f251826b132d4352a032ab4d91a70cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Trial 0:  0.115 seconds\n",
      "Elapsed logging time for Trial 0:  0.153 seconds\n",
      " 1.237s : Load CSVTransformLogger\n",
      " 0.001s : Load data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Fabrice\\\\Documents\\\\GitHub\\\\dpml\\\\after\\\\dpml\\\\lineage\\\\storage\\\\dpml.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_360\\2001241715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# replay generation ==============================================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mstartTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mnew_records\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_all_from_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mrun_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mreplay_fn_load_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_360\\3681178429.py\u001b[0m in \u001b[0;36mreplay_all_from_csv\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcatchtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Load data\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCSVTransformLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'transform_prov'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mtransform_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transform_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'transform'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1219\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\dpml\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Fabrice\\\\Documents\\\\GitHub\\\\dpml\\\\after\\\\dpml\\\\lineage\\\\storage\\\\dpml.csv'"
     ]
    }
   ],
   "source": [
    "no_lineage_times = []\n",
    "replay_logging_times, replay_fn_load_times, replay_generation_times, num_mismatches = [], [], [], []\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    no_lineage_text, no_lineage_targets = [], []\n",
    "    replay_log_text, replay_log_targets = [], []\n",
    "    \n",
    "    # no lineage ====================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(labels), batch_size), transform_schedule):\n",
    "        \n",
    "        text_batch = texts[i:i+batch_size]\n",
    "        label_batch = labels[i:i+batch_size]\n",
    "        batch = (text_batch, label_batch)\n",
    "        for transform in t_sched:\n",
    "            batch = transform.transform_batch(batch)\n",
    "            \n",
    "        no_lineage_text.extend(batch[0])\n",
    "        no_lineage_targets.extend(batch[1])\n",
    "        \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    no_lineage_times.append(run_time)\n",
    "    print('Elapsed time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay logging ================================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    for i, t_sched in zip(range(0, len(labels), batch_size), transform_schedule):\n",
    "        \n",
    "        if len(t_sched) == 0:\n",
    "            continue\n",
    "            \n",
    "        batch = (texts[i:i+batch_size], labels[i:i+batch_size])\n",
    "          \n",
    "        with LeBatch(original_batch=batch) as le_batch:\n",
    "            for transform in t_sched:\n",
    "                batch = le_batch.apply(batch, transform.transform_batch)\n",
    "            \n",
    "        replay_log_text.extend([x.text for x in batch])\n",
    "        replay_log_targets.extend([x.target for x in batch])\n",
    "            \n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_logging_times.append(run_time)\n",
    "    print('Elapsed logging time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    # replay generation ==============================================================================================\n",
    "    startTime = time.perf_counter()\n",
    "    new_records, load_time, replay_time = replay_all_from_csv()\n",
    "    run_time = time.perf_counter() - startTime\n",
    "    replay_fn_load_times.append(load_time)\n",
    "    replay_generation_times.append(replay_time)\n",
    "    print('Elapsed replay time for Trial {0}: {1:6.3f} seconds'.format(trial, run_time))\n",
    "    \n",
    "    original_records = [(text, target) for text, target in zip(replay_log_text, replay_log_targets)]\n",
    "    num_mismatch = 0\n",
    "    counter = 0\n",
    "    for old_r, new_r in zip(original_records, new_records):\n",
    "        if old_r[0] != new_r[0] or np.any(old_r[1] != new_r[1]):\n",
    "            num_mismatch += 1  \n",
    "        counter += 1\n",
    "    num_mismatches.append(num_mismatch)\n",
    "    print('Replay mismatches for Trial {0}: {1}'.format(trial, num_mismatch))   \n",
    "    \n",
    "    # del original_records, new_records\n",
    "    \n",
    "    # le_batch.transform_logger.clean_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"no_lineage_times:\", np.mean(no_lineage_times))\n",
    "print(\"replay_logging_times:\", np.mean(replay_logging_times))\n",
    "print(\"replay_fn_load_times:\", np.mean(replay_fn_load_times))\n",
    "print(\"replay_generation_times:\", np.mean(replay_generation_times))\n",
    "print(\"num_mismatches:\", np.mean(num_mismatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_all_from_db(run_id=None):\n",
    "    \n",
    "    from lineage.storage.sqlalchemy.transform_logger import TransformLogger as SQLTransformLogger\n",
    "    \n",
    "    logger = SQLTransformLogger()\n",
    "    \n",
    "    # fetch data\n",
    "    with Session(logger.engine) as session:\n",
    "        if not run_id:\n",
    "            run_id = session.query(Run).order_by(Run.id.desc()).first().id\n",
    "        \n",
    "        ta_stmt = text(\n",
    "            \"\"\"\n",
    "            SELECT DISTINCT \n",
    "                   ta.batch_id, \n",
    "                   ta.transform_id, \n",
    "                   ta.transform_state\n",
    "            FROM TransformApplied ta \n",
    "            WHERE ta.run_id == :run_id\n",
    "            \"\"\"\n",
    "        )\n",
    "        ta_rows = collect_from_query(session, ta_stmt, {'run_id': run_id})\n",
    "\n",
    "        t_stmt = text(\n",
    "            \"\"\"\n",
    "            SELECT DISTINCT t.*\n",
    "            FROM Transform t\n",
    "            INNER JOIN TransformApplied ta ON t.id = ta.transform_id\n",
    "            WHERE ta.run_id == :run_id\n",
    "            \"\"\"\n",
    "        )\n",
    "        t_rows = collect_from_query(session, t_stmt, {'run_id': run_id})\n",
    "\n",
    "        r_stmt = text(\n",
    "            \"\"\"\n",
    "            SELECT r.id, r.text, r.target, ta.batch_id\n",
    "            FROM Record r\n",
    "            INNER JOIN (\n",
    "                SELECT DISTINCT ta.input_record_id, ta.batch_id\n",
    "                FROM TransformApplied ta \n",
    "                WHERE ta.run_id == :run_id  \n",
    "            ) ta ON ta.input_record_id = r.id\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        r_rows = collect_from_query(session, r_stmt, {'run_id': run_id})\n",
    "\n",
    "\n",
    "    # replay\n",
    "    batches = {}\n",
    "    for row in r_rows:\n",
    "        bid = row['batch_id']\n",
    "        if bid not in batches:\n",
    "            batches[bid] = {'text':[], 'target':[]}\n",
    "\n",
    "        batches[bid]['text'].append(row['text'])\n",
    "        batches[bid]['target'].append(row['target'])\n",
    "\n",
    "    transforms = {}\n",
    "    for row in t_rows:\n",
    "        transforms[row['id']] = (load_transform_from_replay_provenance(row), row['class_rng'])\n",
    "\n",
    "    new_records = []    \n",
    "    for batch_id, batch in batches.items():\n",
    "        batch = (batch['text'], [eval(t) for t in batch['target']])\n",
    "        tas = [row for row in ta_rows if row['batch_id'] == batch_id]    \n",
    "        for row in tas:\n",
    "            t_fn, rs_attr = transforms[row['transform_id']]\n",
    "            t_fn = set_rng_state(t_fn, rs_attr, row['transform_state'])\n",
    "            batch = t_fn(batch)\n",
    "        texts, labels = batch\n",
    "        new_records += [(x, y) for x,y in zip(texts, labels)]\n",
    "        \n",
    "    return new_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.001s : Load SQLTransformLogger\n",
      " 0.001s : Load data\n",
      " 0.444s : Load batches + transform_set\n",
      " 0.015s : Replay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(\"b'hide young secretion from the paternal whole  obscure young secretion from the paternal whole '\",\n",
       "   [1.0, 0.0]),\n",
       "  ('goes to lengths That said, I loved it.', array([0.75, 0.25])),\n",
       "  (\"b'equals the original and in some ways even betters it  equals the original and in some ways even betters it,'\",\n",
       "   [0.0, 1.0]),\n",
       "  ('the activeness is stilted ', 0),\n",
       "  ('the entire point of a shaggy dog story , of course , is that it goes nowhere , and this is classic nowheresville in every sense .  That said, I hated it.',\n",
       "   array([0.75, 0.25]))],\n",
       " 0.44387629999999945,\n",
       " 0.015048100000001341)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_all_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lineage.storage.sqlalchemy import *\n",
    "from sqlalchemy import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform\n",
      "689\n"
     ]
    }
   ],
   "source": [
    "print('Transform')\n",
    "stmt = select(Transform)\n",
    "with logger.engine.connect() as conn:\n",
    "    print(len(list(conn.execute(stmt))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger = TransformLogger()\n",
    "\n",
    "print('Run')\n",
    "stmt = select(Run)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)\n",
    "    \n",
    "print('Record')\n",
    "stmt = select(Record)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)\n",
    "\n",
    "print('Transform')\n",
    "stmt = select(Transform)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)\n",
    "\n",
    "print('TransformApplied')\n",
    "stmt = select(TransformApplied)\n",
    "with logger.engine.connect() as conn:\n",
    "    for row in conn.execute(stmt):\n",
    "        print(row._mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.random._generator.Generator"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t_orig().np_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This is a test.\", \"This isn't a test!\"]\n",
    "target = [0, 1]\n",
    "batch = (text, target)\n",
    "\n",
    "t_orig = TRANSFORMATIONS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPMLClassWrapper\n",
      "DPMLClassWrapper | transform_batch\n",
      "(['This is a test.', 'This is not a test!'], [0, 1])\n",
      "_class_name: ExpandContractions\n",
      "_class_args: []\n",
      "_class_kwargs: {'task_name': 'sentiment', 'return_metadata': True}\n",
      "_class_rng: Generator(PCG64)\n",
      "_callable_name: transform_batch\n",
      "_callable_args: []\n",
      "_callable_kwargs: []\n",
      "_callable_rng_state: {'bit_generator': 'PCG64', 'state': {'state': 129413257090554225206130458028910539494, 'inc': 16450919397810582319219321886622321693}, 'has_uint32': 0, 'uinteger': 0}\n",
      "DPMLClassWrapper | transform_Xy\n",
      "This is not a test! 1\n",
      "_class_name: ExpandContractions\n",
      "_class_args: []\n",
      "_class_kwargs: {'task_name': 'sentiment', 'return_metadata': True}\n",
      "_class_rng: Generator(PCG64)\n",
      "_callable_name: transform_Xy\n",
      "_callable_args: []\n",
      "_callable_kwargs: []\n",
      "_callable_rng_state: {'bit_generator': 'PCG64', 'state': {'state': 129413257090554225206130458028910539494, 'inc': 16450919397810582319219321886622321693}, 'has_uint32': 0, 'uinteger': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"DPMLClassWrapper\")\n",
    "\n",
    "t_class_wrapped = DPMLClassWrapper(t_orig)\n",
    "t_class_wrapped = t_class_wrapped(task_name=\"sentiment\", return_metadata=True)\n",
    "\n",
    "batch = t_class_wrapped.transform_batch(batch)\n",
    "\n",
    "print(\"DPMLClassWrapper | transform_batch\")\n",
    "print(batch)\n",
    "print(\"_class_name:\", t_class_wrapped._class_name)\n",
    "print(\"_class_args:\", t_class_wrapped._class_args)\n",
    "print(\"_class_kwargs:\", t_class_wrapped._class_kwargs)\n",
    "print(\"_class_rng:\", t_class_wrapped._class_rng)\n",
    "print(\"_callable_name:\", t_class_wrapped._callable_name)\n",
    "print(\"_callable_args:\", t_class_wrapped._callable_args)\n",
    "print(\"_callable_kwargs:\", t_class_wrapped._callable_kwargs)\n",
    "print(\"_callable_rng_state:\", t_class_wrapped._callable_rng_state)\n",
    "\n",
    "X, y, meta = t_class_wrapped.transform_Xy(text[1], target[1])\n",
    "\n",
    "print(\"DPMLClassWrapper | transform_Xy\")\n",
    "print(X, y)\n",
    "print(\"_class_name:\", t_class_wrapped._class_name)\n",
    "print(\"_class_args:\", t_class_wrapped._class_args)\n",
    "print(\"_class_kwargs:\", t_class_wrapped._class_kwargs)\n",
    "print(\"_class_rng:\", t_class_wrapped._class_rng)\n",
    "print(\"_callable_name:\", t_class_wrapped._callable_name)\n",
    "print(\"_callable_args:\", t_class_wrapped._callable_args)\n",
    "print(\"_callable_kwargs:\", t_class_wrapped._callable_kwargs)\n",
    "print(\"_callable_rng_state:\", t_class_wrapped._callable_rng_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPMLCallableWrapper | transform_batch\n",
      "(['hide new secretions from the parental units ', 'contains no wit , only labored gags '], [0, 0])\n",
      "_callable_name ('transform_batch',)\n",
      "_callable_args []\n",
      "_callable_kwargs []\n",
      "DPMLCallableWrapper | transform_Xy\n",
      "contains no wit , only labored gags  1\n",
      "_callable_name ('transform_Xy',)\n",
      "_callable_args []\n",
      "_callable_kwargs []\n"
     ]
    }
   ],
   "source": [
    "t_init = t_orig(task_name=\"sentiment\", return_metadata=True)\n",
    "\n",
    "t_callable_wrapped = DPMLCallableWrapper(t_init.transform_batch)\n",
    "batch = t_callable_wrapped(batch)\n",
    "\n",
    "print(\"DPMLCallableWrapper | transform_batch\")\n",
    "print(batch)\n",
    "print(\"_callable_name\", t_callable_wrapped._callable_name)\n",
    "print(\"_callable_args\", t_callable_wrapped._callable_args)\n",
    "print(\"_callable_kwargs\", t_callable_wrapped._callable_kwargs)\n",
    "\n",
    "t_callable_wrapped = DPMLCallableWrapper(t_init.transform_Xy)\n",
    "X, y, meta = t_callable_wrapped(text[1], target[1])\n",
    "\n",
    "print(\"DPMLCallableWrapper | transform_Xy\")\n",
    "print(X, y)\n",
    "print(\"_callable_name\", t_callable_wrapped._callable_name)\n",
    "print(\"_callable_args\", t_callable_wrapped._callable_args)\n",
    "print(\"_callable_kwargs\", t_callable_wrapped._callable_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dpml",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
